[
  {
    "objectID": "pot/index.html",
    "href": "pot/index.html",
    "title": "Peaks-Over-Threshold (POT) Modelling",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)"
  },
  {
    "objectID": "pot/index.html#non-parametric-approach",
    "href": "pot/index.html#non-parametric-approach",
    "title": "Peaks-Over-Threshold (POT) Modelling",
    "section": "8.1 Non-parametric approach",
    "text": "8.1 Non-parametric approach\nAs for the block maxima approach (see trend detection section), we can also apply the non-parametric Mann-Kendall trend test for monotonic trend detection. See the Mann-Kendall section for the complete algorithm of the test. In the chunk below, we use the mk.test(...) function from the {trend} library to perform the trend test.\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.5059: there is not enough statistical evidence to conclude that a significant trend exists in the threshold exceedances serie.\ntau = -3.041649e-02: indicates a weak or no trend.\nz = -0.66528: suggests a potential decreasing trend, but the trend is not significant.\n\n\n\n# perform Mann-Kendall trend test\nmktest = trend::mk.test(x = peaks_final$Q)\nprint(mktest)\n\n\n    Mann-Kendall trend test\n\ndata:  peaks_final$Q\nz = -0.66528, n = 219, p-value = 0.5059\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n-7.220000e+02  1.174512e+06 -3.041649e-02 \n\n\nThe snippet below computes the absolute and relative Sen‚Äôs slope, to quantify the magnitude of trend in the POT data as both a raw rate of change and a percentage relative to the mean. See the Sen‚Äôs slope section for the complete algorithm and the user-defined function sens_slope_percent(...) we apply below.\n\n# compute Sen's slope as a percentage change\nsens_slope_percent(peaks_final$Q)\n\nAbsolute Sen's slope: -0.0017\nP-value: 0.5059\nRelative trend rate: -2.45%/\n\n\n\n\n\n\n\n\n\n\nRelative Sen‚Äôs slope\n\n\n\nThe threshold exceedance show a slight decreasing trend with a slope of -0.0017 mm/year, which is equivalent to a overall decrease of -2.45%% relative to its long-term average. Yet, this negative trend is not significative given the p-value of 0.5059."
  },
  {
    "objectID": "pot/index.html#parametric-approach-with-pot-modelling",
    "href": "pot/index.html#parametric-approach-with-pot-modelling",
    "title": "Peaks-Over-Threshold (POT) Modelling",
    "section": "8.2 Parametric approach with POT modelling",
    "text": "8.2 Parametric approach with POT modelling\n\n8.2.1 Fit a stationary GP distribution\nAs we have already seen in the previous section, the GP distribution is the most appropriate distribution to model threshold exceedances. As explained by Gilleland & Katz (2016) in their paper, an important additional consideration when using the POT approach is the rate of exceedance ‚Äî that is, how often values exceed the chosen threshold. While this rate does not affect the fitting of the GP model itself, it is crucial for calculating return levels, which require scaling the model to real-world time units.`\nThe exceedance rate should be passed the time.units argument in the fevd(...) function. Since our goal is to estimate annual return levels, we must specify the average number of threshold exceedances per year (i.e., the average number of flood events per year in our dataset).\nFigure¬†8 shows that the number of events per year varies ‚Äî some years experience multiple flood events, while others have none. Therefore, a reasonable approach is to use the empirical average number of exceedances per year over the entire study period as the value for time.units. The snippet below shows how to calculate this rate.\n\n# compute the average number of threshold exceedances per year\npeaks_final$year &lt;- year(peaks_final$Date)\nn_years &lt;- length(unique(peaks_final$year))\nn_events &lt;- nrow(peaks_final)\nunits &lt;- n_events / n_years\n\n# Optional: print it nicely\ncat(\"Average number of events per year:\", round(units, 2), \"\\n\")\n\nAverage number of events per year: 3.71 \n\n\nNow, we can fit our GP model to the POT data. The workflow is same as the block maxima models (GEV and Gumbel), and same tools can be used to estimates the GP parameters. In the chunk below, we use the {extRemes} library again to fit the GP distribution to our partial duration serie in a statinary context.\n\n# fit the stationary GP distribution to the PDS\nlibrary(extRemes)\nthres = 10\nx = peaks_final$Q\nsgp_fit = fevd(x, type=\"GP\", method=\"GMLE\", threshold=thres, time.units = paste0(units,\"/year\"))\n\n# show summary of the fit\npretty_fit_summary(sgp_fit, \"GP\")\n\n=== GP  Fit Summary ===\n\nEstimated Parameters:\n scale  shape \n3.6496 0.3099 \n\nModel Fit Criteria:\nLogLik: -570.53\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe only thing that change here, comared to the block maxima models, is the additional threshold value and the time.units arguments passed to the fevd(...) function, which is the same as the threshold used in the declustering process. The pretty_fit_summary(...) is defined in the block maxima section.\n\n\nFigure¬†10 displays the diagnostics from the GP distribution fitted to the partial duration serie: density plot of empirical data and fitted GP df (top-left), and quantiles from a sample drawn from the fitted GP against the empirical data quantiles with 95% confidence bands (top-right).\n\n# show diagnostic plots of the GP distribution fitting\npar(mfrow=c(1,2))\nplot(sgp_fit, type=\"density\", main=\"(a) Density plot\")\nplot(sgp_fit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†10: Diagnostic plots for the General Pareto (GP) distribution fitted to the threshold exceedances serie in a stationary context.\n\n\n\n\n\n\n\n8.2.2 Fit a non-stationary GP distribution\nAs for the block maxima models, one way to account for non-stationary is to incorporate covariates within the parameters of the distributions in a regression-like manner. To perform parametric trend detection with the GP distribution, we model the scale parameter (œÉ) as a function of time, transforming the model into a non-stationary GP. The scale parameter of the non-stationnary GP distribution is computed as follow: \n\\sigma (t) = \\sigma 0 + \\sigma 1¬∑t \\tag{2}\n where \\sigma is the time-dependant scale parameter of the GP distribution, t is the time index, \\sigma 0 is the intercept, and \\sigma 1 is the slope.\nThe chunk below use the {extRemes} library to fit the Non-Stationary GP (NS-GP) distribution to our Partial Duration Serie extracted earlier. Unfortunately, most of the extRemes diagnostic plots currently are not available for non-stationary POT models.\n\n# set threshold\nthres = 10\n\n# use time index for non-stationary context fitting\npeaks_final$scale_cov = 1:nrow(peaks_final)\n\n# fit the GP distribution to the PDS\nPDS = data.frame(peaks_final)\nnsgp_fit = fevd(x=Q, scale.fun=~scale_cov, data=PDS, type=\"GP\", \n                method=\"GMLE\", threshold=thres, time.units=paste0(units,\"/year\"))\n\n# show summary of the fit\npretty_fit_summary(nsgp_fit, \"Non-Stationary GP\")\n\n=== Non-Stationary GP  Fit Summary ===\n\nEstimated Parameters:\n sigma0  sigma1   shape \n 4.1064 -0.0040  0.3066 \n\nModel Fit Criteria:\nLogLik: -570.21\n\n\n\n\n\n\n\n\n\n\nNS-GP parameters\n\n\n\nsigma0 = œÉ0 = 4.1064: is the intercept of the time-varying GP scale parameter. It represents the base scale of the distribution (often the start of the time series).\nsigma1 = œÉ1 = -0.0040: is the slope of the scale parameter with respect to time. This negative value suggests that extreme values are becoming less variable (narrower tail) over time.\nshape = 0.3066: controls the heaviness of the tail of the GP distribution. The positive value (Œæ &gt; 0) suggests a heavy (Fr√©chet) tail (i.e., the probability of very large exceedances decays slowly).\n\n\n\n\n8.2.3 Deviance test\nThe non-parametric Mann-Kendall test indicates a non-significant trend in the POT values. With parametric methods, the significance of a temporal trend is based on a deviance test between a non-stationary (or more complex) model and a stationary one. The deviance test, or Likelihood Ratio (LR) test, compares two nested models ‚Äî a simpler (stationary) model and a more complex (non-stationary) model.\nThe null hypothesis of the deviance test is that the simpler model is sufficient ‚Äî i.e., no trend (e.g., the GP scale parameter is constant over time). See the Deviance test Section for the mathematical details.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe try just to answer this question: Does the more complex non-stationary GP model provide a significantly better fit to the data than the simpler stationary GP model?\n\n\nTo perform the Deviance test, we can use the lr.test(...) of the {extRemes}. This function just requires the previous fitted objects of class ‚Äúfevd‚Äù from the fevd(...) function:\n\nsgp_fit: model with fewer parameters (stationary GP model)\nnsgp_fitmodel with more parameters (non-stationary GP model)\n\n\n# applying the Deviance test\nlr.test(x=sgp_fit, y=nsgp_fit, alpha=0.05, df=1)\n\n\n    Likelihood-ratio Test\n\ndata:  xQ\nLikelihood-ratio = 0.64953, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.4203\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nüö´ We fail to reject the null hypothesis.\n\nD-statistic (0.65) much smaller than the critical value (3.84).\np-value (0.4203) much larger than the significance level (0.05)."
  },
  {
    "objectID": "pot/index.html#stationary-context",
    "href": "pot/index.html#stationary-context",
    "title": "Peaks-Over-Threshold (POT) Modelling",
    "section": "10.1 Stationary context",
    "text": "10.1 Stationary context\nTo estimate return levels for given return periods, we can use the return.level(...) from the {extRemes} package:\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe return level is the value expected to be exceeded on average once every T years.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(sgp_fit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n\nfevd(x = x, threshold = thres, type = \"GP\", method = \"GMLE\", \n    time.units = paste0(units, \"/year\"))\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI Estimate 95% upper CI\n2-year return level       18.33132 20.14264     21.95396\n5-year return level       23.30110 27.34035     31.37960\n10-year return level      27.12648 34.31731     41.50814\n20-year return level      30.77529 42.96605     55.15681\n50-year return level      34.81016 57.65826     80.50637\n100-year return level     36.68461 71.89985    107.11510\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sgp_fit object is the object returned by the fevd(...) function when we have fitted a the stationary GP model."
  },
  {
    "objectID": "pot/index.html#non-stationary-context",
    "href": "pot/index.html#non-stationary-context",
    "title": "Peaks-Over-Threshold (POT) Modelling",
    "section": "10.2 Non-Stationary context",
    "text": "10.2 Non-Stationary context\nIn non-stationary context, since the scale parameter is time-dependent, return levels have to be calculated for each time step. We refer to this as ‚Äúeffective‚Äù return levelsThe same return.level(...) from the {extRemes} package is used to compute \"effective\" return levels.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe nsgp_fit object is the object returned by the fevd(...) function when we have fitted a the non-stationary GP model.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgp_fit, return.period=rperiods)\n\nFor easy visualization, we will plot the effective return levels. Lets convert the output of return.level(...) into a clean data.frame :\n\n# Convert estimates to data.frame\ndf_rlevels &lt;- data.frame(time=1:nrow(peaks_final),\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n\n  time       Q2       Q5      Q10      Q20      Q50     Q100\n1    1 21.36000 29.38540 37.14447 46.74100 63.00002 78.71949\n2    2 21.34887 29.36640 37.11788 46.70500 62.94809 78.65216\n3    3 21.33774 29.34741 37.09128 46.66901 62.89617 78.58484\n4    4 21.32661 29.32842 37.06469 46.63301 62.84424 78.51751\n5    5 21.31549 29.30943 37.03810 46.59702 62.79232 78.45019\n6    6 21.30436 29.29044 37.01150 46.56102 62.74039 78.38286\n\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with ggplot:\n\ndataplot = df_rlevels |&gt; \n           pivot_longer(-1, names_to=\"RL\", values_to=\"Q\") |&gt;\n           # re-index\n           mutate(RL = factor(RL, levels=rev(paste0(\"Q\", c(2, 5,10,20,50,100)))))\n\nFinally, we plot the effective return levels:\n\nrlplot = dataplot |&gt;\n         ggplot(aes(x=time, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n\nFigure¬†11 helps in understanding the variations and trends in flood quantiles across different time periods.‚Äù\n\n\n\n\n\n\n\n\nFigure¬†11: Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles."
  },
  {
    "objectID": "overview/index.html",
    "href": "overview/index.html",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "Imagine we have daily observational data for a random variable (e.g., river discharge) for many years. EVT suggests that the extreme values of this variable are asymptotically close to one of three types of extreme value distributions, regardless of the original distribution of daily observations (see Coles (2001)). EVT helps us estimate the probability of rare, high-flow events like floods.\n\n\n\n\n\n\n\n\n\n(a) Normal distribution of the random variable\n\n\n\n\n\n\n\n(b) Sampling distribution of the mean\n\n\n\n\n\n\n\n(c) Extreme value distribution (maxima)\n\n\n\n\n\n\nIllustration of the Extreme Value Theory (EVT)"
  },
  {
    "objectID": "overview/index.html#eva-in-hydrology",
    "href": "overview/index.html#eva-in-hydrology",
    "title": "Overview, Key Concepts and Objectives",
    "section": "2.1 üåä EVA in Hydrology",
    "text": "2.1 üåä EVA in Hydrology\nEVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIndependance (absence of autocorrelation) and Stationarity of extreme value time series are the underlying assumptions of EVA."
  },
  {
    "objectID": "overview/index.html#key-concepts-of-eva",
    "href": "overview/index.html#key-concepts-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "2.2 üìå Key Concepts of EVA",
    "text": "2.2 üìå Key Concepts of EVA\n\n2.2.1 üìà Block Maxima Approach\nSelects the maximum value in fixed time blocks (e.g., annual maximum flow or rainfall) ‚Üí Fits data to distributions like Gumbel or GEV.\n\n\n2.2.2 üìâ Peak Over Threshold (POT) Method\nConsiders all events exceeding a predefined threshold ‚Üí More data points ‚Üí Fitted to Generalized Pareto Distribution (GPD)."
  },
  {
    "objectID": "overview/index.html#objectives-of-eva",
    "href": "overview/index.html#objectives-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "2.3 üéØ Objectives of EVA",
    "text": "2.3 üéØ Objectives of EVA\nüîÅ Estimate Return Periods: Assess the intensity of rare events expected to be exceeded, on average, once every N years (e.g., for example, a 100-year event corresponds to a 1% annual probability of being exceeded).\nüìä Construct Frequency Curves: Fit statistical distributions (e.g., GEV, Gumbel, Log-Pearson Type III) to observed extremes to relate event magnitude to exceedance probability.\nüß∞ Support Risk-Based Design: Inform engineering design criteria (e.g., for dams, levees, drainage systems) based on scientifically justified extreme thresholds"
  },
  {
    "objectID": "overview/index.html#real-world-applications-of-eva",
    "href": "overview/index.html#real-world-applications-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "2.4 üèóÔ∏è Real-world Applications of EVA",
    "text": "2.4 üèóÔ∏è Real-world Applications of EVA\nüèûÔ∏è Infrastructure Design: Design dams, bridges, culverts based on expected extreme flows.\nüíß Water Resource Management: Plan for droughts, reservoir operations, and sustainable use.\n‚ö†Ô∏è Risk Assessment & Planning: Develop floodplain zoning, disaster risk reduction strategies.\nüß≠ Policy & Climate Adaptation: Inform government strategies and early warning systems in a changing climate."
  },
  {
    "objectID": "softwaresetup/index.html",
    "href": "softwaresetup/index.html",
    "title": "Data, Software, Setup and Libraries (Packages)",
    "section": "",
    "text": "The data used in this practical are available in the GitHub repository associated with this project. We will be working with daily streamflow records from the La Dr√¥me √† Luc-en-Diois hydrometric station in France. The metadata of the station are available on the HydroPortail website (here). The dataset spans from August 1st, 1958, to December 31st, 2020, and will serve as the basis for exploring flood frequency analysis techniques.\n\n\n\n\n\n\nDownload the Data\n\n\n\nüì• Download CSV"
  },
  {
    "objectID": "softwaresetup/index.html#what-is-r",
    "href": "softwaresetup/index.html#what-is-r",
    "title": "Data, Software, Setup and Libraries (Packages)",
    "section": "2.1 üß† What is R?",
    "text": "2.1 üß† What is R?\nR is a programming language and software environment designed primarily for statistical computing, data analysis, and visualization.\n\n\n\n\n\n\nFigure¬†1: Some of the key advantages of learning the R programming language. (Source: C# Corner)"
  },
  {
    "objectID": "softwaresetup/index.html#what-is-rstudio",
    "href": "softwaresetup/index.html#what-is-rstudio",
    "title": "Data, Software, Setup and Libraries (Packages)",
    "section": "2.2 üñ•Ô∏è What is RStudio?",
    "text": "2.2 üñ•Ô∏è What is RStudio?\nRStudio is an IDE (Integrated Development Environment) specifically made for working with R. It provides a much more convenient and powerful interface than the basic R console:\n\nüìù Script editor with syntax highlighting and auto-completion\nüíª Console for interactive execution\nüì¶ Environment viewer to see your variables, functions, and data\nüìä Plot viewer, üóÇÔ∏è file browser, üìñ help pane, and more\n‚öôÔ∏è Easy access to package management, üîÑ version control, and üìÑ R Markdown / Quarto\n\n\n\n\n\n\n\nFigure¬†2: Overview of the four main panes in the RStudio interface. Source pane (top-left): where scripts and R Markdown files are written and edited. Console pane (bottom-left): where R commands are executed interactively. Environment pane (top-right): displays the current workspace, including variables and data. Output pane (bottom-right): shows plots, files, packages, help pages, and more. (Source: Posit)"
  },
  {
    "objectID": "softwaresetup/index.html#how-to-use-them-together",
    "href": "softwaresetup/index.html#how-to-use-them-together",
    "title": "Data, Software, Setup and Libraries (Packages)",
    "section": "2.3 üîß How to use them together?",
    "text": "2.3 üîß How to use them together?\n1- Install R ‚Üí https://cran.r-project.org\n2- Install RStudio ‚Üí https://posit.co/download/rstudio-desktop/\n3- Then, open RStudio ‚Üí it will automatically use the installed R engine in the background.\n\n\n\n\n\n\nFigure¬†3: R environment, the engine powering your data analysis, and RStudio, the dashboard and controls that make driving (coding) efficient and user-friendly. (Source: umanitoba.ca)"
  },
  {
    "objectID": "blockmaxima/index.html",
    "href": "blockmaxima/index.html",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)"
  },
  {
    "objectID": "blockmaxima/index.html#the-block-maxima-approach",
    "href": "blockmaxima/index.html#the-block-maxima-approach",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.1 The block maxima approach",
    "text": "3.1 The block maxima approach\nThe Block-Maxima Approach (Gumbel, 1958), consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and reduce the possiblity of serial dependence.\n\n\n\n\n\n\n\n\nFigure¬†1: Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima"
  },
  {
    "objectID": "blockmaxima/index.html#exract-annual-maximum-floods-amf-from-the-daily-streamflow-serie",
    "href": "blockmaxima/index.html#exract-annual-maximum-floods-amf-from-the-daily-streamflow-serie",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.2 Exract annual maximum floods (AMF) from the daily streamflow serie",
    "text": "3.2 Exract annual maximum floods (AMF) from the daily streamflow serie\n1. To extract annual maximum floods (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\nextract_annual_max &lt;- function(df) {\n    df %&gt;%\n        mutate(Year = year(Date)) %&gt;% # extract year from Date\n        group_by(Year) %&gt;%\n        filter(Q == max(Q)) %&gt;% # keep only max flow per year\n        slice(1) %&gt;% # in case of ties, keep first max\n        ungroup() %&gt;%\n        select(Year, Date, AMAX = Q)\n}\n\n2. Now, we apply the function to the data to extract AMF for each year:\n\nextractedAMF &lt;- extract_annual_max(Qdata)\n\n\n\n\n\nTable¬†2: First (6) rows of the AMF extracted using the Block-Maxima approach\n\n\n\n\n\n\nYear\nDate\nAMAX\n\n\n\n\n1958\n1958-12-20\n39.96984\n\n\n1959\n1959-03-07\n36.89593\n\n\n1960\n1960-10-06\n58.41780\n\n\n1961\n1961-12-11\n18.44796\n\n\n1962\n1962-03-05\n19.47185\n\n\n1963\n1963-04-11\n29.21002"
  },
  {
    "objectID": "blockmaxima/index.html#display-sampled-vvents",
    "href": "blockmaxima/index.html#display-sampled-vvents",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.3 Display sampled vvents",
    "text": "3.3 Display sampled vvents\nNow, we overlap the extracted Annual Maximum Flow with the original daily discharge time serie to visualize the sampling process. We use the functions from the dplyr and ggplot2 library to create the plot.\n\n\nShow the code\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach"
  },
  {
    "objectID": "blockmaxima/index.html#sec-autocorrelation",
    "href": "blockmaxima/index.html#sec-autocorrelation",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "4.1 What is autocorrelation?",
    "text": "4.1 What is autocorrelation?\nAutocorrelation (or autocovariance) refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ‚Ä¶). An autocorrelated series is thus correlated with itself, with a given lag (please refer here for more details).\n\n\n\n\n\n\nTo assess autocorrelation, two methods will be used in this lab\n\n\n\nüîç Correlogram:\n\nVisualization of correlation coefficients between the serie and its lags\nBased on the autocorrelation function (ACF)\nHelps identify the presence and extent of dependencies over time\n\nüìä Wald-Wolfowitz Test (Wald & Wolfowitz, 1940):\n\nNon-parametric to assess the randomness of a time serie\nUsed to detect the absence of randomness in the serie\nNull hypothesis: The sequence of observations is random\n\n\n\n\n4.1.1 Correlogram\nIf you execute the above given chunk, it generates the Figure¬†3 as output:\n\n\nShow the code\n# compute autocorrelation lag\nbacf &lt;- acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf &lt;- with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn &lt;- length(extractedAMF$AMAX)\nconf_limit &lt;- 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +\n    geom_hline(aes(yintercept = 0)) +\n    geom_bar(stat = \"identity\", position = \"identity\", width = .2) +\n    geom_hline(yintercept = conf_limit, linetype = \"dashed\", color = \"red\") +\n    geom_hline(yintercept = -conf_limit, linetype = \"dashed\", color = \"red\") +\n    labs(y = \"ACF\", x = \"Lag\") +\n    theme_minimal() +\n    theme(\n        axis.title = element_text(size = 10, face = \"bold\", family = \"Times\"),\n        axis.text = element_text(size = 10, color = \"black\", family = \"Times\")\n    )\n\n\n\n\n\n\n\n\nFigure¬†3: Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n\n\n\n\n4.1.2 Wald-Wolfowitz (WW) test\nIn the chunk below, we use the ww.test(...) function from {trend} package to perform the WW test at the 0.05 significance level, to test the hypothesis that the maximum annual flood is a random variable.\n\nwwtest &lt;- trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n\n\n    Wald-Wolfowitz test for independence and stationarity\n\ndata:  extractedAMF$AMAX\nz = 0.73738, n = 60, p-value = 0.4609\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.4609: indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness)."
  },
  {
    "objectID": "blockmaxima/index.html#non-parametric-methods",
    "href": "blockmaxima/index.html#non-parametric-methods",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "5.1 Non-parametric methods",
    "text": "5.1 Non-parametric methods\n\n5.1.1 The Mann-Kendall (MK) test\nThe MK test‚Äôs null hypothesis (H0) assumes no trend in the data (Mann, 1945; Kendall, 1975). Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results. The Mann-Kendall test statistic S is defined as:\n\nS = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\text{sgn}(x_j - x_i) \\tag{1}\n\nThe sign function sgn()) is given by:\n\n\\text{sgn}(x_j - x_i) =\n\\begin{cases}\n+1 & \\text{if } (x_j - x_i) &gt; 0 \\\\\n0 & \\text{if } (x_j - x_i) = 0 \\\\\n-1 & \\text{if } (x_j - x_i) &lt; 0\n\\end{cases} \\tag{2}\n\nwhere x_i and x_j are values of the variable at time i and j, respectively, and n is the total number of observations.\nA significance level (commonly \\alpha = 0.05) and the trend slope indicating the direction and magnitude of the trend are key characteristics of the Mann-Kendall test. If n &gt; 8, the test statistic S approximates a normal distribution. In this case, the mean of S is zero and its variance is:\n\n\\text{var}(S) = \\frac{n(n - 1)(2n + 5)}{18} \\tag{3}\n\nThe standardized test statistic Z, which indicates the direction of the trend, is calculated as:\n\nZ =\n\\begin{cases}\n\\frac{S - 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S &gt; 0 \\\\\n0 & \\text{if } S = 0 \\\\\n\\frac{S + 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S &lt; 0\n\\end{cases} \\tag{4}\n\nThe null hypothesis H_0 (no trend) is rejected if |Z| &gt; 1.96 at the 95% confidence level.\n\nA positive trend is significant if Z &gt; 1.96\nA negative trend is significant if Z &lt; -1.96\n\nWe use the mk.test(...) function from {trend} package to perform the MK test at the 0.05 significance level to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNon-Stationarity (NS) refers to changes in statistical properties over time.\n\n\n\nmktest &lt;- trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n\n\n    Mann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.2183: there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\ntau = -0.1096665: Kendall‚Äôs Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\nz = -1.231: the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n\n\nFigure¬†4 shows the evolution of the AMF over time and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\nShow the code\nlineplot &lt;- ggplot(extractedAMF, aes(Year, AMAX)) +\n    geom_line(color = \"blue\", linewidth = 1) +\n    geom_smooth(color = \"red\", linewidth = 1, method = \"lm\", se = F)\nprint(lineplot)\n\n\n\n\n\n\n\n\nFigure¬†4: Evolution of the annual peak flood over time.\n\n\n\n\n\n\n\n5.1.2 Sen‚Äôs slope estimator\nThe non-parametric method of Sen (1968), also referred to as the Theil‚ÄìSen estimator, is commonly combined with the Mann-Kendall test to estimate the slope (\\beta) of the of a trend. Sen‚Äôs algorithm looks at every pair of data points, calculates the slope between them, then uses the the median of all those slopes as the estimated trend.\n\n\\beta = \\text{median} \\left( \\frac{x_j - x_i}{j - i} \\right), \\quad , ‚àÄ\\quad j &gt; i \\tag{5}\n\nTo express the Sen‚Äôs slope as a percentage change over the period, it can be normalized by the mean:\n\n\\%\\beta = \\left( N \\cdot \\frac{\\beta}{\\bar{x}} \\right) \\times 100 \\tag{6}\n where \\%\\beta is the relative trend rate over the period, and \\bar{x} represents the mean value of the variable across the period, N is the number of data points (i.e., the length of period).\nThe Sen‚Äôs slope can be easily computed using the sens.slope(...) function from the {trend} package, as in the chunk below:\n\n\n\n\n\n\n\n\nSen slope\n\n\n\nSlope = -0.1187375 : the median trend is slightly negative, suggesting a weak decline of -0.11 \\quad mm/year.\np-value = 0.2183: this negative trend is not statistically significant.\n\n\n\n# compute Sen's slope and intercept\nsenslope &lt;- trend::sens.slope(x = extractedAMF$AMAX)\nprint(senslope)\n\n\n    Sen's slope\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true z is not equal to 0\n95 percent confidence interval:\n -0.3115676  0.0768529\nsample estimates:\nSen's slope \n -0.1187375 \n\n\nYet, the sens.slope(...) function returns the absolute slope (i.e., the median change per time unit). It is more meaningful to express the trend as a relative percentage change over the time period.\nTo do this, we define a wrapper function that calculates the Sen‚Äôs slope and then converts it into a relative trend rate.\n\n\nShow the code\nsens_slope_percent &lt;- function(x, time_step = NULL) {\n    #' Compute Sen's slope as a percentage change over the full time period\n    #' @param x A numeric vector of observations\n    #' @param time_step Optional (e.g., year) used for display purpose only\n    #' @return Relative percent change over the period\n\n    require(trend)\n\n    N &lt;- length(x)\n    result &lt;- sens.slope(x)\n    slope &lt;- result$estimates # Absolute slope (per time unit)\n    p_val &lt;- result$p.value # p-value for trend test\n    mean_val &lt;- mean(x, na.rm = TRUE)\n\n    percent_change &lt;- (N * slope) / mean_val * 100\n\n    cat(\"Absolute Sen's slope: \", round(slope, 4), \"\\n\", sep = \"\")\n    cat(\"P-value: \", format.pval(p_val, digits = 4), \"\\n\", sep = \"\")\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%/\", \"\\n\", sep = \"\")\n}\n\n\nNow, we can compute the relative Sen‚Äôs slope for the annual maximum flow data:\n\n# compute Sen's slope as a percentage change\nsens_slope_percent(extractedAMF$AMAX, time_step = \"year\")\n\nAbsolute Sen's slope: -0.1187\nP-value: 0.2183\nRelative trend rate: -23.97%/\n\n\n\n\n\n\n\n\n\n\nRelative Sen‚Äôs slope\n\n\n\nThe annual maximum flow shows a decreasing trend of ‚àí24% relative to its long-term average."
  },
  {
    "objectID": "blockmaxima/index.html#parametric-methods",
    "href": "blockmaxima/index.html#parametric-methods",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "5.2 Parametric methods",
    "text": "5.2 Parametric methods\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a non-stationary (NS) context.\n\n\n\n\n\n\nNote\n\n\n\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter (Œº) will be expressed as a linear function of time, denoted as Œº(t), leaving the others parameters constant."
  },
  {
    "objectID": "blockmaxima/index.html#probability-distributions",
    "href": "blockmaxima/index.html#probability-distributions",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.1 Probability distributions",
    "text": "6.1 Probability distributions\nWe will compare two probability distributions adapted to the Block-Maxima Framework: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n\n\n\n\n\nGEV distribution\n\n\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n\nF(x; \\mu, \\alpha, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\alpha} \\right) \\right]^{-1/\\xi} \\right\\}\n\\tag{7}\n\nfor 1 + \\xi (x - \\mu)/\\alpha &gt; 0, where, \\mu is the location parameter, \\alpha &gt; 0 is the scale parameter, and \\xi is the shape parameter.\nThe shape parameter \\xi determines the type of extreme value distribution:\n\nIf \\xi &gt; 0, the distribution is a Fr√©chet type.\nIf \\xi &lt; 0, the distribution is a Weibull type.\nIf \\xi = 0, the distribution is a Gumbel type.\n\n\n\n\n\n\n\n\n\nGumbel distribution\n\n\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter \\xi = 0. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. The cumulative distribution function (CDF) of the Gumbel distribution is given by:\n\nF(x; \\mu, \\alpha) = \\exp\\left\\{ -\\exp\\left[ -\\frac{x - \\mu}{\\alpha} \\right] \\right\\}\n\\tag{8}\n where \\mu is the location parameter, \\alpha &gt; 0 is the scale parameter, and x is the variable"
  },
  {
    "objectID": "blockmaxima/index.html#models-fitting",
    "href": "blockmaxima/index.html#models-fitting",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.2 Models fitting",
    "text": "6.2 Models fitting\nWe will employ the Generalized Maximum Likelihood Estimation (GMLE) method to fit probability distributions to our data. The {extRemes} package offers the function fevd(..), which enables fitting a range of probability distributions ‚Äî such as the GEV, Gumbel distributions ‚Äî to a given dataset using three distinct fitting methods: L-moments, Maximum Likelihood Estimation (MLE), and GMLE.\n\n\n\n\n\n\n\n\nWhy the GML method?\n\n\n\nThe GMLE method use a prior distribution to constrain the GEVC shape parameter within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See Martins & Stedinger (2000) for more details).\n\n\nThe advantage of using GMLE and MLE over the L-moments method is that L-moments are limited in their ability to accommodate non-stationary processes. In contrast, GMLE and MLE can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection."
  },
  {
    "objectID": "blockmaxima/index.html#model-selection",
    "href": "blockmaxima/index.html#model-selection",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.3 Model selection",
    "text": "6.3 Model selection\nTo select the best probability distribution, we will use the AIC and AIC criteria. The model with the lowest AIC and BIC values is the best-suited model. AIC tends to favor models with better fit, even if more complex, while BIC favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection.\n\n\n\n\n\n\n\n\nJoint use of AIC and BIC\n\n\n\n\nif both criteria select the same model, it is strongly preferred\nif not, the choice depends on whether your goal is predictive accuracy (AIC) or model simplicity (BIC)\n\n\n\n\n\n\n\n\n\nAIC (Akaike Information Criterion)\n\n\n\nThe AIC balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n\n\\text{AIC} = 2k - 2\\ln(\\hat{L}) \\tag{9}\n where k is the number of parameters in the model, \\hat{L} is the maximum value of the likelihood function for the model.\n\n\n\n\n\n\n\n\nBIC (Bayesian Information Criterion)\n\n\n\nBIC penalizes model complexity more heavily than AIC, especially for larger sample sizes. This means BIC tends to favor simpler models compared to AIC. The formula for BIC is: \n\\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \\tag{10}\n where: k is the number of parameters in the model, n is the number of data points, and \\hat{L} is the maximum value of the likelihood function for the model."
  },
  {
    "objectID": "blockmaxima/index.html#perform-ffa",
    "href": "blockmaxima/index.html#perform-ffa",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.4 Perform FFA",
    "text": "6.4 Perform FFA\n\n6.4.1 Setup utils\nTo pretty-print the summary of our models‚Äô fitting, we will create a custom function that will take the fitted model as an argument, extract and format key components of the fiited object, and print the output:\n\npretty_fit_summary = function(fitobj, model) {\n  #' Pretty print summary of a  extreme value model fit\n  #'\n  #' @param fitobj Fitted model object from extRemes::fevd()\n  #' @param model Character string describing the model (e.g., \"GEV\")\n  \n  cat(\"===\", model, \" Fit Summary ===\\n\\n\")\n  \n  # Estimated Parameters\n  cat(\"Estimated Parameters:\\n\")\n  params = fitobj$results$par\n  print(round(params, 4))\n  \n  # Log-likelihood, AIC, and BIC\n  look = summary(fitobj, silent=TRUE)\n  cat(\"\\nModel Fit Criteria:\\n\")\n  cat(sprintf(\"LogLik: %.2f\\n\", look$nllh))\n  cat(sprintf(\"AIC: %.2f\\n\", look$AIC))\n  cat(sprintf(\"BIC: %.2f\\n\", look$BIC))\n\n  # return the AIC and BIC values for further comparison\n  return(data.frame(Model=model, Crit=c(\"AIC\", \"BIC\"), Value=c(look$AIC, look$BIC)))\n}\n\n\n\n6.4.2 Distributions fitting\n\n6.4.2.1 Stationary context\n\n\n\n\n\n\n\nWhat about the parameters?\n\n\n\nlocation = 19.4868: is the central tendency (Œº) of the GEV distribution. It defines the central position of the distribution and serves as a baseline around which extreme values are distributed.\nscale = 9.932: reflects the spread or variability of the extreme values. In our case, the AMF distribution has moderate spread around the shifting central value Œº(t).\nshape = 0.3576: this positive shape parameter implies a Fr√©chet-type (heavy-tailed) distribution. This means there is greater probability of very large flood events compared to a Gumbel (shape = 0) or Weibull (shape &lt; 0) distribution.\n\n\n\n\n\n\n\n\n\n‚úÖ Best-suited: GEV\n\n\n\nBoth AIC and BIC are lower for the GEV model, which suggests that it provides a better balance of fit and complexity than the Gumbel model.\n\n\n\nGEV fittingGumbel fittingBest-suited model\n\n\nThe chunk below use the {extRemes} library to fit the GEV distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the GEV distribution to the AMF\nlibrary(extRemes)\ngevfit = fevd(x=extractedAMF$AMAX, type=\"GEV\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_fit_summary() function we have defined above. This will show the AIC and BIC values, as well as the estimated parameters of the fitted GEV distribution.\n\n# show fitting sumary\ngevfit_res = pretty_fit_summary(gevfit, \"GEV\")\n\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale    shape \n 19.4868   9.8254   0.3818 \n\nModel Fit Criteria:\nLogLik: 244.65\nAIC: 495.31\nBIC: 501.59\n\n\n\nFinally, we show in Figure¬†5, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), and quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .043))\nplot(gevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†5: Diagnostic plots for the Generalized Extreme Value (GEV) distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\n\n\n\n\n\n\n\nThe chunk below use the {extRemes} library to fit the Gumbel distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the Gumbel distribution to the AMF\ngumfit = fevd(x=extractedAMF$AMAX, type=\"Gumbel\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_fit_summary() function we have defined previously. This will show the AIC and BIC values, as well as the estimated parameters of the fitted Gumbel distribution.\n\n# show fitting sumary\ngumfit_res = pretty_fit_summary(gumfit, \"Gumbel\")\n\n=== Gumbel  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale \n 21.7606  12.3003 \n\nModel Fit Criteria:\nLogLik: 249.40\nAIC: 502.80\nBIC: 506.99\n\n\nFinally, we show in Figure¬†6, the diagnostics from the Gumbel distribution fitted to the Annual Maximum Floods:\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gumfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .035))\nplot(gumfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†6: Diagnostic plots for the Gumbel distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\n\n\n\n\n\n\n\nAs mentionned previously, the model with the lowest AIC and BIC values is the best-suited model. The pretty_fit_summary() function we have defined previously return a data.frame of AIC and BIC values for each distribution. Lets compare these values to select the best-suited distribution for the next steps.\n\nprint(gevfit_res)\nprint(gumfit_res)\n\n\n\n\nTable¬†3: Table of AIC and BIC values for each distribution\n\n\n\n\n\n\n\n(a) GEV\n\n\n\n\n\nModel\nCrit\nValue\n\n\n\n\nGEV\nAIC\n495.3076\n\n\nGEV\nBIC\n501.5907\n\n\n\n\n\n\n\n\n\n\n\n(b) Gumbel\n\n\n\n\n\nModel\nCrit\nValue\n\n\n\n\nGumbel\nAIC\n502.7966\n\n\nGumbel\nBIC\n506.9853\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2.2 Parametric trend-detection\nWhen covariates are to be incorporated into EVA, a common approach to account for non-stationary is to incorporate covariates within the parameters of the distributions in a regression-like manner. To perform parametric trend detection, we model the location parameter (Œº) as a function of time, transforming the models into non-stationary distributions. The location parameter of the non-stationnary GEV and Gumbel is computed as follow: \n\\mu (t) = \\mu 0 + \\mu 1¬∑t \\tag{11}\n where \\mu (t) is the time-dependant location parameter, t is the time index, \\mu 0 is the intercept, and \\mu 1 is the slope.\n\n\n\n\n\n\nTrend?\n\n\n\nThe decision to adopt a non-stationary (or more complex) model in FFA depends on whether the temporal trend is statistically significant.\nThe deviance test, based on likelihood ratio statistic (Coles, 2001), offers a formal approach to test this hypothesis by comparing the stationary and non-stationary models.\n\n\n\n6.4.2.2.1 Creating covariate\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs mentionned above, we use time index to account for trend in the AMF serie. We create in this chunk above a new column, TimeIndex, to be used as covariate.\n\n\n\n# add time index to use for non-stationary context fitting\nextractedAMF[[\"TimeIndex\"]] = 1:nrow(extractedAMF)\nhead(tibble(extractedAMF), 5)\n\n# A tibble: 5 √ó 4\n   Year Date        AMAX TimeIndex\n  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;int&gt;\n1  1958 1958-12-20  40.0         1\n2  1959 1959-03-07  36.9         2\n3  1960 1960-10-06  58.4         3\n4  1961 1961-12-11  18.4         4\n5  1962 1962-03-05  19.5         5\n\n\n\n\n6.4.2.2.2 Non-Stationary GEV fitting\nThe chunk below use the {extRemes} library to fit the Non-Stationary GEV (NS-GEV) distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the GEV distribution to the AMF\nextractedAMF = data.frame(extractedAMF)\nnsgevfit = fevd(x=AMAX, location.fun=~TimeIndex, data=extractedAMF, type=\"GEV\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_fit_summary() function we have defined above. This will show the AIC and BIC values, as well as the estimated parameters of the fitted NS-GEV distribution.\n\n# show fitting sumary\nnsgevfit_res = pretty_fit_summary(nsgevfit, \"GEV\")\n\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\n    mu0     mu1   scale   shape \n20.5045 -0.0276  9.9322  0.3576 \n\nModel Fit Criteria:\nLogLik: 244.55\nAIC: 497.11\nBIC: 505.48\n\n\n\n\n\n\n\n\n\n\nWhat about the parameters?\n\n\n\n\nmu0 = Œº0 = 20.5045: is the intercept of the time-varying location parameter. It represents the baseline magnitude of the annual maximum flood (AMF) at the starting point of the time serie.\nmu1 = Œº1 = ‚Äì0.0276: is the slope of the location parameter with respect to time. It indicates a decreasing trend in the location (i.e., central tendency) of the AMF over time. Specifically, for each unit increase in time (i.e., each year), the location parameter decreases by 0.0276\\,mm. This suggests that the typical magnitude of extreme floods has been gradually declining over the observation period.\n\n\n\nFinally, we show in Figure¬†7, the diagnostics from the NS-GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(nsgevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .4))\nplot(nsgevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†7: Diagnostic plots for the Non-Stationary Generalized Extreme Value (NS-GEV) distribution fitted to the Annual Maximum Flood (AMF) series\n\n\n\n\n\n\n\n6.4.2.2.3 Deviance test\n\n\n\n\n\n\nMathematical of the deviance test\n\n\n\nAs mentionned above, The Deviance test, or Likelihood Ratio (LR) test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit.\n\nD = 2{log(ML_{NSGEV}) - log(ML_{SGEV})}\n\\tag{12}\n\nD represents the deviance test statistic value (D-statistic), log(ML_{NSGEV}) and log(ML_{SGEV}) are the maximised log-likelihood functions of the NSGEV and the SGEV, respectively.\n\n\n\n\n\n\n\n\n\n\nlog-likelihood function\n\n\n\nThe likelihood function itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n\n\nTo perform the Deviance test, we can use the lr.test(...) of the {extRemes}. This function just requires the previous fitted objects of class ‚Äúfevd‚Äù from the fevd(...) function:\n\ngevfit: model with fewer parameters (stationary GEV model)\nnsgevfitmodel with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the Deviance test is that simpler model is sufficient ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting c.alpha be the (1 - alpha) quantile of the chi-square distribution with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that D = 0 is rejected if D &gt; c.alpha (i.e., in favor of model Non-Stationary GEV model).\n\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n\n\n    Likelihood-ratio Test\n\ndata:  extractedAMF$AMAXAMAX\nLikelihood-ratio = 0.20007, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.6547\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nüö´ We fail to reject the null hypothesis.\n\nD-statistic (0.20) much smaller than the critical value (3.84).\np-value (0.6547) much larger than the significance level (0.05).\n\n\n\n\n\n\n\n\n\nTrend significance\n\n\n\nThere is no significant evidence that the Non-Stationary GEV model (with a time-varying location parameter) provides a better fit than the stationary model. The trend is not statistically significant (p = 0.6547)."
  },
  {
    "objectID": "blockmaxima/index.html#stationary-context-1",
    "href": "blockmaxima/index.html#stationary-context-1",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "8.1 Stationary context",
    "text": "8.1 Stationary context\nTo estimate return levels for given return periods, we can use the return.level(...) from the {extRemes} package:\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe return level is the value expected to be exceeded on average once every T years.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(gevfit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n\nfevd(x = extractedAMF$AMAX, type = \"GEV\", method = \"GMLE\")\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI  Estimate 95% upper CI\n2-year return level       19.60029  23.35205     27.10382\n5-year return level       31.27812  39.37944     47.48076\n10-year return level      38.73868  54.51701     70.29534\n20-year return level      43.75679  73.73668    103.71657\n50-year return level      43.88864 107.90748    171.92631\n100-year return level     35.61747 142.77898    249.94049\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe gevfit object is the gevfit object returned by the fevd(...) function when we have fitted a the stationary GEV model.\n\n\n\n\n\n\n\n\n\n‚ö†Ô∏è Warning\n\n\n\nFor simplicity and illsutration, we use the normal approximation method to compute confidence intervals for the estimated return levels. This method assumes the sampling distribution of the return level estimates is approximately normal (which becomes more reasonable with large sample sizes).\nFor more robust intervals, consider using bootstrap methods."
  },
  {
    "objectID": "blockmaxima/index.html#non-stationary-context",
    "href": "blockmaxima/index.html#non-stationary-context",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "8.2 Non-Stationary context",
    "text": "8.2 Non-Stationary context\nIn non-stationary context, since the location parameter is time-dependent, return levels have to be calculated for each year over time period. We refer to this as ‚Äúeffective‚Äù return levels The same return.level(...) from the {extRemes} package is used to compute \"effective\" return levels.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe nsgevfit object is the object returned by the fevd(...) function when we have fitted a the non-stationary GEV model.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgevfit, return.period=rperiods)\n\nFor easy visualization, we will plot the effective return levels for each year over time period. Lets convert the output of return.level(...) into a clean data.frame :\n\n# Convert to data.frame\ndf_rlevels &lt;- data.frame(year=extractedAMF$Year,\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n\n  year       Q2       Q5      Q10      Q20      Q50     Q100\n1 1958 24.36654 40.19203 54.81061 73.04541 104.8170 136.6172\n2 1959 24.33894 40.16443 54.78301 73.01781 104.7894 136.5896\n3 1960 24.31134 40.13683 54.75541 72.99021 104.7618 136.5620\n4 1961 24.28374 40.10923 54.72781 72.96261 104.7342 136.5344\n5 1962 24.25614 40.08163 54.70021 72.93501 104.7066 136.5068\n6 1963 24.22854 40.05403 54.67261 72.90741 104.6790 136.4792\n\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with ggplot:\n\ndataplot = df_rlevels |&gt; pivot_longer(-1, names_to=\"RL\", values_to=\"Q\")\n\nFinally, we plot the effective return levels:\n\nrlplot = dataplot %&gt;%\n         ggplot(aes(x=year, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01), limits = c(20, 150), breaks=seq(25, 200, by=25)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time (Year)\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n\nFigure¬†8 helps in understanding the variations and trends in flood quantiles across different time periods.‚Äù\n\n\n\n\n\n\n\n\nFigure¬†8: Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Understanding the probabilistic behavior of extreme observations and tail phenomena in statistical distributions: application to hydrological data.\n\n\n\n\n\nResearch scientist at IRD since 2013, works on hydrological hazards in Mediterranean regions and climate change impacts.\n\n\nPhD researcher on flood modeling and climate change."
  },
  {
    "objectID": "index.html#yves-tramblay-orcid-icon",
    "href": "index.html#yves-tramblay-orcid-icon",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Research scientist at IRD since 2013, works on hydrological hazards in Mediterranean regions and climate change impacts.\n\n\nPhD researcher on flood modeling and climate change."
  },
  {
    "objectID": "index.html#serigne-bassirou-diop-orcid-icon",
    "href": "index.html#serigne-bassirou-diop-orcid-icon",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "PhD researcher on flood modeling and climate change."
  },
  {
    "objectID": "sampling_events/index.html",
    "href": "sampling_events/index.html",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blockmaxima/index.html#display-sampled-events",
    "href": "blockmaxima/index.html#display-sampled-events",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.3 Display sampled events",
    "text": "3.3 Display sampled events\nNow, we overlap the extracted Annual Maximum Flow with the original daily discharge time serie to visualize the sampling process. We use the functions from the dplyr and ggplot2 library to create the plot.\n\n\nShow the code\namf.plot &lt;- ggplot() +\n    geom_line(data = Qdata, aes(x = Date, y = Q), color = \"steelblue\", size = 0.7, alpha = 0.8) +\n    geom_point(data = extractedAMF, aes(x = Date, y = AMAX), color = \"red\", size = 1.2) +\n    labs(x = \"Date\", y = \"Discharge (Q)\") +\n    theme_minimal(base_size = 13) +\n    theme(\n        axis.title = element_text(size = 12, face = \"bold\"),\n        axis.text = element_text(size = 12, color = \"black\")\n    )\nprint(amf.plot)\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach"
  }
]