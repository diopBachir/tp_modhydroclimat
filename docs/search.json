[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Introduction to Extreme Value Analysis\nUnderstanding the probabilistic behavior of extreme observations and tail phenomena in statistical distributions: application to hydrological data.\n\n\n\n\nYves Tramblay \nResearch scientist at IRD since 2013. Works on hydrological hazards in Mediterranean regions and climate change impacts.\n[Espace-Dev, Univ. Montpellier, IRD, France]\n\n\nSerigne Bassirou Diop \nPhD researcher on flood modeling and climate change, with a strong interest in hydrology, statistical modelling, and machine learning.\n[Le√Ødi, Univ. Gaston Berger, Saint-Louis, Senegal]\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "sampling_events/index.html",
    "href": "sampling_events/index.html",
    "title": "Extreme Value Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "overview/index.html",
    "href": "overview/index.html",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "EVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events.\n\n\n\n\n\nSelects the maximum value in fixed time blocks (e.g., annual maximum flow or rainfall) ‚Üí Fits data to distributions like Gumbel or GEV.\n\n\n\nConsiders all events exceeding a predefined threshold ‚Üí More data points ‚Üí Fitted to Generalized Pareto Distribution (GPD).\n\n\n\n\nüîÅ Estimate Return Periods: Assess the intensity of rare events expected to be exceeded, on average, once every N years (e.g., for example, a 100-year event corresponds to a 1% annual probability of being exceeded).\nüìä Construct Frequency Curves: Fit statistical distributions (e.g., GEV, Gumbel, Log-Pearson Type III) to observed extremes to relate event magnitude to exceedance probability.\nüß∞ Support Risk-Based Design: Inform engineering design criteria (e.g., for dams, levees, drainage systems) based on scientifically justified extreme thresholds\n\n\n\nüèûÔ∏è Infrastructure Design: Design dams, bridges, culverts based on expected extreme flows.\nüíß Water Resource Management: Plan for droughts, reservoir operations, and sustainable use.\n‚ö†Ô∏è Risk Assessment & Planning: Develop floodplain zoning, disaster risk reduction strategies.\nüß≠ Policy & Climate Adaptation: Inform government strategies and early warning systems in a changing climate."
  },
  {
    "objectID": "overview/index.html#introduction",
    "href": "overview/index.html#introduction",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nFrequency analysis of floods is a statistical method used in hydrology to estimate the probability of occurrence of extreme flow events, such as floods, over a given period. It is typically based on the study of observed annual maximum flood discharges over several years. An alternative approach is the peak over threshold (POT) method, which considers all flood peaks that exceed a certain threshold, rather than only the annual maxima. This method allows for a more detailed analysis of extreme flood events by using more data points above the chosen threshold.\nThe objective is to determine the discharge associated with a return period, that is, the discharge that has a certain probability (e.g., 1%, 10%) of being exceeded in any given year. For example, a 100-year flood has a 1% chance of being exceeded in any year.\nTo achieve this, the data are fitted to a probability distribution (such as the Gumbel distribution), and a frequency curve is constructed linking discharges to return periods. This curve is then used to design hydraulic structures (dams, bridges, levees) and to manage flood risks.\nFrequency analysis thus allows for an objective assessment of flood-related risks and supports informed decision-making in land-use planning and flood protection."
  },
  {
    "objectID": "overview/index.html#extreme-value-theory-evt",
    "href": "overview/index.html#extreme-value-theory-evt",
    "title": "Overview",
    "section": "Extreme Value Theory (EVT)",
    "text": "Extreme Value Theory (EVT)\nImagine we have daily observational data for a random variable (e.g., river discharges, precipitations, etc.) for many years. EVT suggests that the extreme values of this variable are asymptotically close to one of three types of extreme value distributions, regardless of the original distribution of daily flows.\nThe EVD helps us estimate the probability of rare, high-flow events like floods.\n\n\n\n\nIllustration of the Extreme Value Theory (EVT)"
  },
  {
    "objectID": "pot/index.html",
    "href": "pot/index.html",
    "title": "Flood Frequency Analysis (FFA) Peak-Over-Threshold (POT)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blockmaxima/index.html",
    "href": "blockmaxima/index.html",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)"
  },
  {
    "objectID": "blockmaxima/block_maxima.html",
    "href": "blockmaxima/block_maxima.html",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "",
    "text": "Will work with real-world data to analyze flood events using daily streamflow data covering the period from August 1st, 1958, to December 31st, 2020.\nLet‚Äôs consider the formatted daily streamflow data file for the target catchment. We can read the data into a data frame using the read.csv() function. We will also set the column names using the col.names argument, and use the lubridate library to prepare a time serie.\n\nfilename = './data/discharge.csv'\nQdata = read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))  \n\n# construct time serie\nQdata$Date = paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep=\"-\")\nQdata$Date  = ymd(Qdata$Date)\n\n# final time serie\nQdata = Qdata[, c(\"Date\", \"Q\")]\n\nThe output is a data frame (or a matrix) with 22799 rows and 2 columns.\n\n\n\n\nTable¬†1: First (6) lines of the daily streamflow data for Block-Maxima approach\n\n\n\n\n\n\nDate\nQ\n\n\n\n\n1958-08-01\n0.2357639\n\n\n1958-08-02\n0.1841204\n\n\n1958-08-03\n0.1437037\n\n\n1958-08-04\n0.1437037\n\n\n1958-08-05\n0.1437037\n\n\n1958-08-06\n0.1437037\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can display the structure of the data frame using the str() function.\n\nstr(Qdata)  \n\n'data.frame':   22799 obs. of  2 variables:\n $ Date: Date, format: \"1958-08-01\" \"1958-08-02\" ...\n $ Q   : num  0.236 0.184 0.144 0.144 0.144 ..."
  },
  {
    "objectID": "pot/pot.html",
    "href": "pot/pot.html",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "overview/overview.html",
    "href": "overview/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Important\n\n\n\nIndependance (absence of autocorrelation) and Stationarity of extreme value time series are the underlying assumptions of EVA.\n\n\n\nExtreme Value Analysis (EVA)Extreme Value Theory (EVT)Practical Objectives\n\n\n\n\n\n\n\n\nüåä Extreme Value Analysis (EVA) in Hydrology\n\n\n\nEVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events.\n\n\n\n\n\n\n\n\nüìå Key Concepts of EVA\n\n\n\nüìà Block Maxima Approach: Selects the maximum value in fixed time blocks (e.g., annual maximum flow or rainfall) ‚Üí Fits data to distributions like Gumbel or GEV.\nüìâ Peak Over Threshold (POT) Method: Considers all events exceeding a predefined threshold ‚Üí More data points ‚Üí Fitted to Generalized Pareto Distribution (GPD).\n\n\n\n\n\n\n\n\nüéØ Objectives of EVA\n\n\n\nüîÅ Estimate Return Periods: Quantify the magnitude of events expected to occur once every N years (e.g., a 100-year event has a 1% annual exceedance probability).\nüìä Construct Frequency Curves: Fit statistical distributions (e.g., GEV, Gumbel, Log-Pearson Type III) to observed extremes to relate event magnitude to exceedance probability.\nüß∞ Support Risk-Based Design: Inform engineering design criteria (e.g., for dams, levees, drainage systems) based on scientifically justified extreme thresholds\n\n\n\n\n\n\n\n\nüèóÔ∏è Real-world Applications of EVA\n\n\n\nüèûÔ∏è Infrastructure Design: Design dams, bridges, culverts based on expected extreme flows.\nüíß Water Resource Management: Plan for droughts, reservoir operations, and sustainable use.\n‚ö†Ô∏è Risk Assessment & Planning: Develop floodplain zoning, disaster risk reduction strategies.\nüß≠ Policy & Climate Adaptation: Inform government strategies and early warning systems in a changing climate.\n\n\n\n\nImagine we have daily observational data for a random variable (e.g., river discharges, precipitations, etc.) for many years. EVT suggests that the extreme values of this variable are asymptotically close to one of three types of extreme value distributions, regardless of the original distribution of daily flows. EVT helps us estimate the probability of rare, high-flow events like floods.\n\n\n\n\n\n\n\n\n\n(a) Normal distribution of the random variable\n\n\n\n\n\n\n\n(b) Sampling distribution of the mean\n\n\n\n\n\n\n\n(c) Extreme value distribution (maxima)\n\n\n\n\n\n\nIllustration of the Extreme Value Theory (EVT)\n\n\n\n\n\n\nüìä Apply sampling approaches for extreme events\nUse annual maxima and peaks-over-threshold (POT) methods, including threshold selection and declustering to identify independent events.\nüìà Understand trend detection in extreme streamflow series\nGain insight into statistical methods for identifying trends in hydrometeorological extremes.\nüß™ Compare non-parametric and parametric techniques\nImplement and contrast the non-parametric test (e.g., Mann-Kendall) with parametric (distribution-based) models such as Generalized Extreme Value Distribution (GEV) and Generalized Pareto Distribution (GPD) in a non-stationary framework.\nüîç Interpret statistical significance\nEvaluate the reliability and meaning of trend detection results under various statistical assumptions.\nüíª Use open-source R packages for implementation\nApply all methods in practice using reproducible, open-source tools within the R environment.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "overview/overview.html#introduction",
    "href": "overview/overview.html#introduction",
    "title": "Overview",
    "section": "",
    "text": "Frequency analysis of floods is a statistical method used in hydrology to estimate the probability of occurrence of extreme flow events, such as floods, over a given period. It is typically based on the study of observed annual maximum flood discharges over several years. An alternative approach is the peak over threshold (POT) method, which considers all flood peaks that exceed a certain threshold, rather than only the annual maxima. This method allows for a more detailed analysis of extreme flood events by using more data points above the chosen threshold.\nThe objective is to determine the discharge associated with a return period, that is, the discharge that has a certain probability (e.g., 1%, 10%) of being exceeded in any given year. For example, a 100-year flood has a 1% chance of being exceeded in any year.\nTo achieve this, the data are fitted to a probability distribution (such as the Gumbel distribution), and a frequency curve is constructed linking discharges to return periods. This curve is then used to design hydraulic structures (dams, bridges, levees) and to manage flood risks.\nFrequency analysis thus allows for an objective assessment of flood-related risks and supports informed decision-making in land-use planning and flood protection."
  },
  {
    "objectID": "overview/overview.html#extreme-value-theory-evt",
    "href": "overview/overview.html#extreme-value-theory-evt",
    "title": "Overview",
    "section": "",
    "text": "Imagine we have daily observational data for a random variable (e.g., river discharges, precipitations, etc.) for many years. EVT suggests that the extreme values of this variable are asymptotically close to one of three types of extreme value distributions, regardless of the original distribution of daily flows.\nThe EVD helps us estimate the probability of rare, high-flow events like floods.\n\n\n\n\n\n\n\n\n\n(a) Normal distribution of the random variable\n\n\n\n\n\n\n\n(b) Sampling distribution of the mean\n\n\n\n\n\n\n\n(c) Extreme value distribution (maxima)\n\n\n\n\n\n\nIllustration of the Extreme Value Theory (EVT)"
  },
  {
    "objectID": "overview/overview.html#practical-objectives",
    "href": "overview/overview.html#practical-objectives",
    "title": "Overview",
    "section": "Practical Objectives",
    "text": "Practical Objectives\nüìä Apply sampling approaches for extreme events\nUse annual maxima and peaks-over-threshold (POT) methods, including threshold selection and declustering to identify independent events.\nüìà Understand trend detection in extreme rainfall or flood series\nGain insight into statistical methods for identifying trends in hydrometeorological extremes.\nüß™ Compare non-parametric and parametric techniques\nImplement and contrast the non-parametric test (e.g., Mann-Kendall) with parametric (distribution-based) models such as Generalized Extreme Value Distribution (GEV) and Generalized Pareto Distribution (GPD) in a non-stationary framework.\nüîç Interpret statistical significance\nEvaluate the reliability and meaning of trend detection results under various statistical assumptions.\nüíª Use open-source R packages for implementation\nApply all methods in practice using reproducible, open-source tools within the R environment."
  },
  {
    "objectID": "overview/overview.html#materials",
    "href": "overview/overview.html#materials",
    "title": "Overview",
    "section": "Materials",
    "text": "Materials\nüì¶ {extRemes} ‚Äî For fitting and analyzing extreme value distributions.\n\ninstall.packages(\"extRemes\")\n\nüì¶ {trend} ‚Äî Provides non-parametric tests test for trend detection.\n\ninstall.packages(\"trend\")\n\nüì¶ {dplyr} ‚Äî For data manipulation and tidy workflows\n\ninstall.packages(\"dplyr\")\n\nüì¶ {ggplot2} ‚Äî For creating clear and publication-quality plots\n\ninstall.packages(\"ggplot2\")\n\nüì¶ {lubridate} ‚Äî To work with date-times and time-spans.\n\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMust-haves\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe packages dplyr, ggplot2 and lubridate are part of the tidyverse collection of R packages. The core tidyverse includes the packages that you‚Äôre likely to use in everyday data analyses. It‚Äôs is advised to install this set of packages, including dplyr, ggplot2 and lubridate, together by running:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "index.html#extreme-value-analysis-eva-in-hydrology",
    "href": "index.html#extreme-value-analysis-eva-in-hydrology",
    "title": "Extreme Value Analysis",
    "section": "",
    "text": "EVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events."
  },
  {
    "objectID": "blockmaxima/block_maxima.html#the-block-maxima-approach",
    "href": "blockmaxima/block_maxima.html#the-block-maxima-approach",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "2.1 The Block-Maxima Approach",
    "text": "2.1 The Block-Maxima Approach\nThe Block-Maxima Approach, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and ensures, by construction, that the sampled values are statistically independent, a key requirement in extreme value analysis (EVA).\n\n\n\n\n\n\n\n\nFigure¬†1: Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima"
  },
  {
    "objectID": "blockmaxima/block_maxima.html#exract-annual-maximum-flows-amf-from-the-daily-streamflow-serie",
    "href": "blockmaxima/block_maxima.html#exract-annual-maximum-flows-amf-from-the-daily-streamflow-serie",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "2.2 Exract Annual Maximum Flows (AMF) from the daily streamflow serie",
    "text": "2.2 Exract Annual Maximum Flows (AMF) from the daily streamflow serie\n1. To extract annual maximum flows (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\nlibrary(dplyr)\nlibrary(lubridate)\nextract_annual_max = function(df) \n{\n    df %&gt;%\n    mutate(Year = year(Date)) %&gt;%    # extract year from Date\n    group_by(Year) %&gt;%\n    filter(Q == max(Q)) %&gt;%          # keep only max flow per year\n    slice(1) %&gt;%                     # in case of ties, keep first max\n    ungroup() %&gt;%\n    select(Year, Date, AMAX=Q)\n}\n\n2. Now, we apply the function to the data to extract AMF for each year:\n\nextractedAMF = extract_annual_max(Qdata)\n\n\n\n\n\nTable¬†2: First (6) rows of the AMF extracted using the Block-Maxima approach\n\n\n\n\n\n\nYear\nDate\nAMAX\n\n\n\n\n1958\n1958-12-20\n39.96984\n\n\n1959\n1959-03-07\n36.89593\n\n\n1960\n1960-10-06\n58.41780\n\n\n1961\n1961-12-11\n18.44796\n\n\n1962\n1962-03-05\n19.47185\n\n\n1963\n1963-04-11\n29.21002"
  },
  {
    "objectID": "blockmaxima/block_maxima.html#display-sampled-events",
    "href": "blockmaxima/block_maxima.html#display-sampled-events",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "2.3 Display Sampled Events",
    "text": "2.3 Display Sampled Events\nNow, we overlap the extracted Annual Maximum Flow with the original daily discharge time serie to visualize the sampling process. We use the functions from the dplyr and ggplot2 library to create the plot.\n\n\nShow the code\nlibrary(ggplot2)\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach"
  },
  {
    "objectID": "blockmaxima/block_maxima.html#what-is-autocorrelation",
    "href": "blockmaxima/block_maxima.html#what-is-autocorrelation",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.1 What is Autocorrelation?",
    "text": "3.1 What is Autocorrelation?\nAutocorrelation (or autocovariance) of a series refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ‚Ä¶). An autocorrelated series is thus correlated with itself, with a given lag (please refer here for mode details).\n\n\n\n\n\n\nTo assess autocorrelation, two methods will be used in this lab\n\n\n\nüîç Correlogram:\n\nVisualization of correlation coefficients between the serie and its lags\nBased on the autocorrelation function (ACF)\nHelps identify the presence and extent of dependencies over time\n\nüìä Wald-Wolfowitz Test (1940):\n\nNon-parametric to assess the randomness of a time series\nUsed to detect the absence of randomness in the series\nNull hypothesis: The sequence of observations is random\n\n\n\n\n3.1.1 Correlogram\nIf you execute the above given chunk, it generates the Figure¬†3 as output:\n\n\nShow the code\n# compute autocorrelation lag\nbacf &lt;- acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf &lt;- with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn &lt;- length(extractedAMF$AMAX)\nconf_limit &lt;- 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n\n\n\n\n\n\n\n\nFigure¬†3: Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n\n\n\n\n3.1.2 Wald-Wolfowitz (WW) Test\nIn the chunk below, we use the ww.test(...) function from {trend} package to perform the WW test at the 0.05 significance level to test the hypothesis that the maximum annual flood is a random variable.\n\nwwtest = trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n\n\n    Wald-Wolfowitz test for independence and stationarity\n\ndata:  extractedAMF$AMAX\nz = 0.73738, n = 60, p-value = 0.4609\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.4609: indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness)."
  },
  {
    "objectID": "blockmaxima/block_maxima.html#to-assess-autocorrelation-two-methods-will-be-used-in-this-lab",
    "href": "blockmaxima/block_maxima.html#to-assess-autocorrelation-two-methods-will-be-used-in-this-lab",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "To assess autocorrelation, two methods will be used in this lab",
    "text": "To assess autocorrelation, two methods will be used in this lab\nüîç Correlogram:\n\nVisualization of correlation coefficients between the serie and its lags\nBased on the autocorrelation function (ACF)\nHelps identify the presence and extent of dependencies over time\n\nüìä Wald-Wolfowitz Test (1940):\n\nNon-parametric to assess the randomness of a time series\nUsed to detect the absence of randomness in the series\n\n\nCorrelogram\n\n\n\n\n\n\nRecall\n\n\n\nThe extractedAMF variable is created previously in the Sampling Events section The acf function is a R-base function, no installation needed\n\n\n\n\nShow the code\n# compute autocorrelation lag\nbacf &lt;- acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf &lt;- with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn &lt;- length(extractedAMF$AMAX)\nconf_limit &lt;- 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n\n\n\n\n\n\n\n\nFigure¬†3: Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\n\n\n\n\n\n\n\nWald-Wolfowitz Test"
  },
  {
    "objectID": "blockmaxima/block_maxima.html#trend-detection",
    "href": "blockmaxima/block_maxima.html#trend-detection",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.2 Trend Detection",
    "text": "3.2 Trend Detection\n\n3.2.1 Non-Parametric methods: The Mann-Kendall (MK) test\nThe MK test‚Äôs null hypothesis (H0) assumes no trend in the data (please see here for the detailed algorithm). Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results.\nWe use the mk.test(...) function from {trend} package to perform the MK test at the 0.05 significance level to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNon-Stationarity (NS) refers to changes in statistical properties over time.\n\n\n\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n\n\n    Mann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.2183: there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\ntau = -0.1096665: Kendall‚Äôs Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\nz = -1.231: the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n\n\nFigure¬†4 shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\nShow the code\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n\n\n\n\n\n\n\n\nFigure¬†4: Evolution of the annual peak flood over time.\n\n\n\n\n\n\n\n3.2.2 Parametric methods\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a non-stationary (NS) context.\n\n\n\n\n\n\nNote\n\n\n\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter (Œº) will be expressed as a linear function of time, denoted as Œº(t), leaving the others parameters constant."
  },
  {
    "objectID": "blockmaxima/block_maxima.html#ffa",
    "href": "blockmaxima/block_maxima.html#ffa",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.3 FFA",
    "text": "3.3 FFA"
  },
  {
    "objectID": "blockmaxima/block_maxima.html#flood-frequency-analysis-ffa",
    "href": "blockmaxima/block_maxima.html#flood-frequency-analysis-ffa",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.3 Flood Frequency Analysis (FFA)",
    "text": "3.3 Flood Frequency Analysis (FFA)\n\n3.3.1 Probability distributions\nWe will compare two probability distributions adapted to the Block-Maxima Framework: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n\n\n\n\n\nGeneralized Extreme Value (GEV) Distribution\n\n\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n F(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} \nfor 1 + \\xi (x - \\mu)/\\sigma &gt; 0, where:\n\n\\mu is the location parameter.\n\\sigma &gt; 0 is the scale parameter.\n\\xi is the shape parameter.\n\nThe shape parameter \\xi determines the type of extreme value distribution:\n\nIf \\xi &gt; 0, the distribution is a Fr√©chet type.\nIf \\xi &lt; 0, the distribution is a Weibull type.\nIf \\xi = 0, the distribution is a Gumbel type.\n\n\n\n\n\n\n\n\n\nGumbel Distribution\n\n\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter \\xi = 0. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:\n\n\n\n\n3.3.2 Comparison of the probability distributions\nTo select the best probability distribution, we will use the AIC and AIC criteria. The model with the lowest AIC or BIC value will be selected.\n\n\n\n\n\n\nAIC (Akaike Information Criterion)\n\n\n\nThe AIC balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n \\text{AIC} = 2k - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\n\\hat{L} is the maximum value of the likelihood function for the model.\n\n\n\n\n\n\n\n\n\nBIC (Bayesian Information Criterion)\n\n\n\nBIC penalizes model complexity more heavily than AIC, especially for larger sample sizes. This means BIC tends to favor simpler models compared to AIC. The formula for BIC is:\n \\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\nn is the number of data points.\n\\hat{L} is the maximum value of the likelihood function for the model."
  },
  {
    "objectID": "blockmaxima/block_maxima.html#aic-akaike-information-criterion",
    "href": "blockmaxima/block_maxima.html#aic-akaike-information-criterion",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.4 AIC (Akaike Information Criterion)",
    "text": "3.4 AIC (Akaike Information Criterion)\nThe AIC balances the goodness of fit of the model with the number of parameters used by the model."
  },
  {
    "objectID": "blockmaxima/block_maxima.html#bic-bayesian-information-criterion",
    "href": "blockmaxima/block_maxima.html#bic-bayesian-information-criterion",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.5 BIC (Bayesian Information Criterion)",
    "text": "3.5 BIC (Bayesian Information Criterion)\nBIC penalizes model complexity more heavily than AIC, especially for larger sample sizes. This means BIC tends to favor simpler models compared to AIC."
  },
  {
    "objectID": "softwaresetup/softwaresetup.html",
    "href": "softwaresetup/softwaresetup.html",
    "title": "Extreme Value Analysis",
    "section": "",
    "text": "üåä Extreme Value Analysis (EVA) in Hydrology\n\n\n\nEVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events.\n\n\n\n\n\n\n\n\nüìå Key Concepts of EVA\n\n\n\nüìà Block Maxima Approach: Selects the maximum value in fixed time blocks (e.g., annual maximum flow or rainfall) ‚Üí Fits data to distributions like Gumbel or GEV.\nüìâ Peak Over Threshold (POT) Method: Considers all events exceeding a predefined threshold ‚Üí More data points ‚Üí Fitted to Generalized Pareto Distribution (GPD).\n\n\n\n\n\n\n\n\nüéØ Objectives of EVA\n\n\n\nüîÅ Estimate Return Periods: Quantify the magnitude of events expected to occur once every N years (e.g., a 100-year event has a 1% annual exceedance probability).\nüìä Construct Frequency Curves: Fit statistical distributions (e.g., GEV, Gumbel, Log-Pearson Type III) to observed extremes to relate event magnitude to exceedance probability.\nüß∞ Support Risk-Based Design: Inform engineering design criteria (e.g., for dams, levees, drainage systems) based on scientifically justified extreme thresholds\n\n\n\n\n\n\n\n\nüèóÔ∏è Real-world Applications of EVA\n\n\n\nüèûÔ∏è Infrastructure Design: Design dams, bridges, culverts based on expected extreme flows.\nüíß Water Resource Management: Plan for droughts, reservoir operations, and sustainable use.\n‚ö†Ô∏è Risk Assessment & Planning: Develop floodplain zoning, disaster risk reduction strategies.\nüß≠ Policy & Climate Adaptation: Inform government strategies and early warning systems in a changing climate.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "softwaresetup/index.html",
    "href": "softwaresetup/index.html",
    "title": "Software, Setup and Libraries (Packages)",
    "section": "",
    "text": "Note\n\n\n\nAn IDE (Integrated Development Environment) is a software application that provides developers with a comprehensive set of tools for writing, testing, and debugging code in one place.\n\n\n\n\nR is a programming language and software environment designed primarily for statistical computing, data analysis, and visualization.\n\n\n\n\n\n\nFigure¬†1: Some of the key advantages of learning the R programming language. (Source: C# Corner)\n\n\n\n\n\n\nRStudio is an IDE (Integrated Development Environment) specifically made for working with R. It provides a much more convenient and powerful interface than the basic R console:\n\nüìù Script editor with syntax highlighting and auto-completion\nüíª Console for interactive execution\nüì¶ Environment viewer to see your variables, functions, and data\nüìä Plot viewer, üóÇÔ∏è file browser, üìñ help pane, and more\n‚öôÔ∏è Easy access to package management, üîÑ version control, and üìÑ R Markdown / Quarto\n\n\n\n\n\n\n\nFigure¬†2: Overview of the four main panes in the RStudio interface. Source pane (top-left): where scripts and R Markdown files are written and edited. Console pane (bottom-left): where R commands are executed interactively. Environment pane (top-right): displays the current workspace, including variables and data. Output pane (bottom-right): shows plots, files, packages, help pages, and more. (Source: Posit)\n\n\n\n\n\n\n1- Install R ‚Üí https://cran.r-project.org\n2- Install RStudio ‚Üí https://posit.co/download/rstudio-desktop/\n3- Then, open RStudio ‚Üí it will automatically use the installed R engine in the background.\n\n\n\n\n\n\nFigure¬†3: R environment, the engine powering your data analysis, and RStudio, the dashboard and controls that make driving (coding) efficient and user-friendly. (Source: umanitoba.ca)"
  },
  {
    "objectID": "softwaresetup/index.html#materials",
    "href": "softwaresetup/index.html#materials",
    "title": "Software and Setup",
    "section": "",
    "text": "üì¶ {extRemes} ‚Äî For fitting and analyzing extreme value distributions.\n\ninstall.packages(\"extRemes\")\n\nüì¶ {trend} ‚Äî Provides non-parametric tests test for trend detection.\n\ninstall.packages(\"trend\")\n\nüì¶ {dplyr} ‚Äî For data manipulation and tidy workflows\n\ninstall.packages(\"dplyr\")\n\nüì¶ {ggplot2} ‚Äî For creating clear and publication-quality plots\n\ninstall.packages(\"ggplot2\")\n\nüì¶ {lubridate} ‚Äî To work with date-times and time-spans.\n\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\n\n\nMust-haves\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe packages dplyr, ggplot2 and lubridate are part of the tidyverse collection of R packages. The core tidyverse includes the packages that you‚Äôre likely to use in everyday data analyses. It‚Äôs is advised to install this set of packages, including dplyr, ggplot2 and lubridate, together by running:\n\ninstall.packages(\"tidyverse\")\n\n\n\n:::"
  },
  {
    "objectID": "softwaresetup/index.html#what-is-r",
    "href": "softwaresetup/index.html#what-is-r",
    "title": "Software, Setup and Libraries (Packages)",
    "section": "",
    "text": "R is a programming language and software environment designed primarily for statistical computing, data analysis, and visualization.\n\n\n\n\n\n\nFigure¬†1: Some of the key advantages of learning the R programming language. (Source: C# Corner)"
  },
  {
    "objectID": "softwaresetup/index.html#what-is-rstudio",
    "href": "softwaresetup/index.html#what-is-rstudio",
    "title": "Software, Setup and Libraries (Packages)",
    "section": "",
    "text": "RStudio is an IDE (Integrated Development Environment) specifically made for working with R. It provides a much more convenient and powerful interface than the basic R console:\n\nüìù Script editor with syntax highlighting and auto-completion\nüíª Console for interactive execution\nüì¶ Environment viewer to see your variables, functions, and data\nüìä Plot viewer, üóÇÔ∏è file browser, üìñ help pane, and more\n‚öôÔ∏è Easy access to package management, üîÑ version control, and üìÑ R Markdown / Quarto\n\n\n\n\n\n\n\nFigure¬†2: Overview of the four main panes in the RStudio interface. Source pane (top-left): where scripts and R Markdown files are written and edited. Console pane (bottom-left): where R commands are executed interactively. Environment pane (top-right): displays the current workspace, including variables and data. Output pane (bottom-right): shows plots, files, packages, help pages, and more. (Source: Posit)"
  },
  {
    "objectID": "softwaresetup/index.html#how-to-use-them-together",
    "href": "softwaresetup/index.html#how-to-use-them-together",
    "title": "Software, Setup and Libraries (Packages)",
    "section": "",
    "text": "1- Install R ‚Üí https://cran.r-project.org\n2- Install RStudio ‚Üí https://posit.co/download/rstudio-desktop/\n3- Then, open RStudio ‚Üí it will automatically use the installed R engine in the background.\n\n\n\n\n\n\nFigure¬†3: R environment, the engine powering your data analysis, and RStudio, the dashboard and controls that make driving (coding) efficient and user-friendly. (Source: umanitoba.ca)"
  },
  {
    "objectID": "overview/index.html#eva-in-hydrology",
    "href": "overview/index.html#eva-in-hydrology",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "EVA is a powerful statistical tool to analyze rare events in hydrology and climate science. It focuses on understanding the frequency and intensity of extreme events."
  },
  {
    "objectID": "overview/index.html#key-concepts-of-eva",
    "href": "overview/index.html#key-concepts-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "Selects the maximum value in fixed time blocks (e.g., annual maximum flow or rainfall) ‚Üí Fits data to distributions like Gumbel or GEV.\n\n\n\nConsiders all events exceeding a predefined threshold ‚Üí More data points ‚Üí Fitted to Generalized Pareto Distribution (GPD)."
  },
  {
    "objectID": "overview/index.html#objectives-of-eva",
    "href": "overview/index.html#objectives-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "üîÅ Estimate Return Periods: Assess the intensity of rare events expected to be exceeded, on average, once every N years (e.g., for example, a 100-year event corresponds to a 1% annual probability of being exceeded).\nüìä Construct Frequency Curves: Fit statistical distributions (e.g., GEV, Gumbel, Log-Pearson Type III) to observed extremes to relate event magnitude to exceedance probability.\nüß∞ Support Risk-Based Design: Inform engineering design criteria (e.g., for dams, levees, drainage systems) based on scientifically justified extreme thresholds"
  },
  {
    "objectID": "overview/index.html#real-world-applications-of-eva",
    "href": "overview/index.html#real-world-applications-of-eva",
    "title": "Overview, Key Concepts and Objectives",
    "section": "",
    "text": "üèûÔ∏è Infrastructure Design: Design dams, bridges, culverts based on expected extreme flows.\nüíß Water Resource Management: Plan for droughts, reservoir operations, and sustainable use.\n‚ö†Ô∏è Risk Assessment & Planning: Develop floodplain zoning, disaster risk reduction strategies.\nüß≠ Policy & Climate Adaptation: Inform government strategies and early warning systems in a changing climate."
  },
  {
    "objectID": "overview/index.html#practical-objectives-of-this-module",
    "href": "overview/index.html#practical-objectives-of-this-module",
    "title": "Overview",
    "section": "2.1 Practical Objectives of this Module",
    "text": "2.1 Practical Objectives of this Module\nüìä Apply sampling approaches for extreme events\nUse annual maxima and peaks-over-threshold (POT) methods, including threshold selection and declustering to identify independent events.\nüìà Understand trend detection in extreme streamflow series\nGain insight into statistical methods for identifying trends in hydrometeorological extremes.\nüß™ Compare non-parametric and parametric techniques\nImplement and contrast the non-parametric test (e.g., Mann-Kendall) with parametric (distribution-based) models such as Generalized Extreme Value Distribution (GEV) and Generalized Pareto Distribution (GPD) in a non-stationary framework.\nüîç Interpret statistical significance\nEvaluate the reliability and meaning of trend detection results under various statistical assumptions.\nüíª Use open-source R packages for implementation\nApply all methods in practice using reproducible, open-source tools within the R environment."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Finley Malloc",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html#this-content-appears-above-the-formatted-about-page-content.",
    "href": "index.html#this-content-appears-above-the-formatted-about-page-content.",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": ":::{#hero-heading}\nFinley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.¬†Paul, MN B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "index.html#yves-tramblay",
    "href": "index.html#yves-tramblay",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Research scientist of the French Institute of Research for Development since 2013 - Working on hydrological hazards in Mediterranean countries, to improve the modelling of floods, heavy rainfall and droughts in the context of climate change [Espace-Dev, Univ. Montpellier, IRD, Montpellier, France]."
  },
  {
    "objectID": "index.html#serigne-bassirou-diop",
    "href": "index.html#serigne-bassirou-diop",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "PhD Researcher on flood modelling in the context of climate change ‚Äì Passionate about Hydrology, Climate Change, Statistical Modelling and Machine Learning [Le√Ødi, Univ. Gaston Berger, Saint-Louis, Senegal]"
  },
  {
    "objectID": "index.html#serigne-bassirou-diop-httpsorcid.org0009-0002-8743-3660",
    "href": "index.html#serigne-bassirou-diop-httpsorcid.org0009-0002-8743-3660",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "Currently pursuing a PhD on flood modelling in the context of climate change ‚Äì Passionate about Hydrology, Climate Change, Statistical Modelling and Machine Learning [Le√Ødi, Univ. Gaston Berger, Saint-Louis, Senegal]"
  },
  {
    "objectID": "index.html#yves-tramblay-espace-dev-univ.-montpellier-ird-montpellier-france",
    "href": "index.html#yves-tramblay-espace-dev-univ.-montpellier-ird-montpellier-france",
    "title": "Introduction to Extreme Value Analysis",
    "section": "",
    "text": "I am a research scientist of the French Institute of Research for Development since 2013. I work on hydrological hazards in Mediterranean countries, to improve the modelling of floods, heavy rainfall and droughts in the context of climate change\nMacalester College | St.¬†Paul, MN B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "blockmaxima/index.html#the-block-maxima-approach",
    "href": "blockmaxima/index.html#the-block-maxima-approach",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.1 The Block-Maxima Approach",
    "text": "3.1 The Block-Maxima Approach\nThe Block-Maxima Approach, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and ensures, by construction, that the sampled values are statistically independent, a key requirement in extreme value analysis (EVA).\n\n\n\n\n\n\n\n\nFigure¬†1: Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima"
  },
  {
    "objectID": "blockmaxima/index.html#exract-annual-maximum-flows-amf-from-the-daily-streamflow-serie",
    "href": "blockmaxima/index.html#exract-annual-maximum-flows-amf-from-the-daily-streamflow-serie",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "2.2 Exract Annual Maximum Flows (AMF) from the daily streamflow serie",
    "text": "2.2 Exract Annual Maximum Flows (AMF) from the daily streamflow serie\n1. To extract annual maximum flows (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\nlibrary(dplyr)\nlibrary(lubridate)\nextract_annual_max = function(df) \n{\n    df %&gt;%\n    mutate(Year = year(Date)) %&gt;%    # extract year from Date\n    group_by(Year) %&gt;%\n    filter(Q == max(Q)) %&gt;%          # keep only max flow per year\n    slice(1) %&gt;%                     # in case of ties, keep first max\n    ungroup() %&gt;%\n    select(Year, Date, AMAX=Q)\n}\n\n2. Now, we apply the function to the data to extract AMF for each year:\n\nextractedAMF = extract_annual_max(Qdata)\n\n\n\n\n\nTable¬†2: First (6) rows of the AMF extracted using the Block-Maxima approach\n\n\n\n\n\n\nYear\nDate\nAMAX\n\n\n\n\n1958\n1958-12-20\n39.96984\n\n\n1959\n1959-03-07\n36.89593\n\n\n1960\n1960-10-06\n58.41780\n\n\n1961\n1961-12-11\n18.44796\n\n\n1962\n1962-03-05\n19.47185\n\n\n1963\n1963-04-11\n29.21002"
  },
  {
    "objectID": "blockmaxima/index.html#display-sampled-events",
    "href": "blockmaxima/index.html#display-sampled-events",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.3 Display Sampled Events",
    "text": "3.3 Display Sampled Events\nNow, we overlap the extracted Annual Maximum Flow with the original daily discharge time serie to visualize the sampling process. We use the functions from the dplyr and ggplot2 library to create the plot.\n\n\nShow the code\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach"
  },
  {
    "objectID": "blockmaxima/index.html#what-is-autocorrelation",
    "href": "blockmaxima/index.html#what-is-autocorrelation",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "4.1 What is Autocorrelation?",
    "text": "4.1 What is Autocorrelation?\nAutocorrelation (or autocovariance) of a series refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ‚Ä¶). An autocorrelated series is thus correlated with itself, with a given lag (please refer here for mode details).\n\n\n\n\n\n\nTo assess autocorrelation, two methods will be used in this lab\n\n\n\nüîç Correlogram:\n\nVisualization of correlation coefficients between the serie and its lags\nBased on the autocorrelation function (ACF)\nHelps identify the presence and extent of dependencies over time\n\nüìä Wald-Wolfowitz Test (1940):\n\nNon-parametric to assess the randomness of a time series\nUsed to detect the absence of randomness in the series\nNull hypothesis: The sequence of observations is random\n\n\n\n\n4.1.1 Correlogram\nIf you execute the above given chunk, it generates the Figure¬†3 as output:\n\n\nShow the code\n# compute autocorrelation lag\nbacf = acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf = with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn = length(extractedAMF$AMAX)\nconf_limit = 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n\n\n\n\n\n\n\n\nFigure¬†3: Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n\n\n\n\n4.1.2 Wald-Wolfowitz (WW) Test\nIn the chunk below, we use the ww.test(...) function from {trend} package to perform the WW test at the 0.05 significance level to test the hypothesis that the maximum annual flood is a random variable.\n\nwwtest = trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n\n\n    Wald-Wolfowitz test for independence and stationarity\n\ndata:  extractedAMF$AMAX\nz = 0.73738, n = 60, p-value = 0.4609\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.4609: indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness)."
  },
  {
    "objectID": "blockmaxima/index.html#trend-detection",
    "href": "blockmaxima/index.html#trend-detection",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.2 Trend Detection",
    "text": "3.2 Trend Detection\n\n3.2.1 Non-Parametric methods: The Mann-Kendall (MK) test\nThe MK test‚Äôs null hypothesis (H0) assumes no trend in the data (please see here for the detailed algorithm). Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results.\nWe use the mk.test(...) function from {trend} package to perform the MK test at the 0.05 significance level to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNon-Stationarity (NS) refers to changes in statistical properties over time.\n\n\n\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n\n\n    Mann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.2183: there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\ntau = -0.1096665: Kendall‚Äôs Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\nz = -1.231: the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n\n\nFigure¬†4 shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\nShow the code\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n\n\n\n\n\n\n\n\nFigure¬†4: Evolution of the annual peak flood over time.\n\n\n\n\n\n\n\n3.2.2 Parametric methods\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a non-stationary (NS) context.\n\n\n\n\n\n\nNote\n\n\n\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter (Œº) will be expressed as a linear function of time, denoted as Œº(t), leaving the others parameters constant."
  },
  {
    "objectID": "blockmaxima/index.html#flood-frequency-analysis-ffa",
    "href": "blockmaxima/index.html#flood-frequency-analysis-ffa",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "4.2 Flood Frequency Analysis (FFA)",
    "text": "4.2 Flood Frequency Analysis (FFA)"
  },
  {
    "objectID": "blockmaxima/index.html#non-parametric-methods-the-mann-kendall-mk-test",
    "href": "blockmaxima/index.html#non-parametric-methods-the-mann-kendall-mk-test",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "5.1 Non-Parametric methods: The Mann-Kendall (MK) test",
    "text": "5.1 Non-Parametric methods: The Mann-Kendall (MK) test\nThe MK test‚Äôs null hypothesis (H0) assumes no trend in the data (please see here for the detailed algorithm). Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results.\nWe use the mk.test(...) function from {trend} package to perform the MK test at the 0.05 significance level to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNon-Stationarity (NS) refers to changes in statistical properties over time.\n\n\n\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n\n\n    Mann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\np.value = 0.2183: there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\ntau = -0.1096665: Kendall‚Äôs Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\nz = -1.231: the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n\n\nFigure¬†4 shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\nShow the code\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n\n\n\n\n\n\n\n\nFigure¬†4: Evolution of the annual peak flood over time."
  },
  {
    "objectID": "blockmaxima/index.html#description-of-the-probability-distributions",
    "href": "blockmaxima/index.html#description-of-the-probability-distributions",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.1 Description of the Probability distributions",
    "text": "6.1 Description of the Probability distributions\nWe will compare two probability distributions adapted to the Block-Maxima Framework: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n\n\n\n\n\nGeneralized Extreme Value (GEV) Distribution\n\n\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n F(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} \nfor 1 + \\xi (x - \\mu)/\\sigma &gt; 0, where:\n\n\\mu is the location parameter.\n\\sigma &gt; 0 is the scale parameter.\n\\xi is the shape parameter.\n\nThe shape parameter \\xi determines the type of extreme value distribution:\n\nIf \\xi &gt; 0, the distribution is a Fr√©chet type.\nIf \\xi &lt; 0, the distribution is a Weibull type.\nIf \\xi = 0, the distribution is a Gumbel type.\n\n\n\n\n\n\n\n\n\nGumbel Distribution\n\n\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter \\xi = 0. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:"
  },
  {
    "objectID": "blockmaxima/index.html#fitting-of-the-probability-distributions",
    "href": "blockmaxima/index.html#fitting-of-the-probability-distributions",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.2 Fitting of the probability distributions",
    "text": "6.2 Fitting of the probability distributions\nWe will employ the Generalized Maximum Likelihood Estimation (GMLE) method to fit probability distributions to our data. The {extRemes} package offers the function fevd(..), which enables fitting a range of probability distributions ‚Äî such as the GEV, Gumbel distributions ‚Äî to a given dataset using three distinct fitting methods: L-moments, Maximum Likelihood Estimation (MLE), and GMLE.\n\n\n\n\n\n\n\n\nWhy the GML method?\n\n\n\nThe GMLE method use a prior distribution to constrain the GEVC shape parameter within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See Martins & Stedinger (2000) for more details).\n\n\nThe advantage of using GMLE and MLE over the L-moments method is that L-moments are limited in their ability to accommodate non-stationary processes. In contrast, GMLE and MLE can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection."
  },
  {
    "objectID": "blockmaxima/index.html#comparison-of-the-probability-distributions",
    "href": "blockmaxima/index.html#comparison-of-the-probability-distributions",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.3 Comparison of the probability distributions",
    "text": "6.3 Comparison of the probability distributions\nTo select the best probability distribution, we will use the AIC and AIC criteria. The model with the lowest AIC or BIC value will be selected.\n\n\n\n\n\n\nAIC (Akaike Information Criterion)\n\n\n\nThe AIC balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n \\text{AIC} = 2k - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\n\\hat{L} is the maximum value of the likelihood function for the model.\n\n\n\n\n\n\n\n\n\n\n\nHow to select the best model?\n\n\n\nThe model with the lowest AIC and BIC values is the best-suited model. AIC tends to favor models with better fit, even if more complex, while BIC favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection:\n\nif both criteria select the same model, it is strongly preferred\nif not, the choice depends on whether your goal is predictive accuracy (AIC) or model simplicity (BIC)\n\n\n\n\n\n\n\n\n\nBIC (Bayesian Information Criterion)\n\n\n\nBIC penalizes model complexity more heavily than AIC, especially for larger sample sizes. This means BIC tends to favor simpler models compared to AIC. The formula for BIC is:\n \\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\nn is the number of data points.\n\\hat{L} is the maximum value of the likelihood function for the model."
  },
  {
    "objectID": "blockmaxima/index.html#perform-ffa",
    "href": "blockmaxima/index.html#perform-ffa",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.4 Perform FFA",
    "text": "6.4 Perform FFA\n\n6.4.1 Setup utils\nTo pretty-print the summary of our models‚Äô fitting, we will create a custom function that will take the fitted model as an argument, extract and format key components of the fiited object, and print the output:\n\npretty_gev_summary = function(fitobj, model) {\n  #' Pretty print summary of a  extreme value model fit\n  #'\n  #' @param fitobj Fitted model object from extRemes::fevd()\n  #' @param model Character string describing the model (e.g., \"GEV\")\n  \n  cat(\"===\", model, \" Fit Summary ===\\n\\n\")\n  \n  # Estimated Parameters\n  cat(\"Estimated Parameters:\\n\")\n  params = fitobj$results$par\n  print(round(params, 4))\n  \n  # Log-likelihood, AIC, and BIC\n  look = summary(fitobj, silent=TRUE)\n  cat(\"\\nModel Fit Criteria:\\n\")\n  cat(sprintf(\"LogLik: %.2f\\n\", look$nllh))\n  cat(sprintf(\"AIC: %.2f\\n\", look$AIC))\n  cat(sprintf(\"BIC: %.2f\\n\", look$BIC))\n\n  # return the AIC and BIC values for further comparison\n  return(data.frame(Model=model, Crit=c(\"AIC\", \"BIC\"), Value=c(look$AIC, look$BIC)))\n}\n\n\n\n6.4.2 Distributions fitting\n\n6.4.2.1 Stationary context\n\n\n\n\n\n\n\nWhat about the parameters?\n\n\n\nlocation = 19.4868: is the central tendency (Œº) of the GEV distribution. It defines the central position of the distribution and serves as a baseline around which extreme values are distributed.\nscale = 9.932: reflects the spread or variability of the extreme values. In our case, the AMF distribution has moderate spread around the shifting central value Œº(t).\nshape = 0.3576: this positive shape parameter implies a Fr√©chet-type (heavy-tailed) distribution. This means there is greater probability of very large flood events compared to a Gumbel (shape = 0) or Weibull (shape &lt; 0) distribution.\n\n\n\n\n\n\n\n\n\n‚úÖ Best-suited: GEV\n\n\n\nBoth AIC and BIC are lower for the GEV model, which suggests that it provides a better balance of fit and complexity than the Gumbel model.\n\n\n\nGEV fittingGumbel fittingBest-Suited Model\n\n\nThe chunk below use the {extRemes} library to fit the GEV distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the GEV distribution to the AMF\ngevfit = fevd(x=extractedAMF$AMAX, type=\"GEV\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_gev_summary() function we have defined above. This will show the AIC and BIC values, as well as the estimated parameters of the fitted GEV distribution.\n\n# show fitting sumary\ngevfit_res = pretty_gev_summary(gevfit, \"GEV\")\n\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale    shape \n 19.4868   9.8254   0.3818 \n\nModel Fit Criteria:\nLogLik: 244.65\nAIC: 495.31\nBIC: 501.59\n\n\n\nFinally, we show in Figure¬†5, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), and quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .043))\nplot(gevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†5: Diagnostic plots for the Generalized Extreme Value (GEV) distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\n\n\n\n\n\n\n\nThe chunk below use the {extRemes} library to fit the Gumbel distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the Gumbel distribution to the AMF\ngumfit = fevd(x=extractedAMF$AMAX, type=\"Gumbel\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_gev_summary() function we have defined previously. This will show the AIC and BIC values, as well as the estimated parameters of the fitted Gumbel distribution.\n\n# show fitting sumary\ngumfit_res = pretty_gev_summary(gumfit, \"Gumbel\")\n\n=== Gumbel  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale \n 21.7606  12.3003 \n\nModel Fit Criteria:\nLogLik: 249.40\nAIC: 502.80\nBIC: 506.99\n\n\nFinally, we show in Figure¬†6, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods:\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gumfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .035))\nplot(gumfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†6: Diagnostic plots for the Gumbel distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\n\n\n\n\n\n\n\nAs mentionned previously, the model with the lowest AIC and BIC values is the best-suited model. The pretty_gev_summary() function we have defined previously return a data.frame of AIC and BIC values for each distribution. Lets compare these values to select the best-suited distribution for the next steps.\n\nprint(gevfit_res)\nprint(gumfit_res)\n\n\n\n\nTable¬†3: Table of AIC and BIC values for each distribution\n\n\n\n\n\n\n\n(a) GEV\n\n\n\n\n\nModel\nCrit\nValue\n\n\n\n\nGEV\nAIC\n495.3076\n\n\nGEV\nBIC\n501.5907\n\n\n\n\n\n\n\n\n\n\n\n(b) Gumbel\n\n\n\n\n\nModel\nCrit\nValue\n\n\n\n\nGumbel\nAIC\n502.7966\n\n\nGumbel\nBIC\n506.9853\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2.2 Parametric trend-detection\nWhen covariates are to be incorporated into EVA, a common approach is to model them within the parameters of the distributions in a regression-like manner. To perform parametric trend detection with the GEV distribution, we model location parameter (Œº) as a function of time, transforming the model into a non-stationary GEV.\n\n\n\n\n\n\nTrend?\n\n\n\nThe decision to adopt a non-stationary (or more complex) model in FFA depends on whether the temporal trend is statistically significant.\nThe deviance test, based on likelihood ratio statistic (Coles, 2001), offers a formal approach to test this hypothesis by comparing the stationary and non-stationary models.\n\n\n\n6.4.2.2.1 Creating covariate\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs mentionned above, we use time index to account for trend in the AMF serie. We create in this chunk above a new column, TimeIndex, to be used as covariate.\n\n\n\n# add time index to use for non-stationary context fitting\nextractedAMF[[\"TimeIndex\"]] = 1:nrow(extractedAMF)\nhead(tibble(extractedAMF), 5)\n\n# A tibble: 5 √ó 4\n   Year Date        AMAX TimeIndex\n  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;int&gt;\n1  1958 1958-12-20  40.0         1\n2  1959 1959-03-07  36.9         2\n3  1960 1960-10-06  58.4         3\n4  1961 1961-12-11  18.4         4\n5  1962 1962-03-05  19.5         5\n\n\n\n\n6.4.2.2.2 Non-Stationary GEV fitting\nThe chunk below use the {extRemes} library to fit the Non-Stationary GEV (NS-GEV) distribution to the Annual Maximum Floods extracted earlier.\n\n# fit the GEV distribution to the AMF\nextractedAMF = data.frame(extractedAMF)\nnsgevfit = fevd(x=AMAX, location.fun=~TimeIndex, data=extractedAMF, type=\"GEV\", method=\"GMLE\")\n\nNow, we will print the results of the fit, using the pretty_gev_summary() function we have defined above. This will show the AIC and BIC values, as well as the estimated parameters of the fitted NS-GEV distribution.\n\n# show fitting sumary\nnsgevfit_res = pretty_gev_summary(nsgevfit, \"GEV\")\n\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\n    mu0     mu1   scale   shape \n20.5045 -0.0276  9.9322  0.3576 \n\nModel Fit Criteria:\nLogLik: 244.55\nAIC: 497.11\nBIC: 505.48\n\n\n\n\n\n\n\n\n\n\nWhat about the parameters?\n\n\n\nŒº(t) = Œº0 + Œº1¬∑t, THE time-varying location parameter can be expressed as a simple linear model, where:\n\nmu0 = Œº0 = 20.5045: is the intercept of the time-varying location parameter. It represents the baseline magnitude of the annual maximum flood (AMF) at the starting point of the time serie.\nmu1 = Œº1 = ‚Äì0.0276: is the slope of the location parameter with respect to time. It indicates a decreasing trend in the location (i.e., central tendency) of the AMF over time. Specifically, for each unit increase in time (i.e., each year), the location parameter decreases by 0.0276m^3.s^{-1}. This suggests that the typical magnitude of extreme floods has been gradually declining over the observation period.\n\n\n\nFinally, we show in Figure¬†7, the diagnostics from the NS-GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(nsgevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .4))\nplot(nsgevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n\n\n\n\n\n\n\nFigure¬†7: Diagnostic plots for the Non-Stationary Generalized Extreme Value (NS-GEV) distribution fitted to the Annual Maximum Flood (AMF) series\n\n\n\n\n\n\n\n6.4.2.2.3 Deviance test\n\n\n\n\n\n\nMathematical of the deviance test\n\n\n\nAs mentionned above, The Deviance test, or Likelihood Ratio (LR) test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit.\n D = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \nD represents the deviance test statistic value (D-statistic), log(ML_{NSGEV}) and log(ML_{SGEV}) are the maximised log-likelihood functions of the NSGEV and the SGEV, respectively.\n\n\n\n\n\n\n\n\n\n\nlog-likelihood function\n\n\n\nThe likelihood function itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n\n\nTo perform the Deviance test, we can use the lr.test(...) of the {extRemes}. The lr.test(...) just requires the previous fitted objects of class ‚Äúfevd‚Äù from the fevd(...) function:\n\ngevfit: model with fewer parameters (stationary GEV model)\nnsgevfitmodel with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the Deviance test is that simpler model is sufficient ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting c.alpha be the (1 - alpha) quantile of the chi-square distribution with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that D = 0 is rejected if D &gt; c.alpha (i.e., in favor of model Non-Stationary GEV model).\n\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n\n\n    Likelihood-ratio Test\n\ndata:  extractedAMF$AMAXAMAX\nLikelihood-ratio = 0.20007, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.6547\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nüö´ We fail to reject the null hypothesis.\n\nD-statistic (0.20) much smaller than the critical value (3.84).\np-value (0.6547) much larger than the significance level (0.05).\n\n\n\n\n\n\n\n\n\nTrend Significance\n\n\n\nThere is no significant evidence that the Non-Stationary GEV model (with a time-varying location parameter) provides a better fit than the stationary model. The trend is not statistically significant (p = 0.6547)."
  },
  {
    "objectID": "blockmaxima/index.html#exract-annual-maximum-floods-amf-from-the-daily-streamflow-serie",
    "href": "blockmaxima/index.html#exract-annual-maximum-floods-amf-from-the-daily-streamflow-serie",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "3.2 Exract Annual Maximum Floods (AMF) from the daily streamflow serie",
    "text": "3.2 Exract Annual Maximum Floods (AMF) from the daily streamflow serie\n1. To extract annual maximum floods (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\nextract_annual_max = function(df) \n{\n    df %&gt;%\n    mutate(Year = year(Date)) %&gt;%    # extract year from Date\n    group_by(Year) %&gt;%\n    filter(Q == max(Q)) %&gt;%          # keep only max flow per year\n    slice(1) %&gt;%                     # in case of ties, keep first max\n    ungroup() %&gt;%\n    select(Year, Date, AMAX=Q)\n}\n\n2. Now, we apply the function to the data to extract AMF for each year:\n\nextractedAMF = extract_annual_max(Qdata)\n\n\n\n\n\nTable¬†2: First (6) rows of the AMF extracted using the Block-Maxima approach\n\n\n\n\n\n\nYear\nDate\nAMAX\n\n\n\n\n1958\n1958-12-20\n39.96984\n\n\n1959\n1959-03-07\n36.89593\n\n\n1960\n1960-10-06\n58.41780\n\n\n1961\n1961-12-11\n18.44796\n\n\n1962\n1962-03-05\n19.47185\n\n\n1963\n1963-04-11\n29.21002"
  },
  {
    "objectID": "blockmaxima/index.html#trend-detection-1",
    "href": "blockmaxima/index.html#trend-detection-1",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.5 Trend detection",
    "text": "6.5 Trend detection\nOne question that arise when fitting non-stationary distribution in FFA is wether the trend is significant or not. In other words, the decision to use a non-stationary model (or a more complex) in FFA hinges on whether the trend is statistically significant. The deviance test, based on likelihood ratio (Coles, 2001), provides a formal way to assess this, while model selection criteria help balance fit and complexity.\n\n\n\n\n\n\n\n\nWhy the Deviance test?\n\n\n\nNon-stationary models are more complex and require careful validation, as a better fit doesn‚Äôt provide sufficient information to justify the addition of extra parameters.\n\n\n\n\n\n\n\n\nMathematical of the deviance test\n\n\n\nThe likelihood ratio (LR) test enables the selection of the best-fitting model between two nested models by evaluating the statistical significance of the difference in their deviance (D-statistic). D-statistic is defined as:\n D = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \nD represents the deviance test statistic value (D-statistic), log(ML_{NSGEV}) and log(ML_{SGEV}) are the maximised log-likelihood functions of the NSGEV and the SGEV, respectively.\n\n\n\n\n\n\n\n\n\n\nlog-likelihood function\n\n\n\nThe likelihood function itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n\n\nTo perform the LR test"
  },
  {
    "objectID": "blockmaxima/index.html#deviance-test",
    "href": "blockmaxima/index.html#deviance-test",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.5 Deviance test",
    "text": "6.5 Deviance test\n\n\n\n\n\n\nMathematical of the deviance test\n\n\n\nAs mentionned above, The Deviance test, or Likelihood Ratio (LR) test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit.\n D = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \nD represents the deviance test statistic value (D-statistic), log(ML_{NSGEV}) and log(ML_{SGEV}) are the maximised log-likelihood functions of the NSGEV and the SGEV, respectively.\n\n\n\n\n\n\n\n\n\n\nlog-likelihood function\n\n\n\nThe likelihood function itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n\n\nTo perform the Deviance test, we can use the lr.test(...) of the {extRemes}. The lr.test(...) just requires the previous fitted objects of class ‚Äúfevd‚Äù from the fevd(...) function:\n\ngevfit: model with fewer parameters (stationary GEV model)\nnsgevfitmodel with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the Deviance test is that simpler model is sufficient ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting c.alpha be the (1 - alpha) quantile of the chi-square distribution with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that D = 0 is rejected if D &gt; c.alpha (i.e., in favor of model Non-Stationary GEV model).\n\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n\n\n    Likelihood-ratio Test\n\ndata:  extractedAMF$AMAXAMAX\nLikelihood-ratio = 0.20007, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.6547\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n‚ùå We ail to reject the null hypothesis.\n\nD-statistic (0.20) much smaller than the critical value (3.84).\np-value (0.6547) much larger than the significance level (0.05).\n\n\n\n\n\n\n\n\n\nTrend Significance\n\n\n\nThere is no significant evidence that the Non-Stationary GEV model (with a time-varying location parameter) provides a better fit than the stationary model. The trend is not statistically significant (p = 0.6547)."
  },
  {
    "objectID": "blockmaxima/index.html#parametric-methods",
    "href": "blockmaxima/index.html#parametric-methods",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "5.2 Parametric methods",
    "text": "5.2 Parametric methods\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a non-stationary (NS) context.\n\n\n\n\n\n\nNote\n\n\n\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter (Œº) will be expressed as a linear function of time, denoted as Œº(t), leaving the others parameters constant."
  },
  {
    "objectID": "blockmaxima/index.html#probability-distributions",
    "href": "blockmaxima/index.html#probability-distributions",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.1 Probability distributions",
    "text": "6.1 Probability distributions\nWe will compare two probability distributions adapted to the Block-Maxima Framework: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n\n\n\n\n\nGEV Distribution\n\n\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n F(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} \nfor 1 + \\xi (x - \\mu)/\\sigma &gt; 0, where:\n\n\\mu is the location parameter.\n\\sigma &gt; 0 is the scale parameter.\n\\xi is the shape parameter.\n\nThe shape parameter \\xi determines the type of extreme value distribution:\n\nIf \\xi &gt; 0, the distribution is a Fr√©chet type.\nIf \\xi &lt; 0, the distribution is a Weibull type.\nIf \\xi = 0, the distribution is a Gumbel type.\n\n\n\n\n\n\n\n\n\nGumbel Distribution\n\n\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter \\xi = 0. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:"
  },
  {
    "objectID": "blockmaxima/index.html#models-fitting",
    "href": "blockmaxima/index.html#models-fitting",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.2 Models fitting",
    "text": "6.2 Models fitting\nWe will employ the Generalized Maximum Likelihood Estimation (GMLE) method to fit probability distributions to our data. The {extRemes} package offers the function fevd(..), which enables fitting a range of probability distributions ‚Äî such as the GEV, Gumbel distributions ‚Äî to a given dataset using three distinct fitting methods: L-moments, Maximum Likelihood Estimation (MLE), and GMLE.\n\n\n\n\n\n\n\n\nWhy the GML method?\n\n\n\nThe GMLE method use a prior distribution to constrain the GEVC shape parameter within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See Martins & Stedinger (2000) for more details).\n\n\nThe advantage of using GMLE and MLE over the L-moments method is that L-moments are limited in their ability to accommodate non-stationary processes. In contrast, GMLE and MLE can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection."
  },
  {
    "objectID": "blockmaxima/index.html#model-selection",
    "href": "blockmaxima/index.html#model-selection",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "6.3 Model selection",
    "text": "6.3 Model selection\nTo select the best probability distribution, we will use the AIC and AIC criteria. The model with the lowest AIC or BIC value will be selected.\n\n\n\n\n\n\nAIC (Akaike Information Criterion)\n\n\n\nThe AIC balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n \\text{AIC} = 2k - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\n\\hat{L} is the maximum value of the likelihood function for the model.\n\n\n\n\n\n\n\n\n\n\n\nHow to select the best model?\n\n\n\nThe model with the lowest AIC and BIC values is the best-suited model. AIC tends to favor models with better fit, even if more complex, while BIC favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection:\n\nif both criteria select the same model, it is strongly preferred\nif not, the choice depends on whether your goal is predictive accuracy (AIC) or model simplicity (BIC)\n\n\n\n\n\n\n\n\n\nBIC (Bayesian Information Criterion)\n\n\n\nBIC penalizes model complexity more heavily than AIC, especially for larger sample sizes. This means BIC tends to favor simpler models compared to AIC. The formula for BIC is:\n \\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \nwhere:\n\nk is the number of parameters in the model.\nn is the number of data points.\n\\hat{L} is the maximum value of the likelihood function for the model."
  },
  {
    "objectID": "blockmaxima/index.html#stationary-context-1",
    "href": "blockmaxima/index.html#stationary-context-1",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "8.1 Stationary context",
    "text": "8.1 Stationary context\nTo estimate return levels for given return periods, we can use the return.level(...) from the {extRemes} package:\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe return level is the value expected to be exceeded on average once every T years.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(gevfit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n\nfevd(x = extractedAMF$AMAX, type = \"GEV\", method = \"GMLE\")\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI  Estimate 95% upper CI\n2-year return level       19.60029  23.35205     27.10382\n5-year return level       31.27812  39.37944     47.48076\n10-year return level      38.73868  54.51701     70.29534\n20-year return level      43.75679  73.73668    103.71657\n50-year return level      43.88864 107.90748    171.92631\n100-year return level     35.61747 142.77898    249.94049\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe gevfit object is the gevfit object returned by the fevd(...) function when we have fitted a the stationary GEV model.\n\n\n\n\n\n\n\n\n\n‚ö†Ô∏è Warning\n\n\n\nFor simplicity and illsutration, we use the normal approximation method to compute confidence intervals for the estimated return levels. This method assumes the sampling distribution of the return level estimates is approximately normal (which becomes more reasonable with large sample sizes).\nFor more robust intervals, consider using bootstrap methods."
  },
  {
    "objectID": "blockmaxima/index.html#non-stationary-context",
    "href": "blockmaxima/index.html#non-stationary-context",
    "title": "Flood Frequency Analysis (FFA) with Block-Maxima",
    "section": "8.2 Non-Stationary context",
    "text": "8.2 Non-Stationary context\nIn non-stationary context, since the location parameter is time-dependent, return levels have to be calculated for each year over time period. We refer to this as ‚Äúeffective‚Äù return levels The same return.level(...) from the {extRemes} package is used to compute \"effective\" return levels.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe nsgevfit object is the object returned by the fevd(...) function when we have fitted a the non-stationary GEV model.\n\n\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgevfit, return.period=rperiods)\n\nFor easy visualization, we will plot the effective return levels for each year over time period. Lets convert the output of return.level(...) into a clean data.frame :\n\n# Convert to data.frame\ndf_rlevels &lt;- data.frame(year=extractedAMF$Year,\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n\n  year       Q2       Q5      Q10      Q20      Q50     Q100\n1 1958 24.36654 40.19203 54.81061 73.04541 104.8170 136.6172\n2 1959 24.33894 40.16443 54.78301 73.01781 104.7894 136.5896\n3 1960 24.31134 40.13683 54.75541 72.99021 104.7618 136.5620\n4 1961 24.28374 40.10923 54.72781 72.96261 104.7342 136.5344\n5 1962 24.25614 40.08163 54.70021 72.93501 104.7066 136.5068\n6 1963 24.22854 40.05403 54.67261 72.90741 104.6790 136.4792\n\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with ggplot:\n\ndataplot = df_rlevels |&gt; pivot_longer(-1, names_to=\"RL\", values_to=\"Q\")\n\nFinally, we plot the effective return levels:\n\nrlplot = dataplot %&gt;%\n         ggplot(aes(x=year, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01), limits = c(20, 150), breaks=seq(25, 200, by=25)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time (Year)\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n\nFigure¬†8 helps in understanding the variations and trends in flood quantiles across different time periods and stations.‚Äù\n\n\n\n\n\n\n\n\nFigure¬†8: Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles."
  }
]