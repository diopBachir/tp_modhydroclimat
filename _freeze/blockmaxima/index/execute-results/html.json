{
  "hash": "ee48ecc10a18a68f726844bc7851e4ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Flood Frequency Analysis (FFA) with Block-Maxima\"\n---\n\n\n# Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n```\n:::\n\n\n# Data import {#sec-bmdata}\n\n::: {.callout-note icon=\"false\"}\n## Download the data used in this section\n\n<a href=\"https://github.com/diopBachir/tp_eva/blob/master/blockmaxima/data/discharge.csv\" download class=\"download-btn\">üì• Download CSV</a>\n:::\n\nLet‚Äôs consider the formatted daily streamflow data file for the target catchment. We can read the data into a data frame using the `read.csv()` function. We will also set the column names using the `col.names` argument, and use the `lubridate` library to prepare a time serie.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\n\nfilename <- \"./data/discharge.csv\"\nQdata <- read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))\n\n# construct time serie\nQdata$Date <- paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep = \"-\")\nQdata$Date <- ymd(Qdata$Date)\n\n# final time serie\nQdata <- Qdata[, c(\"Date\", \"Q\")]\n```\n:::\n\n\nThe output is a data frame (or a matrix) with 22799 rows and 2 columns.\n\n\n::: {#tbl-bm_loaddata .cell tbl-cap='First (6) lines of the daily streamflow data for Block-Maxima approach'}\n::: {.cell-output-display}\n\n\n|      |    Date    |    Q     |\n|:-----|:----------:|:--------:|\n|22794 | 2020-12-26 | 2.326204 |\n|22795 | 2020-12-27 | 2.173519 |\n|22796 | 2020-12-28 | 2.193727 |\n|22797 | 2020-12-29 | 2.090440 |\n|22798 | 2020-12-30 | 1.966944 |\n|22799 | 2020-12-31 | 1.854676 |\n\n\n:::\n:::\n\n\n::: {.column-margin}\n\n::: {.callout-note}\nWe can display the structure of the data frame using the `str()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Qdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t22799 obs. of  2 variables:\n $ Date: Date, format: \"1958-08-01\" \"1958-08-02\" ...\n $ Q   : num  0.236 0.184 0.144 0.144 0.144 ...\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# Extreme events sampling\n\n\n## The block maxima approach\n\nThe **Block-Maxima Approach [@gumbel1958statistics]**, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and reduce the possiblity of serial dependence.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima](index_files/figure-html/fig-bm_illust-1.png){#fig-bm_illust width=672}\n:::\n:::\n\n\n## Exract annual maximum floods (AMF) from the daily streamflow serie\n\n**1.** To extract annual maximum floods (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_annual_max <- function(df) {\n    df %>%\n        mutate(Year = year(Date)) %>% # extract year from Date\n        group_by(Year) %>%\n        filter(Q == max(Q)) %>% # keep only max flow per year\n        slice(1) %>% # in case of ties, keep first max\n        ungroup() %>%\n        select(Year, Date, AMAX = Q)\n}\n```\n:::\n\n\n**2.** Now, we apply the function to the data to extract AMF for each year:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextractedAMF <- extract_annual_max(Qdata)\n```\n:::\n\n::: {#tbl-bm_extract .cell tbl-cap='First (6) rows of the AMF extracted using the Block-Maxima approach'}\n::: {.cell-output-display}\n\n\n| Year |    Date    |   AMAX   |\n|:----:|:----------:|:--------:|\n| 1958 | 1958-12-20 | 39.96984 |\n| 1959 | 1959-03-07 | 36.89593 |\n| 1960 | 1960-10-06 | 58.41780 |\n| 1961 | 1961-12-11 | 18.44796 |\n| 1962 | 1962-03-05 | 19.47185 |\n| 1963 | 1963-04-11 | 29.21002 |\n\n\n:::\n:::\n\n\n## Display sampled events\n\nNow, we overlap the extracted **Annual Maximum Flow** with the original daily discharge time serie to visualize the sampling process. We use the functions from the `dplyr` and `ggplot2` library to create the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\namf.plot <- ggplot() +\n    geom_line(data = Qdata, aes(x = Date, y = Q), color = \"steelblue\", size = 0.7, alpha = 0.8) +\n    geom_point(data = extractedAMF, aes(x = Date, y = AMAX), color = \"red\", size = 1.2) +\n    labs(x = \"Date\", y = \"Discharge (Q)\") +\n    theme_minimal(base_size = 13) +\n    theme(\n        axis.title = element_text(size = 12, face = \"bold\"),\n        axis.text = element_text(size = 12, color = \"black\")\n    )\nprint(amf.plot)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach](index_files/figure-html/fig-amf_with_dailystreamflow-1.png){#fig-amf_with_dailystreamflow width=672}\n:::\n:::\n\n\n\n\n# Autocorrelation testing\n\n::: {.column-margin}\n::: {.callout-tip}\n## Recall\nThe `extractedAMF` variable used in the following code blocs is created previously in the **Sampling Events** section\nThe `acf` function is a R-base function, no installation needed\n:::\n:::\n\n\n## What is autocorrelation? {#sec-autocorrelation}\n\n**`Autocorrelation (or autocovariance)`** refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ...). An autocorrelated series is thus correlated with itself, with a given lag (please refer [here](https://perso.ens-lyon.fr/lise.vaudor/autocorrelation-de-series-temporelles-ou-spatiales/) for more details). \n\n::: {.callout-note}\n\n## To assess autocorrelation, two methods will be used in this lab\n\nüîç **Correlogram:**\n\n* Visualization of correlation coefficients between the serie and its lags\n* Based on the [autocorrelation function (ACF)](https://medium.com/@kis.andras.nandor/understanding-autocorrelation-and-partial-autocorrelation-functions-acf-and-pacf-2998e7e1bcb5)\n* Helps identify the presence and extent of dependencies over time\n\nüìä **Wald-Wolfowitz Test [@wald_test_1940]:**\n\n* Non-parametric to assess the randomness of a time serie\n* Used to detect the absence of randomness in the serie\n* **Null hypothesis: The sequence of observations is random**\n\n:::\n\n### Correlogram\n\nIf you execute the above given chunk, it generates the @fig-acfplot as output:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# compute autocorrelation lag\nbacf <- acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf <- with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn <- length(extractedAMF$AMAX)\nconf_limit <- 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +\n    geom_hline(aes(yintercept = 0)) +\n    geom_bar(stat = \"identity\", position = \"identity\", width = .2) +\n    geom_hline(yintercept = conf_limit, linetype = \"dashed\", color = \"red\") +\n    geom_hline(yintercept = -conf_limit, linetype = \"dashed\", color = \"red\") +\n    labs(y = \"ACF\", x = \"Lag\") +\n    theme_minimal() +\n    theme(\n        axis.title = element_text(size = 10, face = \"bold\", family = \"Times\"),\n        axis.text = element_text(size = 10, color = \"black\", family = \"Times\")\n    )\n```\n\n::: {.cell-output-display}\n![Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.](index_files/figure-html/fig-acfplot-1.png){#fig-acfplot width=672}\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n:::\n:::\n\n\n### Wald-Wolfowitz (WW) test {#sec-wwtest}\n\nIn the chunk below, we use the `ww.test(...)` function from `{trend}` package to perform the `WW test` at the **0.05 significance level**, to test the hypothesis that the maximum annual flood is a random variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwwtest <- trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWald-Wolfowitz test for independence and stationarity\n\ndata:  extractedAMF$AMAX\nz = 0.73738, n = 60, p-value = 0.4609\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.4609:`** indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness).\n:::\n:::\n\n# Trend detection {#sec-trend}\n\n## Non-parametric methods\n\n### The Mann-Kendall (MK) test {#sec-mk}\n\nThe MK test's null hypothesis (H0) assumes no trend in the data [@mann1945nonparametric; @kendall_rank_1975]. Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that **are rarely normally distributed**. Additionally, outlier values have very little influence on the results. The Mann-Kendall test statistic $S$ is defined as:\n\n$$\nS = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\text{sgn}(x_j - x_i) \\tag{1}\n$$\n\nThe sign function $sgn())$ is given by:\n\n$$\n\\text{sgn}(x_j - x_i) =\n\\begin{cases}\n+1 & \\text{if } (x_j - x_i) > 0 \\\\\n0 & \\text{if } (x_j - x_i) = 0 \\\\\n-1 & \\text{if } (x_j - x_i) < 0\n\\end{cases} \\tag{2}\n$$\n\nwhere $x_i$ and $x_j$ are values of the variable at time $i$ and $j$, respectively, and $n$ is the total number of observations.\n\nA significance level (commonly $\\alpha = 0.05$) and the trend slope indicating the direction and magnitude of the trend are key characteristics of the Mann-Kendall test. If $n > 8$, the test statistic $S$ approximates a normal distribution. In this case, the mean of $S$ is zero and its variance is:\n\n$$\n\\text{var}(S) = \\frac{n(n - 1)(2n + 5)}{18} \\tag{3}\n$$\n\nThe standardized test statistic $Z$, which indicates the direction of the trend, is calculated as:\n\n$$\nZ =\n\\begin{cases}\n\\frac{S - 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S > 0 \\\\\n0 & \\text{if } S = 0 \\\\\n\\frac{S + 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S < 0\n\\end{cases} \\tag{4}\n$$\n\nThe null hypothesis $H_0$ (no trend) is rejected if $|Z| > 1.96$ at the 95% confidence level.\n\n- A **positive trend** is significant if $Z > 1.96$\n- A **negative trend** is significant if $Z < -1.96$\n\n\nWe use the `mk.test(...)` function from `{trend}` package to perform the `MK test` at the **0.05 significance level** to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n::: {.column-margin}\n::: {.callout-note}\n**`Non-Stationarity (NS)`** refers to changes in statistical properties over time.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmktest <- trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.2183:`** there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\n\n**`tau = -0.1096665:`** Kendall's Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\n\n**`z = -1.231:`** the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n:::\n:::\n\n@fig-amfevolution shows the evolution of the AMF over time and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlineplot <- ggplot(extractedAMF, aes(Year, AMAX)) +\n    geom_line(color = \"blue\", linewidth = 1) +\n    geom_smooth(color = \"red\", linewidth = 1, method = \"lm\", se = F)\nprint(lineplot)\n```\n\n::: {.cell-output-display}\n![Evolution of the annual peak flood over time.](index_files/figure-html/fig-amfevolution-1.png){#fig-amfevolution width=672}\n:::\n:::\n\n\n### Sen's slope estimator {#sec-sens}\n\nThe **non-parametric method of Sen [-@Sen1968]**,  also referred to as the **Theil‚ÄìSen estimator**, is commonly combined with the Mann-Kendall test to estimate the slope $(\\beta)$ of the of a trend. Sen‚Äôs algorithm looks at every pair of data points, calculates the slope between them, then uses the the median of all those slopes as the estimated trend.\n\n$$\n\\beta = \\text{median} \\left( \\frac{x_j - x_i}{j - i} \\right), \\quad , ‚àÄ\\quad j > i \\tag{5}\n$$\n\nTo express the Sen's slope as a **percentage change over the period**, it can be normalized by the mean:\n\n$$\n\\%\\beta = \\left( N \\cdot \\frac{\\beta}{\\bar{x}} \\right) \\times 100 \\tag{6}\n$$\nwhere $\\%\\beta$ is the relative trend rate over the period, and $\\bar{x}$ represents the mean value of the variable across the period, $N$ is the number of data points (i.e., the length of period).\n\nThe Sen's slope can be easily computed using the `sens.slope(...)` function from the `{trend}` package, as in the chunk below:\n\n::: {.column-margin}\n::: {.callout-tip}\n## Sen slope\n**`Slope = -0.1187375 :`** the median trend is slightly negative, suggesting a weak decline of $-0.11 \\quad mm/year$.\n\n**`p-value = 0.2183:`** this negative trend is not statistically significant.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute Sen's slope and intercept\nsenslope <- trend::sens.slope(x = extractedAMF$AMAX)\nprint(senslope)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSen's slope\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true z is not equal to 0\n95 percent confidence interval:\n -0.3115676  0.0768529\nsample estimates:\nSen's slope \n -0.1187375 \n```\n\n\n:::\n:::\n\n\n\nYet, the `sens.slope(...)` function returns the **absolute** slope (i.e., the median change per time unit). It is more meaningful to express the trend as a **relative percentage** change over the time period. \n\nTo do this, we define a **wrapper function** that calculates the Sen's slope and then converts it into a relative trend rate.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nsens_slope_percent <- function(x, time_step = NULL) {\n    #' Compute Sen's slope as a percentage change over the full time period\n    #' @param x A numeric vector of observations\n    #' @param time_step Optional (e.g., year) used for display purpose only\n    #' @return Relative percent change over the period\n\n    require(trend)\n\n    N <- length(x)\n    result <- sens.slope(x)\n    slope <- result$estimates # Absolute slope (per time unit)\n    p_val <- result$p.value # p-value for trend test\n    mean_val <- mean(x, na.rm = TRUE)\n\n    percent_change <- (N * slope) / mean_val * 100\n\n    cat(\"Absolute Sen's slope: \", round(slope, 4), \"\\n\", sep = \"\")\n    cat(\"P-value: \", format.pval(p_val, digits = 4), \"\\n\", sep = \"\")\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%\", \"\\n\", sep = \"\")\n}\n```\n:::\n\n\nNow, we can compute the **relative Sen's slope** for the annual maximum flow data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute Sen's slope as a percentage change\nsens_slope_percent(extractedAMF$AMAX, time_step = \"year\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute Sen's slope: -0.1187\nP-value: 0.2183\nRelative trend rate: -23.97%\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## Relative Sen's slope\nThe annual maximum flow shows a decreasing trend of ‚àí24% relative to its long-term average.\n:::\n:::\n\n\n## Parametric methods\n\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a `non-stationary (NS)` context. \n\n::: {.callout-note}\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter **`(Œº)`** will be expressed as a linear function of time, denoted as **`Œº(t)`**, leaving the others parameters constant.\n:::\n\n# Flood frequency analysis (FFA)\n\n## Probability distributions\n\nWe will compare two probability distributions adapted to the **`Block-Maxima Framework`**: the **Generalized Extreme Value (GEV)** and the **Gumbel** probability distributions.\n\n::: {.callout-note}\n## GEV distribution\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the **Gumbel**, **Fr√©chet**, and **Weibull** distributions. According to the [**Extreme Value Theorem**](../overview/index.qmd#sec-evt), the **GEV** distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The **cumulative distribution function (CDF)** of the **GEV** distribution is given by:\n\n$$ \nF(x; \\mu, \\alpha, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\alpha} \\right) \\right]^{-1/\\xi} \\right\\} \n\\tag{7}\n$$\n\nfor $1 + \\xi (x - \\mu)/\\alpha > 0$, where, $\\mu$ is the location parameter, $\\alpha > 0$ is the scale parameter, and $\\xi$ is the shape parameter.\n\nThe shape parameter $\\xi$ determines the type of extreme value distribution:\n\n* If $\\xi > 0$, the distribution is a Fr√©chet type.\n* If $\\xi < 0$, the distribution is a Weibull type.\n* If $\\xi = 0$, the distribution is a Gumbel type.\n\n:::\n\n::: {.callout-note}\n\n## Gumbel distribution\n\nThe **Gumbel** distribution is a special case of the GEV distribution when the shape parameter $\\xi = 0$. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. The **cumulative distribution function (CDF)** of the **Gumbel** distribution is given by:\n\n$$\nF(x; \\mu, \\alpha) = \\exp\\left\\{ -\\exp\\left[ -\\frac{x - \\mu}{\\alpha} \\right] \\right\\}\n\\tag{8}\n$$ \nwhere $\\mu$ is the location parameter, $\\alpha > 0$ is the scale parameter, and $x$ is the variable\n\n:::\n\n## Models fitting\n\nWe will employ the **`Generalized Maximum Likelihood Estimation (GMLE)`** method to fit probability distributions to our data. The **`{extRemes}`** package offers the function **`fevd(..)`**, which enables fitting a range of probability distributions ‚Äî such as the **GEV**, **Gumbe**l distributions ‚Äî to a given dataset using three distinct fitting methods: **`L-moments`**, **`Maximum Likelihood Estimation (MLE)`**, and **`GMLE`**.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Why the **`GML`** method?\n\nThe **`GMLE`** method use a prior distribution to constrain the **`GEVC shape parameter`** within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See [**`Martins & Stedinger (2000)`**](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1999WR900330) for more details).\n:::\n:::\n\nThe advantage of using **`GMLE`** and **`MLE`** over the **`L-moments`** method is that **`L-moments`** are limited in their ability to accommodate non-stationary processes. In contrast, **`GMLE`** and **`MLE`** can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection.\n\n## Model selection\n\nTo select the best probability distribution, we will use the **`AIC`** and **`AIC`** criteria. The model with the **lowest** **`AIC`** and **`BIC`** values is the best-suited model. **`AIC`** tends to favor models with better fit, even if more complex, while **`BIC`** favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Joint use of AIC and BIC\n* if both criteria select the same model, it is strongly preferred\n* if not, the choice depends on whether your goal is **predictive accuracy (AIC)** or **model simplicity (BIC)**\n:::\n:::\n\n::: {.callout-note}\n## AIC (Akaike Information Criterion)\n\nThe **`AIC`** balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n\n$$ \n\\text{AIC} = 2k - 2\\ln(\\hat{L}) \\tag{9}\n$$\nwhere $k$ is the number of parameters in the model, $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n::: {.callout-note}\n## BIC (Bayesian Information Criterion)\n\n**`BIC`** penalizes model complexity **more heavily than `AIC`**, especially for larger sample sizes. This means **`BIC`** tends to favor simpler models compared to AIC. The formula for BIC is:\n$$ \n\\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \\tag{10}\n$$\nwhere: $k$ is the number of parameters in the model, $n$ is the number of data points, and $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n\n## Perform FFA\n\n### Setup utils {#sec-utils}\n\nTo pretty-print the summary of our models' fitting, we will create a custom function that will take the fitted model as an argument, extract and format key components of the fiited object, and print the output: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npretty_fit_summary = function(fitobj, model) {\n  #' Pretty print summary of a  extreme value model fit\n  #'\n  #' @param fitobj Fitted model object from extRemes::fevd()\n  #' @param model Character string describing the model (e.g., \"GEV\")\n  \n  cat(\"===\", model, \" Fit Summary ===\\n\\n\")\n  \n  # Estimated Parameters\n  cat(\"Estimated Parameters:\\n\")\n  params = fitobj$results$par\n  print(round(params, 4))\n  \n  # Log-likelihood, AIC, and BIC\n  look = summary(fitobj, silent=TRUE)\n  cat(\"\\nModel Fit Criteria:\\n\")\n  cat(sprintf(\"LogLik: %.2f\\n\", look$nllh))\n  cat(sprintf(\"AIC: %.2f\\n\", look$AIC))\n  cat(sprintf(\"BIC: %.2f\\n\", look$BIC))\n\n  # return the AIC and BIC values for further comparison\n  return(data.frame(Model=model, Crit=c(\"AIC\", \"BIC\"), Value=c(look$AIC, look$BIC)))\n}\n```\n:::\n\n\n### Distributions fitting\n\n#### Stationary context\n\n::: {.panel-tabset}\n\n## [GEV fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the GEV distribution to the **Annual Maximum Floods** extracted earlier. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the GEV distribution to the AMF\nlibrary(extRemes)\ngevfit = fevd(x=extractedAMF$AMAX, type=\"GEV\", method=\"GMLE\")\n```\n:::\n\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted GEV distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show fitting sumary\ngevfit_res = pretty_fit_summary(gevfit, \"GEV\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale    shape \n 19.4868   9.8254   0.3818 \n\nModel Fit Criteria:\nLogLik: 244.65\nAIC: 495.31\nBIC: 501.59\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n**`location = 19.4868:`** is the **`central tendency (Œº)`** of the GEV distribution. It defines the central position of the distribution and serves as a baseline around which extreme values are distributed.\n\n**`scale = 9.932:`** reflects the spread or variability of the extreme values. In our case, the AMF distribution has moderate spread around the shifting central value **`Œº(t)`**.\n\n**`shape = 0.3576:`** this positive shape parameter implies a Fr√©chet-type (heavy-tailed) distribution. This means there is greater probability of very large flood events compared to a Gumbel (shape = 0) or Weibull (shape < 0) distribution.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-stationary, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), and  quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .043))\nplot(gevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for the Generalized Extreme Value (GEV) distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context](index_files/figure-html/fig-gevfitdiag-stationary-1.png){#fig-gevfitdiag-stationary width=672}\n:::\n:::\n\n\n## [Gumbel fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the Gumbel distribution to the **Annual Maximum Floods** extracted earlier. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the Gumbel distribution to the AMF\ngumfit = fevd(x=extractedAMF$AMAX, type=\"Gumbel\", method=\"GMLE\")\n```\n:::\n\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined previously. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted Gumbel distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show fitting sumary\ngumfit_res = pretty_fit_summary(gumfit, \"Gumbel\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Gumbel  Fit Summary ===\n\nEstimated Parameters:\nlocation    scale \n 21.7606  12.3003 \n\nModel Fit Criteria:\nLogLik: 249.40\nAIC: 502.80\nBIC: 506.99\n```\n\n\n:::\n:::\n\n\nFinally, we show in @fig-gumfitdiag-stationary, the diagnostics from the Gumbel distribution fitted to the Annual Maximum Floods:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gumfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .035))\nplot(gumfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for the Gumbel distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context](index_files/figure-html/fig-gumfitdiag-stationary-1.png){#fig-gumfitdiag-stationary width=672}\n:::\n:::\n\n\n## [Best-suited model]{.custom-tabset-title2}\n\nAs mentionned previously, the model with the lowest **`AIC`** and **`BIC`** values is the best-suited model. The **`pretty_fit_summary()`** function we have defined previously return a data.frame of **AIC** and **BIC** values for each distribution. Lets compare these values to select the best-suited distribution for the next steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(gevfit_res)\nprint(gumfit_res)\n```\n:::\n\n::: {#tbl-bm_aicbictable .cell layout-ncol=\"2\" tbl-cap='Table of AIC and BIC values for each distribution' tbl-subcap='[\"GEV\",\"Gumbel\"]'}\n::: {.cell-output-display}\n\n\n| Model | Crit |  Value   |\n|:-----:|:----:|:--------:|\n|  GEV  | AIC  | 495.3076 |\n|  GEV  | BIC  | 501.5907 |\n\n\n:::\n\n::: {.cell-output-display}\n\n\n| Model  | Crit |  Value   |\n|:------:|:----:|:--------:|\n| Gumbel | AIC  | 502.7966 |\n| Gumbel | BIC  | 506.9853 |\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## ‚úÖ Best-suited: GEV\nBoth **AIC and BIC are lower for the GEV model**, which suggests that it provides a better balance of fit and complexity than the Gumbel model.\n:::\n:::\n\n:::\n\n\n#### Parametric trend-detection\n\nWhen covariates are to be incorporated into EVA, a common approach to account for non-stationary is to incorporate covariates within the parameters of the distributions in a **`regression-like manner`**. To perform **parametric trend detection**, we model the **`location parameter (Œº)`** as a function of time, transforming the models into **`non-stationary distributions`**. The location parameter of the non-stationnary GEV and Gumbel is computed as follow:\n$$\n\\mu (t) = \\mu 0 + \\mu 1¬∑t \\tag{11}\n$$\nwhere $\\mu (t)$ is the time-dependant location parameter, $t$ is the time index, $\\mu 0$ is the intercept, and $\\mu 1$ is the slope.\n\n::: {.callout-tip}\n# Trend?\nThe decision to adopt a non-stationary (or more complex) model in FFA depends on whether the temporal trend is **`statistically significant`**.\n\nThe **`deviance test`**, based on **`likelihood ratio statistic`** [(Coles, 2001)](https://link.springer.com/book/10.1007/978-1-4471-3675-0), offers a formal approach to test this hypothesis by comparing the stationary and non-stationary models.\n:::\n\n##### Creating covariate\n\n::: {.column-margin}\n::: {.callout-note}\nAs mentionned above, we use **time index** to account for trend in the AMF serie. We create in this chunk above a new column, `TimeIndex`, to be used as covariate.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add time index to use for non-stationary context fitting\nextractedAMF[[\"TimeIndex\"]] = 1:nrow(extractedAMF)\nhead(tibble(extractedAMF), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 √ó 4\n   Year Date        AMAX TimeIndex\n  <dbl> <date>     <dbl>     <int>\n1  1958 1958-12-20  40.0         1\n2  1959 1959-03-07  36.9         2\n3  1960 1960-10-06  58.4         3\n4  1961 1961-12-11  18.4         4\n5  1962 1962-03-05  19.5         5\n```\n\n\n:::\n:::\n\n\n\n##### Non-Stationary GEV fitting\n\nThe chunk below use the **` {extRemes} `** library to fit the **`Non-Stationary GEV (NS-GEV) distribution`**  to the **Annual Maximum Floods** extracted earlier. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the GEV distribution to the AMF\nextractedAMF = data.frame(extractedAMF)\nnsgevfit = fevd(x=AMAX, location.fun=~TimeIndex, data=extractedAMF, type=\"GEV\", method=\"GMLE\")\n```\n:::\n\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted **`NS-GEV`** distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show fitting sumary\nnsgevfit_res = pretty_fit_summary(nsgevfit, \"GEV\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== GEV  Fit Summary ===\n\nEstimated Parameters:\n    mu0     mu1   scale   shape \n20.5045 -0.0276  9.9322  0.3576 \n\nModel Fit Criteria:\nLogLik: 244.55\nAIC: 497.11\nBIC: 505.48\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n* **`mu0 = Œº0 = 20.5045:`** is the **`intercept`** of the time-varying location parameter. It represents the baseline magnitude of the annual maximum flood (AMF) at the starting point of the time serie.\n\n* **`mu1 = Œº1 = ‚Äì0.0276:`** is the **`slope`** of the location parameter with respect to time. It indicates a decreasing trend in the location (i.e., central tendency) of the AMF over time. Specifically, for each unit increase in time (i.e., each year), the location parameter decreases by $0.0276\\,mm$. This suggests that the typical magnitude of extreme floods has been gradually declining over the observation period.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-ns, the diagnostics from the **`NS-GEV`** distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(nsgevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .4))\nplot(nsgevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for the Non-Stationary Generalized Extreme Value (NS-GEV) distribution fitted to the Annual Maximum Flood (AMF) series](index_files/figure-html/fig-gevfitdiag-ns-1.png){#fig-gevfitdiag-ns width=672}\n:::\n:::\n\n\n##### Deviance test {#sec-deviancetest}\n\n::: {.callout-note}\n## Mathematical of the deviance test\nAs mentionned above, The **Deviance test**, or **Likelihood Ratio (LR)** test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit. \n\n$$ \nD = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \n\\tag{12}\n$$\n\n$D$ represents the deviance test statistic value $(D-statistic)$, $log(ML_{NSGEV})$ and $log(ML_{SGEV})$ are the maximised log-likelihood functions of the $NSGEV$ and the $SGEV$, respectively.\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## log-likelihood function\nThe **`likelihood function`** itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n:::\n:::\n\nTo perform the **`Deviance test`**, we can use the **`lr.test(...)`** of the **`{extRemes}`**. This function just requires the previous fitted objects of class **`‚Äúfevd‚Äù`** from the **`fevd(...)`** function: \n\n* **`gevfit`**: model with fewer parameters (stationary GEV model)\n* **`nsgevfit`**model with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the **Deviance test** is that **simpler model is sufficient** ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting **`c.alpha`** be the **`(1 - alpha)`** quantile of the **`chi-square distribution`** with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that $D = 0$ is rejected if $D > c.alpha$ (i.e., in favor of model Non-Stationary GEV model).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLikelihood-ratio Test\n\ndata:  extractedAMF$AMAXAMAX\nLikelihood-ratio = 0.20007, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.6547\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## Interpretation\nüö´ We fail to reject the null hypothesis.\n\n* **`D-statistic (0.20)`** much smaller than the **`critical value (3.84)`**.\n* **`p-value (0.6547)`** much larger than the **`significance level (0.05)`**.\n:::\n:::\n\n\n::: {.callout-note}\n## Trend significance\nThere is **no significant evidence** that the **Non-Stationary GEV model (with a time-varying location parameter)** provides a better fit than the **stationary model**. The trend is not statistically significant (p = 0.6547).\n:::\n\n\n# Trend analysis conclusions\n\nüîé **Trend Detection Methods Used**\n\n* Parametric: **`Non-stationary GEV model`** with **time-varying location parameter**\n* Non-parametric: **`Mann-Kendall test`**\n\nüö´ **No Significant Trend Detected**\n\n* Both methods consistently indicate **no statistically significant trend** in the annual maximum flow series.\n* The **Likelihood Ratio Test** showed a high p-value (0.65), and the **Mann-Kendall test** similarly returned a non-significant result (p = 0.22).\n\n‚úÖ **Modeling Implication**\n\n* The results support the use of a stationary GEV model.\n* There is **no strong evidence** of temporal change in flood extremes over the study period.\n\n\n# Flood quantiles estimation\n\n## Stationary context\n\nTo estimate **return levels** for given **return periods**, we can use the **`return.level(...)`** from the **`{extRemes}`** package: \n\n::: {.column-margin}\n::: {.callout-note}\nThe **`return level`** is the value expected to be exceeded on average **once every T years**.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(gevfit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfevd(x = extractedAMF$AMAX, type = \"GEV\", method = \"GMLE\")\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI  Estimate 95% upper CI\n2-year return level       19.60029  23.35205     27.10382\n5-year return level       31.27812  39.37944     47.48076\n10-year return level      38.73868  54.51701     70.29534\n20-year return level      43.75679  73.73668    103.71657\n50-year return level      43.88864 107.90748    171.92631\n100-year return level     35.61747 142.77898    249.94049\n```\n\n\n:::\n:::\n\n\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`gevfit object`** is the **`gevfit`** object returned by the **`fevd(...)`** function when we have fitted a the **stationary GEV model**.\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-warning icon=\"false\"}\n## ‚ö†Ô∏è Warning\nFor simplicity and illsutration, we use the **normal approximation method** to compute confidence intervals for the estimated return levels. This method assumes the **sampling distribution** of the return level estimates is **approximately normal** (which becomes more reasonable with large sample sizes).\n\n**For more robust intervals, consider using `bootstrap` methods.**\n:::\n:::\n\n## Non-Stationary context\n\nIn non-stationary context, since the **location parameter is time-dependent**, **return levels** have to be calculated for each year over time period. We refer to this as **`‚Äúeffective‚Äù return levels`** The same **`return.level(...)`** from the **`{extRemes}`** package is used to compute **`\"effective\" return levels`**.\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`nsgevfit object`** is the object returned by the **`fevd(...)`** function when we have fitted a the **non-stationary GEV model**.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgevfit, return.period=rperiods)\n```\n:::\n\n\nFor easy visualization, we will plot the **`effective return levels`** for each year over time period. Lets convert the output of **`return.level(...)`** into a clean data.frame :\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to data.frame\ndf_rlevels <- data.frame(year=extractedAMF$Year,\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  year       Q2       Q5      Q10      Q20      Q50     Q100\n1 1958 24.36654 40.19203 54.81061 73.04541 104.8170 136.6172\n2 1959 24.33894 40.16443 54.78301 73.01781 104.7894 136.5896\n3 1960 24.31134 40.13683 54.75541 72.99021 104.7618 136.5620\n4 1961 24.28374 40.10923 54.72781 72.96261 104.7342 136.5344\n5 1962 24.25614 40.08163 54.70021 72.93501 104.7066 136.5068\n6 1963 24.22854 40.05403 54.67261 72.90741 104.6790 136.4792\n```\n\n\n:::\n:::\n\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with **`ggplot`**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataplot = df_rlevels |> pivot_longer(-1, names_to=\"RL\", values_to=\"Q\")\n```\n:::\n\n\nFinally, we plot the **`effective return levels`**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrlplot = dataplot %>%\n         ggplot(aes(x=year, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01), limits = c(20, 150), breaks=seq(25, 200, by=25)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time (Year)\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n```\n:::\n\n\n@fig-effective-return-levels helps in understanding the variations and trends in flood quantiles across different time periods.\"\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles.](index_files/figure-html/fig-effective-return-levels-1.png){#fig-effective-return-levels width=672}\n:::\n:::\n\n\n# Uncertainty estimation\n\nComputing uncertainties in FFA is essential as it helps quantify the **confidence** and **reliability** of estimated flood magnitudes and return levels.\n\nüîç **Acknowledge Data Limitations**\n\nHydrological records are often short or incomplete. Uncertainty analysis helps reflect the limited information available and avoids overconfidence in estimates.\n\nüìä **Support Risk-Based Decision Making**\n\nInfrastructure design, floodplain mapping, and insurance policies rely on return levels. Knowing the uncertainty helps stakeholders assess risk margins and make safer choices.\n\nüìà **Evaluate Model Robustness**\n\nComparing uncertainty across models (e.g., GEV vs Gumbel) can guide model selection and reveal sensitivity to assumptions or input data.\n\n::: {.callout-note icon=\"false\"}\n## üåä Want to Go Further? Quantify Your Confidence!\nThe simpler way to estimate the uncertainty is to compute a **normalized uncertainty range**, as defined below:\n\n$$ \n\\text{Unc [-]} = \\frac{Q_{97.5} - Q_{2.5}}{Q_{est}} \n\\tag{13}\n$$\nwhere $Q_{97.5}$ is the upper (97.5%) bound of the confidence interval, $Q_{2.5}$ is the lower (2.5%) bound of the confidence interval, $Q_{est}$ is the estimated quantile.\n:::\n\n\n# References\n\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}