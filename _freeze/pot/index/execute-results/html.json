{
  "hash": "3c01aac0ebcf7790bfc9aab4c4237adb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Peaks-Over-Threshold (POT) Modelling\"\n---\n\n\n# Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n```\n:::\n\n\n\n# The threshold exceedance method\n\nThe **threshold exceedance method**, often referred to as the **peaks over threshold (POT)**, is based on the **`Pickands‚ÄìBalkema‚Äìde Haan Theorem`**, sometimes referred to as the **`extreme value theorem (EVT) II`**. **`EVT II`** states that for a wide class of underlying distributions, the distribution of excesses above a sufficiently high threshold converges to the **`Generalized Pareto Distribution (GPD)`**. The **POT** approach remains relatively under-employed, largely due to the practical complexities involved in its implementation.\n\nUnlike **block maxima approach**, **POT method** is independent of time and instead relies on selecting a cumulative probability level, where data points falling below this threshold are excluded, while values above it are retained for analysis. As in the **block maxima method**, the data are assumed to be independent, so careful extraction of peaks is essential. \n\n\n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nFor shorter time series, **POT** approach would be preferable since it allows to use a larger portion of the data, leading to lower uncertainty in estimates. Unlike **block maxima method**, which retains only the largest annual flood (not always extreme!), the **POT method** ensures that all significant flood events are included, even if several occur in the same year.\n:::\n:::\n\n# Generalized Pareto (GP) distribution\n\n**GP** is a continuous two parameters probability distribution used to model exceedances over a high threshold, often referred to as **`partial duration series (PDS)`**. The cumulative distribution function (CDF) of the GP distribution is given below, where $x$ is the data, $\\alpha > 0$ is the scale parameter, $k$ is the shape parameter, and $q0$ the threshold.\n\n$$\nF(x) =\n\\begin{cases}\n1 - \\exp\\left( -\\dfrac{x - q0}{\\alpha} \\right) & \\text{if } k = 0 \\\\[10pt]\n1 - \\left( 1 + k \\dfrac{(x - q0)}{\\alpha} \\right)^{-1/k} & \\text{if } k \\ne 0\n\\end{cases}\n\\tag{1}\n$$\n\n\n# Data import \n\nWill work with the same data used in the [**block maxima section**](../blockmaxima/index.qmd#sec-bmdata), from the **`La Dr√¥me √† Luc-en-Diois`** station in France. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilename = './data/discharge.csv'\nQdata = read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))  \n\n# construct time serie\nQdata$Date = paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep=\"-\")\nQdata$Date  = ymd(Qdata$Date)\n\n# final time serie\nQdata = Qdata[, c(\"Date\", \"Q\")]\n```\n:::\n\n::: {#tbl-pot_loaddata .cell tbl-cap='First (6) lines of the daily streamflow data for Block-Maxima approach'}\n::: {.cell-output-display}\n\n\n|      |    Date    |    Q     |\n|:-----|:----------:|:--------:|\n|22794 | 2020-12-26 | 2.326204 |\n|22795 | 2020-12-27 | 2.173519 |\n|22796 | 2020-12-28 | 2.193727 |\n|22797 | 2020-12-29 | 2.090440 |\n|22798 | 2020-12-30 | 1.966944 |\n|22799 | 2020-12-31 | 1.854676 |\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily streamflow time serie.](index_files/figure-html/fig-streamflow_ts-1.png){#fig-streamflow_ts width=672}\n:::\n:::\n\n\n\n# Choice of the appropriate threshold {#sec-thresholdident}\n\nChoosing the appropriate threshold involves balancing **`bias`** and **`variance`**: setting it **lower** provides more data but risks introducing bias, whereas setting it **too high** reduces the data available, increasing uncertainty [@gilleland2016extremes]. Like choosing block size in the Block Maxima method, selecting a threshold in the POT approach aims to capture true extremes, while maintaining sufficient data for robust analysis. Even if several methods to select the threshold are available in the litterature, it is still a subjective task. The most common practice is to use graphical methods [@coles2001]. \n\n::: {.column-margin}\n::: {.callout-note}\nInterpreting these plots often requires a certain degree of **subjective judgment**, as clear-cut decisions about threshold suitability are rarely evident.\n:::\n:::\n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nThe threshold process can be automated (see @solari2017) for details.\n:::\n:::\n\nOne way to perform threshold selection, is a focused analysis within an specfic interval. @gilleland2016extremes proposes a procedure, implemented in the **`{extRemes}`** ecosystem, that repeatedly fits the GP distribution to the data across a sequence of threshold values, while also providing measures of variability. The idea, as they stated in their tutorial, is to identify the lowest threshold above which the fitted model remains stable, meaning that increasing the threshold further does not significantly change the results, apart from expected variation within the uncertainty bounds. \n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nThe **`{extRemes}`** library provides the **`mrlplot(...)`** function to produce the **MRL** plot. \n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = Qdata$Q\nthreshrange.plot(x, r = c(5, 30), nint=25, na.action=na.omit, set.panels=FALSE)\n```\n:::\n\n\n@fig-thrangeplot illustrates how the shape and modified scale parameters of the GP distribution evolve across a range of threshold values. The parameters appear to stabilize around a threshold of **10**, while higher thresholds (from 15) exhibit increased variability, likely due to the decreasing number of exceedances.\n\n\n::: {#fig-thrangeplot .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Variability of the (reparametrized) scale parameter](index_files/figure-html/fig-thrangeplot-1.png){#fig-thrangeplot-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Variability of the shape parameter](index_files/figure-html/fig-thrangeplot-2.png){#fig-thrangeplot-2 width=672}\n:::\n\nThreshold range plots showing the variability of GP distribution parameters across different threshold values.\n:::\n\n\nAnother most widely used graphical tools: the **`Mean Residual Life (MRL) plot`**. **MRL** is based on the theoretical mean of the GP [@ribatet2011pot], and helps assess the adequacy of a threshold for GP distribution modeling. The MRL plot displays the average excess over a range of thresholds, defined as the mean of the exceedances above a given threshold $q$. If the exceedances above a certain threshold follow a GP distribution, the **MRL** function should appear approximately linear beyond that point (see @gilleland2016extremes for details). Thus, identifying a linear region in the plot guide the selection of a suitable threshold. Despite its simplicity, the **MRL** plot provides a useful diagnostic to balance threshold stability and GP distribution fit (cf. @coles2001, Section 4.3.1). \n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nThe **`{extRemes}`** library provides the **`mrlplot(...)`** function to produce the **MRL** plot. \n:::\n:::\n\nFor diagnosing an appropriate choice, the chunk above produces the **MRL** plot with a **95% confidence intervals (CIs)**. **CIs** are based on the **normal df** for the **mean excesses**. The **MRL** plot is expected to exhibit **approximately linear** behavior above a suitable threshold [@coles2001]. As shown in @fig-mrlplot, the threshold $q = 10$ appears appropriate, since the linearity assumption is reasonably satisfied beyond this point.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = Qdata$Q\nmrlplot(x, na.action=na.omit)\nabline(v = 10, col = \"red\", lty = 2, lwd = 2)  # Vertical line at q = 10\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![The threshold selection: the mean residual life (mean excess) plot with dashed green lines indicating the 95% confidence intervals for the mean excesses. The vertical red dashed line indicates the threshold $q = 10$.](index_files/figure-html/fig-mrlplot-1.png){#fig-mrlplot width=672}\n:::\n:::\n\n\n# Threshold exceedances (POT) sampling\n\nWe need to extract all exceedances above the threshold for further analysis. @fig-allexcedences show daily streamflow time serie with all exceedances above the threshold. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract all flow values above the threshold and sort by date\nthres = 10\nall_exceedances = Qdata |>\n                  filter(Q > thres) |>\n                  arrange(Date)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Streamflow time serie with all exceedances (red dots) above the threshold (horizontal blue line)](index_files/figure-html/fig-allexcedences-1.png){#fig-allexcedences width=672}\n:::\n:::\n\n\n The **correlogram** (@fig-acfplot_pot) shows that the extracted peaks are autocorrelated for lag 1 and 2. This reflects dependence between consecutive peaks, and even one time step apart, values still influence each other. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Autocorrelation (at different lags) of peaks over the selected threshold. The red line represents the 95% confidence interval.](index_files/figure-html/fig-acfplot_pot-1.png){#fig-acfplot_pot width=672}\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\nThis is expected, as peaks are likely to be close to each ‚Äî i.e., a peak is often followed by another peak of similar magnitude within the next 1‚Äì2 time steps.\n:::\n:::\n\nWe can confirm the dependance of the threshold exceedances with the Wald-Wolfowitz independence test [@wald_test_1940] (see [**autocorrelation section**](../blockmaxima/index.qmd#sec-wwtest)) performed with the chunk below. The test summary indicates that the data are highly autocorrelated, whith a **very low p-value (< 2.2e-16)**, supporting the rejection, with high confidence, of the null hyposthesis that the data are independent.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWald-Wolfowitz test for independence and stationarity\n\ndata:  x\nz = 13.377, n = 800, p-value < 2.2e-16\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n```\n\n\n:::\n:::\n\n\n\n# Declustering\n\nThe naive selection of events above a specfic threshold may lead to dependent (or auto-correlated) events. It‚Äôs important that the exceedances over the threshold are independent because **extreme value theory** assumes each event is separate and not part of the same physical phenomenon (like one long flood event). However, floods and other hydrological extremes often occur in clusters due to persistent weather systems. To ensure independence, a process called **`de-clustering`** is often applied after sampling events above the selected threshold. \n\n::: {.column-margin}\n::: {.callout-note}\n## Note\n**`Declustering`** is essential for satisfying the **independence assumption** required by **extreme value analysis**.\n:::\n:::\n\n**`De-clustering`** is the process of identifying and retaining independent extreme events from a time serie by removing clustered values. A two-rules **`de-clustering`** method is commonly applied in the litterature to separate dependent events: (i) **`time separation rule`** and (ii) **`magnitude differential rule`** [@lang1999].\n\n![Inter-flood duration criteria (adapted from @lang1999). $Œ∏$ represents a minimum number of days between two consecutive peaks. $(Xs)_1$ and $(Xs)_2$ are respectively the first and second peaks, and $X_{min}$ is the minimum flow value between the two peaks.](images/declust_modern.png){#fig-decluster}\n\n1. **`Time separation rule:`** a minimum number of days between successive peaks. This criterion can be computed with the following equation [@wrc1975]: \n$$Ts = 5days + log(A)$$\nwhere $A$ is the catchment area. Let $Œ∏$ represents the minimum number of days between two consecutive peaks: \n\n    - If $Œ∏ < Ts$ ‚Üí **`REJECT`** the second peak.\n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nThis **time separation rule** assumes that larger watersheds require more time for floods to travel through and fully drain from the system.\n:::\n:::\n\n2. **`Magnitude differential rule:`** flow should drop below a certain fraction of the smaller peak between events. @cunnane1979 propose the following condition, where $X_{min}$ is the minimum flow value between the two peaks, and $(Xs)_1$ is the first peak:\n\n    - If $X_{min} > (2/3)(Xs)_1$ ‚Üí **`REJECT`** the second peak $(Xs)_2$. \n\n::: {.column-margin}\n::: {.callout-note}\n## Note\n**Cunnane‚Äôs condition** assesses whether the flood \"drops sufficiently\" after the first peak. If it not, it is considered a continuation of the same flood event.\n:::\n:::\n\nThese two rules described above not alternatives to each other, but **complementary**. That is, both conditions should be satisfied for an event to be considered a separate flood. We will first apply the **time separation rule** to ensure enough time passes for the basin to drain fully between events, and then the **magnitude differential rule** to ensures a significant drop in flow indicating a ‚Äúnew flood wave‚Äù.\n\nWe will **decluster** our time serie using the **threshold value of 10**, as identified in @sec-thresholdident. The Area of the studied catchment is $194 km¬≤$, as described in the [HydroPortail website (here)](https://www.hydro.eaufrance.fr/sitehydro/V4214010/fiche)\n\n\n::: {.cell}\n\n:::\n\n\n\nWe first define a function which takes as arguments the **streamflow data**, the **peak-over-threshold** value, and the **drainage basin area**, performs the **temporal declustering technique**:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n#' Select Time-Separated Peaks (De-clustering)\n#'\n#' Identifies independent flood peaks by applying a minimum time separation rule \n#' between exceedances over a given threshold. This is commonly used to de-cluster \n#' peaks in the context of Peaks Over Threshold (POT) modeling.\n#'\n#' @param data A data frame containing at least `Date` and `Flow` columns.\n#' @param threshold Numeric. The flow threshold above which a value is considered an exceedance.\n#' @param Area Numeric. Catchment Area.\n#'\n#' @return A data frame of de-clustered peaks (exceedances separated by at least `Ts` days).\n\ntime_sep_declust = function(data, threshold, Area) \n{  \n  # convert Area from km¬≤to Miles¬≤\n  A_miles = Area*0.386\n\n  # compute the number of days required (Ts)\n  Ts = ceiling(5 + log(A_miles))\n\n  # Filter flow values above the threshold and sort by date\n  exceedances = data |>\n                filter(Q > threshold) |>\n                arrange(Date)\n  \n  # initialize first exceedance as the first independent peak\n  kept_peaks = exceedances[1, , drop=FALSE]\n  last_peak_date = exceedances$Date[1]\n  \n  # Loop through remaining exceedances to apply the time-separation rule\n  for (i in 2:nrow(exceedances)) \n  {\n    current_date = exceedances$Date[i]\n    if (as.numeric(current_date - last_peak_date) >= Ts) {\n      kept_peaks = bind_rows(kept_peaks, exceedances[i, , drop=FALSE])\n      last_peak_date = current_date\n    }\n  }\n\n  # Return the de-clustered peaks\n  return(kept_peaks)\n\n}\n```\n:::\n\n\nWe can now apply our temporal declustering function to the POT serie. The Area of the studied catchment is $194 km¬≤$, as described in the [HydroPortail website (here)](https://www.hydro.eaufrance.fr/sitehydro/V4214010/fiche)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# time separation\npeaks_time_declust = time_sep_declust(data=Qdata, threshold=thres, Area=area)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Streamflow time serie with all exceedances (red dots) above the threshold (horizontal blue line), along with the declustered peaks (green crosses) using the time-separation rule.](index_files/figure-html/fig-temporaldeclust-1.png){#fig-temporaldeclust width=672}\n:::\n:::\n\n\n\nNow, we apply the **magnitude diffrential rule** to the temporally declustered peaks. To achieve this, we need to define aother function which takes as arguments the **temporally declustered data**, the the **original streamflow data**, and the **fraction** to compute magnitude diffrential, set to **2/3**.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n#' Filter peaks based on intervening flow magnitude\n#'\n#' This function removes clustered peaks in a time series based on the magnitude \n#' difference between consecutive peaks. Specifically, a peak is discarded if \n#' the minimum flow between it and the previous peak is greater than a specified \n#' fraction (`frac`) of the previous peak's magnitude.\n#'\n#' @param data A data frame containing temporallydeclustered peaks, with columns `Date` and `Flow`.\n#' @param original_data The full original time series with at least `Date` and `Flow` columns.\n#' @param frac A numeric value (default is 2/3). A peak is discarded if the minimum \n#'        flow between it and the previous peak is greater than `frac * previous_peak`.\n#'\n#' @return A filtered data frame of peaks after magnitude-based declustering.\n\nmagnitude_diff_declust = function(data, original_data, frac = 2/3) \n{\n  # Return as-is if there is only one or no peak\n  if (nrow(data) <= 1) return(data)\n  \n  # Logical vector to mark which peaks to keep\n  keep = rep(TRUE, nrow(data))\n  \n  # Iterate through peaks, comparing each with the previous one\n  for (i in 2:nrow(data)) {\n    peak1_date = data$Date[i - 1]\n    peak2_date = data$Date[i]\n    \n    # Extract the flow values between two peaks from the full time series\n    in_between = original_data |>\n      filter(Date > peak1_date, Date < peak2_date)\n    \n    # If no data in between, keep the current peak\n    if (nrow(in_between) == 0) next\n    \n    # Safely calculate minimum flow in the interval\n    flows_between = in_between$Flow\n    flows_between = flows_between[!is.na(flows_between)]\n    \n    # Skip if no valid flow values between peaks\n    if (length(flows_between) == 0) next\n    \n    Xmin = min(flows_between)\n    Xs1 = data$Flow[i - 1]\n    \n    # Skip if previous peak flow is NA or missing\n    if (is.na(Xs1)) next\n    \n    # Discard current peak if in-between flow is too high\n    if (Xmin > frac * Xs1) {\n      keep[i] = FALSE\n    }\n  }\n  \n  # Return only the peaks that meet the magnitude difference criterion\n  return(data[keep, ])\n}\n```\n:::\n\n\n\n@fig-finaldeclust show that the additional declustering step after temporal separation rule is not necessary, as the any peak is discarded by this second rule.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# magnitude differential\npeaks_final = magnitude_diff_declust(data=peaks_time_declust, original_data=Qdata)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Streamflow time serie with with the declustered peaks (purple dots) using the magnitude difference rule.](index_files/figure-html/fig-finaldeclust-1.png){#fig-finaldeclust width=672}\n:::\n:::\n\n\n\n@fig-acfdeclust shows the **correlogram** computed on the declustered peaks. There is no statistically significant autocorrelation beyond lag 0. Thus, the declustered peaks can be considered approximately independent.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Autocorrelation (at different lags) of **declustered** peaks value. The red line represents the 95% confidence interval.](index_files/figure-html/fig-acfdeclust-1.png){#fig-acfdeclust width=672}\n:::\n:::\n\n\nTo further assess the independence of the declustered peaks, we apply the Wald-Wolfowitz independence test. From the results displayed below, we conclude that **the declustering worked well**, as we now fall to reject the null hypothesis of independance, with **p.value equal to 0.2301**.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWald-Wolfowitz test for independence and stationarity\n\ndata:  x\nz = 1.2001, n = 219, p-value = 0.2301\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n```\n\n\n:::\n:::\n\n\n# Trend detection\n\n## Non-parametric approach\n\nAs for the **block maxima** approach (see [trend detection section](../blockmaxima/index.qmd#sec-trend)), we can also apply the **non-parametric Mann-Kendall** trend test for monotonic trend detection. See the [Mann-Kendall section](../blockmaxima/index.qmd#sec-mk) for the complete algorithm of the test. In the chunk below, we use the **`mk.test(...)`** function from the **`{trend}`** library to perform the trend test.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.5059:`** there is not enough statistical evidence to conclude that a significant trend exists in the threshold exceedances serie.\n\n**`tau = -3.041649e-02:`** indicates a weak or no trend.\n\n**`z = -0.66528:`** suggests a potential decreasing trend, but the trend is not significant.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform Mann-Kendall trend test\nmktest = trend::mk.test(x = peaks_final$Q)\nprint(mktest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMann-Kendall trend test\n\ndata:  peaks_final$Q\nz = -0.66528, n = 219, p-value = 0.5059\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n-7.220000e+02  1.174512e+06 -3.041649e-02 \n```\n\n\n:::\n:::\n\n\n\nThe snippet below computes the absolute and relative **Sen's slope**, to quantify the magnitude of trend in the POT data as both a raw rate of change and a percentage relative to the mean. See the [Sen's slope section](../blockmaxima/index.qmd#sec-sens) for the complete algorithm and the user-defined function **`sens_slope_percent(...)`** we apply below.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute Sen's slope as a percentage change\nsens_slope_percent(peaks_final$Q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute Sen's slope: -0.0017\nP-value: 0.5059\nRelative trend rate: -0.01%\n```\n\n\n:::\n:::\n\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## Relative Sen's slope\nThe threshold exceedances serie show a slight decreasing trend with a **slope of -0.0017 rate**, which is equivalent to **‚àí0.01% relative to its long-term average**. Yet, this negative trend is not significative given the **p-value of 0.5059**.\n:::\n:::\n\n## Parametric approach with POT modelling\n\n### Fit a stationary GP distribution\n\nAs we have already seen in the previous section, the GP distribution is the most appropriate distribution to model threshold exceedances. As explained by @gilleland2016extremes in their paper, an important additional consideration when using the POT approach is the **rate of exceedance** ‚Äî that is, how often values exceed the chosen threshold. While this rate does not affect the fitting of the GP model itself, it is crucial for calculating return levels, which require scaling the model to real-world time units.\n\n\n::: {.cell}\n\n:::\n\n\n\nThe **exceedance rate** should be passed the **`time.units`** argument in the **`fevd(...)`** function. Since our goal is to estimate annual return levels, we must specify the average number of threshold exceedances per year (i.e., the average number of flood events per year in our dataset). \n\n@fig-finaldeclust shows that the number of events per year varies ‚Äî some years experience multiple flood events, while others have none. Therefore, a reasonable approach is to use the empirical average number of exceedances per year over the entire study period as the value for **`time.units`**. The snippet below shows how to calculate this rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the average number of threshold exceedances per year\npeaks_final$year <- year(peaks_final$Date)\nn_years <- length(unique(peaks_final$year))\nn_events <- nrow(peaks_final)\nunits <- n_events / n_years\n\n# Optional: print it nicely\ncat(\"Average number of events per year:\", round(units, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage number of events per year: 3.71 \n```\n\n\n:::\n:::\n\n\nNow, we can fit our **GP model** to the POT data. The workflow is same as the block maxima models (GEV and Gumbel), and same tools can be used to estimates the GP parameters. In the chunk below, we use the **` {extRemes}`** library again to fit the GP distribution to our partial duration serie in a statinary context. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the stationary GP distribution to the PDS\nlibrary(extRemes)\nthres = 10\nx = peaks_final$Q\nsgp_fit = fevd(x, type=\"GP\", method=\"GMLE\", threshold=thres, time.units = paste0(units,\"/year\"))\n\n# show summary of the fit\npretty_fit_summary(sgp_fit, \"GP\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== GP  Fit Summary ===\n\nEstimated Parameters:\n scale  shape \n3.6496 0.3099 \n\nModel Fit Criteria:\nLogLik: -570.53\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\n## Note\nThe only thing that change here, comared to the **block maxima models**, is the additional **threshold value** and the **time.units** arguments passed to the **`fevd(...)`** function, which is the same as the threshold used in the declustering process. The **`pretty_fit_summary(...)`** is defined in the [block maxima section](../blockmaxima/index.qmd#sec-utils).\n:::\n:::\n\n@fig-gpfitdiag-stationary displays the diagnostics from the GP distribution fitted to the partial duration serie: density plot of empirical data and fitted GP df (top-left), and  quantiles from a sample drawn from the fitted GP against the empirical data quantiles with 95% confidence bands (top-right).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show diagnostic plots of the GP distribution fitting\npar(mfrow=c(1,2))\nplot(sgp_fit, type=\"density\", main=\"(a) Density plot\")\nplot(sgp_fit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for the General Pareto (GP) distribution fitted to the threshold exceedances serie in a stationary context.](index_files/figure-html/fig-gpfitdiag-stationary-1.png){#fig-gpfitdiag-stationary width=672}\n:::\n:::\n\n\n\n### Fit a non-staionary GP distribution\n\nAs for the **block maxima models**, one way to account for non-stationary is to incorporate covariates within the parameters of the distributions in a **`regression-like manner`**. To perform **parametric trend detection** with the **GP distribution**, we model the **`scale parameter (œÉ)`** as a function of time, transforming the model into a **`non-stationary GP`**. The scale parameter of the non-stationnary GP distribution is computed as follow:\n$$\n\\sigma (t) = \\sigma 0 + \\sigma 1¬∑t \\tag{2}\n$$\nwhere $\\sigma$ is the time-dependant scale parameter of the GP distribution, $t$ is the time index, $\\sigma 0$ is the intercept, and $\\sigma 1$ is the slope. \n\nThe chunk below use the **` {extRemes} `** library to fit the **`Non-Stationary GP (NS-GP) distribution`**  to our **Partial Duration Serie** extracted earlier. Unfortunately, most of the **`extRemes`** diagnostic plots currently are not available for non-stationary POT models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set threshold\nthres = 10\n\n# use time index for non-stationary context fitting\npeaks_final$scale_cov = 1:nrow(peaks_final)\n\n# fit the GP distribution to the PDS\nPDS = data.frame(peaks_final)\nnsgp_fit = fevd(x=Q, scale.fun=~scale_cov, data=PDS, type=\"GP\", \n                method=\"GMLE\", threshold=thres, time.units=paste0(units,\"/year\"))\n\n# show summary of the fit\npretty_fit_summary(nsgp_fit, \"Non-Stationary GP\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== Non-Stationary GP  Fit Summary ===\n\nEstimated Parameters:\n sigma0  sigma1   shape \n 4.1064 -0.0040  0.3066 \n\nModel Fit Criteria:\nLogLik: -570.21\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## NS-GP parameters\n\n**`sigma0 = œÉ0 = 4.1064:`** is the **intercept** of the time-varying GP scale parameter. It represents the base scale of the distribution (often the start of the time series). \n\n**`sigma1 = œÉ1 = -0.0040:`** is the **slope** of the scale parameter with respect to time. This negative value suggests that extreme values are becoming less variable (narrower tail) over time.\n\n**`shape = 0.3066:`** controls the **heaviness** of the tail of the GP distribution. The positive value (Œæ > 0) suggests a heavy (Fr√©chet) tail (i.e., the probability of very large exceedances decays slowly).\n:::\n:::\n\n### Deviance test\n\nThe **non-parametric Mann-Kendall** test indicates a non-significant trend in the POT values. With **parametric methods**, the significance of a temporal trend is based on a **deviance test** between a non-stationary (or more complex) model and a stationary one. The **deviance test**, or **Likelihood Ratio (LR)** test, compares two nested models ‚Äî a simpler (stationary) model and a more complex (non-stationary) model. \n\nThe null hypothesis of the **deviance test** is that the **simpler model is sufficient ‚Äî i.e., no trend** (e.g., the GP scale parameter is constant over time). See the [Deviance test Section](../blockmaxima/index.qmd##sec-deviancetest) for the mathematical details. \n\n::: {.column-margin}\n::: {.callout-note}\nWe try just to answer this question: Does the more **complex non-stationary GP model** provide a significantly better fit to the data than the **simpler stationary GP model**?\n:::\n:::\n\nTo perform the **`Deviance test`**, we can use the **`lr.test(...)`** of the **`{extRemes}`**. This function just requires the previous fitted objects of class **`‚Äúfevd‚Äù`** from the **`fevd(...)`** function: \n\n* **`sgp_fit`**: model with fewer parameters (stationary GP model)\n* **`nsgp_fit`**model with more parameters (non-stationary GP model)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# applying the Deviance test\nlr.test(x=sgp_fit, y=nsgp_fit, alpha=0.05, df=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLikelihood-ratio Test\n\ndata:  xQ\nLikelihood-ratio = 0.64953, chi-square critical value = 3.8415, alpha =\n0.0500, Degrees of Freedom = 1.0000, p-value = 0.4203\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## Interpretation\nüö´ We fail to reject the null hypothesis.\n\n* **`D-statistic (0.65)`** much smaller than the **`critical value (3.84)`**.\n* **`p-value (0.4203)`** much larger than the **`significance level (0.05)`**.\n:::\n:::\n\n# Trend analysis conclusions\n\nüîé **Trend Detection Methods Used**\n\n* Parametric: **`Non-stationary GP model`** with **time-varying scale parameter**\n* Non-parametric: **`Mann-Kendall test`**\n\nüö´ **No Significant Trend Detected**\n\n* Both methods consistently indicate **no statistically significant trend** in the POT serie.\n* The **Likelihood Ratio Test** showed a high p-value (0.4203), and both the **Mann-Kendall test** and the **Sen's slope** similarly returned a non-significant result, with a p-value of 0.5059 and 0.5059, respectively.\n\n‚úÖ **Modeling Implication**\n\n* The results support the use of a stationary GP model.\n* There is **no strong evidence** of temporal change in flood extremes over the study period.\n\n\n# Flood quantiles estimation\n\n## Stationary context\n\nTo estimate **return levels** for given **return periods**, we can use the **`return.level(...)`** from the **`{extRemes}`** package: \n\n::: {.column-margin}\n::: {.callout-note}\nThe **`return level`** is the value expected to be exceeded on average **once every T years**.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(sgp_fit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfevd(x = x, threshold = thres, type = \"GP\", method = \"GMLE\", \n    time.units = paste0(units, \"/year\"))\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI Estimate 95% upper CI\n2-year return level       18.33132 20.14264     21.95396\n5-year return level       23.30110 27.34035     31.37960\n10-year return level      27.12648 34.31731     41.50814\n20-year return level      30.77529 42.96605     55.15681\n50-year return level      34.81016 57.65826     80.50637\n100-year return level     36.68461 71.89985    107.11510\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`sgp_fit object`** is the object returned by the **`fevd(...)`** function when we have fitted a the **stationary GP model**.\n:::\n:::\n\n## Non-Stationary context\n\nIn non-stationary context, since the **scale parameter is time-dependent**, **return levels** have to be calculated for each time step. We refer to this as **`‚Äúeffective‚Äù return levels`**The same **`return.level(...)`** from the **`{extRemes}`** package is used to compute **`\"effective\" return levels`**.\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`nsgp_fit object`** is the object returned by the **`fevd(...)`** function when we have fitted a the **non-stationary GP model**.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgp_fit, return.period=rperiods)\n```\n:::\n\n\nFor easy visualization, we will plot the **`effective return levels`**. Lets convert the output of **`return.level(...)`** into a clean data.frame :\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert estimates to data.frame\ndf_rlevels <- data.frame(time=1:nrow(peaks_final),\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  time       Q2       Q5      Q10      Q20      Q50     Q100\n1    1 21.36000 29.38540 37.14447 46.74100 63.00002 78.71949\n2    2 21.34887 29.36640 37.11788 46.70500 62.94809 78.65216\n3    3 21.33774 29.34741 37.09128 46.66901 62.89617 78.58484\n4    4 21.32661 29.32842 37.06469 46.63301 62.84424 78.51751\n5    5 21.31549 29.30943 37.03810 46.59702 62.79232 78.45019\n6    6 21.30436 29.29044 37.01150 46.56102 62.74039 78.38286\n```\n\n\n:::\n:::\n\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with **`ggplot`**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataplot = df_rlevels |> \n           pivot_longer(-1, names_to=\"RL\", values_to=\"Q\") |>\n           # re-index\n           mutate(RL = factor(RL, levels=rev(paste0(\"Q\", c(2, 5,10,20,50,100)))))\n```\n:::\n\n\nFinally, we plot the **`effective return levels`**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrlplot = dataplot |>\n         ggplot(aes(x=time, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n```\n:::\n\n\n@fig-effective-return-levels helps in understanding the variations and trends in flood quantiles across different time periods.\"\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles.](index_files/figure-html/fig-effective-return-levels-1.png){#fig-effective-return-levels width=672}\n:::\n:::\n\n\n\n\n# References\n\n\n::: {#refs}\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}