{"title":"Flood Frequency Analysis (FFA) with Block-Maxima","markdown":{"yaml":{"title":"Flood Frequency Analysis (FFA) with Block-Maxima"},"headingText":"Libraries","containsRefs":true,"markdown":"\n\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n```\n\n# Data import {#sec-bmdata}\n\nWill work with real-world data, from the **`La Dr√¥me √† Luc-en-Diois`** station in France, to analyze flood events using daily streamflow data covering the period from August 1st, 1958, to December 31st, 2020. The metadata of the station are available on the [HydroPortail website (here)](https://www.hydro.eaufrance.fr/sitehydro/V4214010/fiche).\n\nLet‚Äôs consider the formatted daily streamflow data file for the target catchment. We can read the data into a data frame using the `read.csv()` function. We will also set the column names using the `col.names` argument, and use the `lubridate` library to prepare a time serie.\n\n```{r}\n#| echo: true\n#| eval: true\n\nlibrary(lubridate)\n\nfilename = './data/discharge.csv'\nQdata = read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))  \n\n# construct time serie\nQdata$Date = paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep=\"-\")\nQdata$Date  = ymd(Qdata$Date)\n\n# final time serie\nQdata = Qdata[, c(\"Date\", \"Q\")]\n```\n\nThe output is a data frame (or a matrix) with 22799 rows and 2 columns.\n\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_loaddata\n#| tbl-cap: \"First (6) lines of the daily streamflow data for Block-Maxima approach\"\n\ntail(Qdata, 6) %>% knitr::kable(align=rep(\"c\", 2))\n```\n\n::: {.column-margin}\n\n::: {.callout-note}\nWe can display the structure of the data frame using the `str()` function.\n\n```{r}\n#| echo: true\n#| eval: true\n\nstr(Qdata)  \n```\n\n:::\n\n:::\n\n# Extreme events sampling\n\n\n## The block maxima approach\n\nThe **Block-Maxima Approach [@gumbel1958statistics]**, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and reduce the possiblity of serial dependence.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n#| fig-height: 2\n#| label: fig-bm_illust\n#| fig-cap: \"Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima\"\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(scales)  # for better date formatting\n\n# Simulate daily streamflow data\nset.seed(42)\ndates = seq.Date(from = as.Date(\"1958-08-01\"), to = as.Date(\"1962-07-31\"), by = \"day\")\nQ = rgamma(length(dates), shape = 3, scale = 10)\nstreamflow = tibble(date = dates, Q = Q)\n\n# Define hydrological year starting August 1\nstreamflow = streamflow %>%\n  mutate(hydro_year = if_else(month(date) >= 8, year(date) + 1, year(date)),\n         hydro_year_start = as.Date(paste0(hydro_year - 1, \"-08-01\")))\n\n# Extract block maxima per hydrological year\nmaxima = streamflow %>%\n  group_by(hydro_year) %>%\n  filter(Q == max(Q)) %>%\n  ungroup()\n\n# Get unique hydrological year start dates for vertical lines\nhydro_year_starts = unique(streamflow$hydro_year_start)\n\n# Plot with enhancements\nggplot(streamflow, aes(x = date, y = Q)) +\n  geom_line(color = \"steelblue\", size = 0.3, alpha = 0.8) +\n  geom_point(data = maxima, aes(x = date, y = Q), color = \"red\", size = .7, shape = 8) +\n  geom_vline(xintercept = hydro_year_starts, linetype = \"dashed\", color = \"gray40\", alpha = 0.6) +\n  scale_x_date(date_breaks = \"1 years\", date_labels = \"%Y\") +\n  labs(\n    x = \"Date\",\n    y = \"Streamflow (Q)\"\n  ) +\n  theme_minimal(base_size = 5) +\n  theme(\n    axis.title = element_text(size=5, face=\"bold\"),\n    axis.text = element_text(size=5, color=\"black\"),\n  )\n\n\n```\n\n## Exract annual maximum floods (AMF) from the daily streamflow serie\n\n**1.** To extract annual maximum floods (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\nextract_annual_max = function(df) \n{\n    df %>%\n    mutate(Year = year(Date)) %>%    # extract year from Date\n    group_by(Year) %>%\n    filter(Q == max(Q)) %>%          # keep only max flow per year\n    slice(1) %>%                     # in case of ties, keep first max\n    ungroup() %>%\n    select(Year, Date, AMAX=Q)\n}\n```\n\n**2.** Now, we apply the function to the data to extract AMF for each year:\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\nextractedAMF = extract_annual_max(Qdata)\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_extract\n#| tbl-cap: \"First (6) rows of the AMF extracted using the Block-Maxima approach\"\n\nhead(extractedAMF, 6) %>% knitr::kable(align=rep(\"c\", 2))\n```\n\n## Display sampled vvents\n\nNow, we overlap the extracted **Annual Maximum Flow** with the original daily discharge time serie to visualize the sampling process. We use the functions from the `dplyr` and `ggplot2` library to create the plot.\n\n```{r}\n#| echo: true\n#| message: false\n#| warning: false\n#| output: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n```\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n#| fig-height: 3\n#| label: fig-amf_with_dailystreamflow\n#| fig-cap: \"Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach\"\n\nprint(amf.plot)\n```\n\n\n\n# Autocorrelation testing\n\n::: {.column-margin}\n::: {.callout-tip}\n## Recall\nThe `extractedAMF` variable used in the following code blocs is created previously in the **Sampling Events** section\nThe `acf` function is a R-base function, no installation needed\n:::\n:::\n\n\n## What is autocorrelation? {#sec-autocorrelation}\n\n**`Autocorrelation (or autocovariance)`** of a series refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ...). An autocorrelated series is thus correlated with itself, with a given lag (please refer [here](https://perso.ens-lyon.fr/lise.vaudor/autocorrelation-de-series-temporelles-ou-spatiales/) for mode details). \n\n::: {.callout-note}\n\n## To assess autocorrelation, two methods will be used in this lab\n\nüîç **Correlogram:**\n\n* Visualization of correlation coefficients between the serie and its lags\n* Based on the [autocorrelation function (ACF)](https://medium.com/@kis.andras.nandor/understanding-autocorrelation-and-partial-autocorrelation-functions-acf-and-pacf-2998e7e1bcb5)\n* Helps identify the presence and extent of dependencies over time\n\nüìä **Wald-Wolfowitz Test [@wald_test_1940]:**\n\n* Non-parametric to assess the randomness of a time series\n* Used to detect the absence of randomness in the series\n* **Null hypothesis: The sequence of observations is random**\n\n:::\n\n### Correlogram\n\nIf you execute the above given chunk, it generates the @fig-acfplot as output:\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| message: true\n#| warning: false\n#| fig-height: 2.5\n#| label: fig-acfplot\n#| fig-cap: \"Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\"\n\n# compute autocorrelation lag\nbacf = acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf = with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn = length(extractedAMF$AMAX)\nconf_limit = 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n:::\n:::\n\n\n### Wald-Wolfowitz (WW) test {#sec-wwtest}\n\nIn the chunk below, we use the `ww.test(...)` function from `{trend}` package to perform the `WW test` at the **0.05 significance level** to test the hypothesis that the maximum annual flood is a random variable.\n\n```{r}\n#| echo: true\n\nwwtest = trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.4609:`** indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness).\n:::\n:::\n\n# Trend detection {#sec-trend}\n\n## Non-parametric methods\n\n### The Mann-Kendall (MK) test {#sec-mk}\n\nThe MK test's null hypothesis (H0) assumes no trend in the data [@mann1945nonparametric; @kendall_rank_1975]. Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results. The Mann-Kendall test statistic $S$ is defined as:\n\n$$\nS = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\text{sgn}(x_j - x_i) \\tag{1}\n$$\n\nThe sign function is given by:\n\n$$\n\\text{sgn}(x_j - x_i) =\n\\begin{cases}\n+1 & \\text{if } (x_j - x_i) > 0 \\\\\n0 & \\text{if } (x_j - x_i) = 0 \\\\\n-1 & \\text{if } (x_j - x_i) < 0\n\\end{cases} \\tag{2}\n$$\n\nwhere $x_i$ and $x_j$ are values of the variable at time $i$ and $j$, respectively, and $n$ is the total number of observations.\n\nA significance level (commonly $\\alpha = 0.05$) and the trend slope indicating the direction and magnitude of the trend are key characteristics of the Mann-Kendall test. If $n > 8$, the test statistic $S$ approximates a normal distribution. In this case, the mean of $S$ is zero and its variance is:\n\n$$\n\\text{var}(S) = \\frac{n(n - 1)(2n + 5)}{18} \\tag{3}\n$$\n\nThe standardized test statistic $Z$, which indicates the direction of the trend, is calculated as:\n\n$$\nZ =\n\\begin{cases}\n\\frac{S - 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S > 0 \\\\\n0 & \\text{if } S = 0 \\\\\n\\frac{S + 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S < 0\n\\end{cases} \\tag{4}\n$$\n\nThe null hypothesis $H_0$ (no trend) is rejected if $|Z| > 1.96$ at the 95% confidence level.\n\n- A **positive trend** is significant if $Z > 1.96$\n- A **negative trend** is significant if $Z < -1.96$\n\n\nWe use the `mk.test(...)` function from `{trend}` package to perform the `MK test` at the **0.05 significance level** to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n::: {.column-margin}\n::: {.callout-note}\n**`Non-Stationarity (NS)`** refers to changes in statistical properties over time.\n:::\n:::\n\n```{r}\n#| echo: true\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.2183:`** there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\n\n**`tau = -0.1096665:`** Kendall's Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\n\n**`z = -1.231:`** the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n:::\n:::\n\n@fig-amfevolution shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n```{r}\n#| echo: true\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 3\n#| label: fig-amfevolution\n#| fig-cap: \"Evolution of the annual peak flood over time.\"\n\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n```\n\n### Sen's slope estimator {#sec-sens}\n\nThe **non-parametric method of Sen [-@Sen1968]** is commonly combined with the Mann-Kendall test to estimate the slope of the $(\\beta)$ of a trend. Sen‚Äôs method looks at every pair of data points, calculates the slope between them, then uses the the median of all those slopes as the estimated trend.\n\n$$\n\\beta = \\text{median} \\left( \\frac{x_j - x_i}{j - i} \\right), \\quad , ‚àÄ\\quad j > i \\tag{5}\n$$\n\nTo express the Sen's slope as a **percentage change per time step (i.e., relative trend rate per year)**, it can be normalized by the mean:\n\n$$\n\\beta (\\%) = \\left( \\frac{\\beta}{\\bar{x}} \\right) \\times 100 \\tag{6}\n$$\nwhere $\\beta (\\%)$ is the relative trend rate per year, $x_i$ and $x_j$ are respectively the observed values at time points $i$ and $j$, and $\\bar{x}$ represents the mean value of the variable across the period.\n\nThe Sen's slope can be easily computed using the `sens.slope(...)` function from the `{trend}` package, as in the chunk below:\n\n::: {.column-margin}\n::: {.callout-tip}\n## Sen slope\n**`Slope = -0.1187375 :`** the median trend is slightly negative, suggesting a weak decline.\n\n**`p-value = 0.2183:`** this negative trend is not statistically significant.\n:::\n:::\n\n```{r}\n#| echo: true\n\n#compute Sen's slope and intercept\nsenslope = trend::sens.slope(x = extractedAMF$AMAX)\nprint(senslope)\n```\n\n\nYet, the `sens.slope(...)` function returns the **absolute** slope (i.e., the median change per time unit). It is more meaningful to express the trend as a **relative percentage** change over the time period. \n\nTo do this, we define a **wrapper function** that calculates the Sen's slope and then converts it into a relative trend rate per time step.\n\n```{r}\n#| echo: true\n#| cache: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nsens_slope_percent <- function(x, time_step=NULL) \n{\n  #' Compute Sen's slope as a percentage change over the full time period\n  #' @param x A numeric vector of observations\n  #' @param time_step Optional (e.g., year) used for display purpose only\n  #' @return Relative percent change over the period\n\n  require(trend)\n  \n  result <- sens.slope(x)\n  slope <- result$estimates              # Absolute slope (per time unit)\n  p_val <- result$p.value                # p-value for trend test\n  mean_val <- mean(x, na.rm = TRUE)\n  \n  percent_change <- (slope / mean_val) * 100\n  \n  cat(\"Absolute Sen's slope: \", round(slope, 4), \"\\n\", sep=\"\")\n  cat(\"P-value: \", format.pval(p_val, digits = 4), \"\\n\", sep=\"\")\n  if(!is.null(time_step)){\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%/\", time_step, \"\\n\", sep=\"\")\n  }else{\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%\\n\", sep=\"\")\n  }\n  \n}\n```\n\nNow, we can compute the **relative Sen's slope** for the annual maximum flow data:\n\n```{r}\n#| echo: true\n\n# compute Sen's slope as a percentage change\nsens_slope_percent(extractedAMF$AMAX, time_step = \"year\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Relative Sen's slope\nThe annual maximum flow shows a slight decreasing trend of ‚àí0.4% per year relative to its long-term average.\n:::\n:::\n\n\n## Parametric methods\n\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a `non-stationary (NS)` context. \n\n::: {.callout-note}\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter **`(Œº)`** will be expressed as a linear function of time, denoted as **`Œº(t)`**, leaving the others parameters constant.\n:::\n\n# Flood frequency analysis (FFA)\n\n## Probability distributions\n\nWe will compare two probability distributions adapted to the `Block-Maxima Framework`: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n::: {.callout-note}\n## GEV distribution\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n\n$$ \nF(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} \n\\tag{7}\n$$\n\nfor $1 + \\xi (x - \\mu)/\\sigma > 0$, where, $\\mu$ is the location parameter, $\\sigma > 0$ is the scale parameter, and $\\xi$ is the shape parameter.\n\nThe shape parameter $\\xi$ determines the type of extreme value distribution:\n\n* If $\\xi > 0$, the distribution is a Fr√©chet type.\n* If $\\xi < 0$, the distribution is a Weibull type.\n* If $\\xi = 0$, the distribution is a Gumbel type.\n\n:::\n\n::: {.callout-note}\n\n## Gumbel distribution\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter $\\xi = 0$. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:\n\n:::\n\n## Models fitting\n\nWe will employ the **`Generalized Maximum Likelihood Estimation (GMLE)`** method to fit probability distributions to our data. The **`{extRemes}`** package offers the function **`fevd(..)`**, which enables fitting a range of probability distributions ‚Äî such as the **GEV**, **Gumbe**l distributions ‚Äî to a given dataset using three distinct fitting methods: **`L-moments`**, **`Maximum Likelihood Estimation (MLE)`**, and **`GMLE`**.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Why the **`GML`** method?\n\nThe **`GMLE`** method use a prior distribution to constrain the **`GEVC shape parameter`** within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See [**`Martins & Stedinger (2000)`**](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1999WR900330) for more details).\n:::\n:::\n\nThe advantage of using **`GMLE`** and **`MLE`** over the **`L-moments`** method is that **`L-moments`** are limited in their ability to accommodate non-stationary processes. In contrast, **`GMLE`** and **`MLE`** can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection.\n\n## Model selection\n\nTo select the best probability distribution, we will use the **`AIC`** and **`AIC`** criteria. The model with the lowest **`AIC`** or **`BIC`** value will be selected. The model with the **lowest** **`AIC`** and **`BIC`** values is the best-suited model. **`AIC`** tends to favor models with better fit, even if more complex, while **`BIC`** favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Joint use of AIC and BIC\n* if both criteria select the same model, it is strongly preferred\n* if not, the choice depends on whether your goal is **predictive accuracy (AIC)** or **model simplicity (BIC)**\n:::\n:::\n\n::: {.callout-note}\n## AIC (Akaike Information Criterion)\n\nThe **`AIC`** balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n\n$$ \n\\text{AIC} = 2k - 2\\ln(\\hat{L}) \\tag{8}\n$$\nwhere $k$ is the number of parameters in the model, $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n::: {.callout-note}\n## BIC (Bayesian Information Criterion)\n\n**`BIC`** penalizes model complexity **more heavily than `AIC`**, especially for larger sample sizes. This means **`BIC`** tends to favor simpler models compared to AIC. The formula for BIC is:\n$$ \n\\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \\tag{9}\n$$\nwhere: $k$ is the number of parameters in the model, $n$ is the number of data points, and $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n\n## Perform FFA\n\n### Setup utils {#sec-utils}\n\nTo pretty-print the summary of our models' fitting, we will create a custom function that will take the fitted model as an argument, extract and format key components of the fiited object, and print the output: \n\n```{r}\n#| echo: true\n\npretty_fit_summary = function(fitobj, model) {\n  #' Pretty print summary of a  extreme value model fit\n  #'\n  #' @param fitobj Fitted model object from extRemes::fevd()\n  #' @param model Character string describing the model (e.g., \"GEV\")\n  \n  cat(\"===\", model, \" Fit Summary ===\\n\\n\")\n  \n  # Estimated Parameters\n  cat(\"Estimated Parameters:\\n\")\n  params = fitobj$results$par\n  print(round(params, 4))\n  \n  # Log-likelihood, AIC, and BIC\n  look = summary(fitobj, silent=TRUE)\n  cat(\"\\nModel Fit Criteria:\\n\")\n  cat(sprintf(\"LogLik: %.2f\\n\", look$nllh))\n  cat(sprintf(\"AIC: %.2f\\n\", look$AIC))\n  cat(sprintf(\"BIC: %.2f\\n\", look$BIC))\n\n  # return the AIC and BIC values for further comparison\n  return(data.frame(Model=model, Crit=c(\"AIC\", \"BIC\"), Value=c(look$AIC, look$BIC)))\n}\n\n```\n\n### Distributions fitting\n\n#### Stationary context\n\n::: {.panel-tabset}\n\n## [GEV fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the GEV distribution to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| echo: true\n#| cache: false\n\n# fit the GEV distribution to the AMF\nlibrary(extRemes)\ngevfit = fevd(x=extractedAMF$AMAX, type=\"GEV\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted GEV distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\ngevfit_res = pretty_fit_summary(gevfit, \"GEV\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n**`location = 19.4868:`** is the **`central tendency (Œº)`** of the GEV distribution. It defines the central position of the distribution and serves as a baseline around which extreme values are distributed.\n\n**`scale = 9.932:`** reflects the spread or variability of the extreme values. In our case, the AMF distribution has moderate spread around the shifting central value **`Œº(t)`**.\n\n**`shape = 0.3576:`** this positive shape parameter implies a Fr√©chet-type (heavy-tailed) distribution. This means there is greater probability of very large flood events compared to a Gumbel (shape = 0) or Weibull (shape < 0) distribution.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-stationary, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), and  quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gevfitdiag-stationary\n#| fig-cap: \"Diagnostic plots for the Generalized Extreme Value (GEV) distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .043))\nplot(gevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n## [Gumbel fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the Gumbel distribution to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| warning: false\n#| echo: true\n#| cache: false\n\n# fit the Gumbel distribution to the AMF\ngumfit = fevd(x=extractedAMF$AMAX, type=\"Gumbel\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined previously. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted Gumbel distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\ngumfit_res = pretty_fit_summary(gumfit, \"Gumbel\")\n```\n\nFinally, we show in @fig-gumfitdiag-stationary, the diagnostics from the Gumbel distribution fitted to the Annual Maximum Floods:\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gumfitdiag-stationary\n#| fig-cap: \"Diagnostic plots for the Gumbel distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gumfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .035))\nplot(gumfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n## [Best-suited model]{.custom-tabset-title2}\n\nAs mentionned previously, the model with the lowest **`AIC`** and **`BIC`** values is the best-suited model. The **`pretty_fit_summary()`** function we have defined previously return a data.frame of **AIC** and **BIC** values for each distribution. Lets compare these values to select the best-suited distribution for the next steps.\n\n```{r}\n#| echo: true\n#| eval: false\n\nprint(gevfit_res)\nprint(gumfit_res)\n```\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_aicbictable\n#| tbl-cap: \"Table of AIC and BIC values for each distribution\"\n#| tbl-subcap: \n#|   - GEV\n#|   - Gumbel\n#| layout-ncol: 2\n\nknitr::kable(slice(gevfit_res, -3), align=rep(\"c\", 2))\nknitr::kable(slice(gumfit_res, -3), align=rep(\"c\", 2))\n```\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## ‚úÖ Best-suited: GEV\nBoth **AIC and BIC are lower for the GEV model**, which suggests that it provides a better balance of fit and complexity than the Gumbel model.\n:::\n:::\n\n:::\n\n\n#### Parametric trend-detection\n\nWhen covariates are to be incorporated into EVA, a common approach to account for non-stationary is to incorporate covariates within the parameters of the distributions in a **`regression-like manner`**. To perform **parametric trend detection**, we model the **`location parameter (Œº)`** as a function of time, transforming the models into **`non-stationary distributions`**. The location parameter of the non-stationnary GEV and Gumbel is computed as follow:\n$$\n\\mu (t) = \\mu 0 + \\mu 1¬∑t \\tag{10}\n$$\nwhere $\\mu$ is the time-dependant location parameter, $t$ is the time index, $\\mu 0$ is the intercept, and $\\mu 1$ is the slope.\n\n::: {.callout-tip}\n# Trend?\nThe decision to adopt a non-stationary (or more complex) model in FFA depends on whether the temporal trend is **`statistically significant`**.\n\nThe **`deviance test`**, based on **`likelihood ratio statistic`** [(Coles, 2001)](https://link.springer.com/book/10.1007/978-1-4471-3675-0), offers a formal approach to test this hypothesis by comparing the stationary and non-stationary models.\n:::\n\n##### Creating covariate\n\n::: {.column-margin}\n::: {.callout-note}\nAs mentionned above, we use **time index** to account for trend in the AMF serie. We create in this chunk above a new column, `TimeIndex`, to be used as covariate.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# add time index to use for non-stationary context fitting\nextractedAMF[[\"TimeIndex\"]] = 1:nrow(extractedAMF)\nhead(tibble(extractedAMF), 5)\n```\n\n\n##### Non-Stationary GEV fitting\n\nThe chunk below use the **` {extRemes} `** library to fit the **`Non-Stationary GEV (NS-GEV) distribution`**  to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| echo: true\n#| cache: false\n\n# fit the GEV distribution to the AMF\nextractedAMF = data.frame(extractedAMF)\nnsgevfit = fevd(x=AMAX, location.fun=~TimeIndex, data=extractedAMF, type=\"GEV\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted **`NS-GEV`** distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\nnsgevfit_res = pretty_fit_summary(nsgevfit, \"GEV\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n* **`mu0 = Œº0 = 20.5045:`** is the **`intercept`** of the time-varying location parameter. It represents the baseline magnitude of the annual maximum flood (AMF) at the starting point of the time serie.\n\n* **`mu1 = Œº1 = ‚Äì0.0276:`** is the **`slope`** of the location parameter with respect to time. It indicates a decreasing trend in the location (i.e., central tendency) of the AMF over time. Specifically, for each unit increase in time (i.e., each year), the location parameter decreases by $0.0276m^3.s^{-1}$. This suggests that the typical magnitude of extreme floods has been gradually declining over the observation period.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-ns, the diagnostics from the **`NS-GEV`** distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gevfitdiag-ns\n#| fig-cap: \"Diagnostic plots for the Non-Stationary Generalized Extreme Value (NS-GEV) distribution fitted to the Annual Maximum Flood (AMF) series\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(nsgevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .4))\nplot(nsgevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n##### Deviance test {#sec-deviancetest}\n\n::: {.callout-note}\n## Mathematical of the deviance test\nAs mentionned above, The **Deviance test**, or **Likelihood Ratio (LR)** test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit. \n\n$$ \nD = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \n\\tag{11}\n$$\n\n$D$ represents the deviance test statistic value $(D-statistic)$, $log(ML_{NSGEV})$ and $log(ML_{SGEV})$ are the maximised log-likelihood functions of the $NSGEV$ and the $SGEV$, respectively.\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## log-likelihood function\nThe **`likelihood function`** itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n:::\n:::\n\nTo perform the **`Deviance test`**, we can use the **`lr.test(...)`** of the **`{extRemes}`**. This function just requires the previous fitted objects of class **`‚Äúfevd‚Äù`** from the **`fevd(...)`** function: \n\n* **`gevfit`**: model with fewer parameters (stationary GEV model)\n* **`nsgevfit`**model with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the **Deviance test** is that **simpler model is sufficient** ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting **`c.alpha`** be the **`(1 - alpha)`** quantile of the **`chi-square distribution`** with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that $D = 0$ is rejected if $D > c.alpha$ (i.e., in favor of model Non-Stationary GEV model).\n\n```{r}\n#| echo: true\n#| cache: false\n\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n```\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## Interpretation\nüö´ We fail to reject the null hypothesis.\n\n* **`D-statistic (0.20)`** much smaller than the **`critical value (3.84)`**.\n* **`p-value (0.6547)`** much larger than the **`significance level (0.05)`**.\n:::\n:::\n\n\n::: {.callout-note}\n## Trend significance\nThere is **no significant evidence** that the **Non-Stationary GEV model (with a time-varying location parameter)** provides a better fit than the **stationary model**. The trend is not statistically significant (p = 0.6547).\n:::\n\n\n# Trend analysis conclusions\n\nüîé **Trend Detection Methods Used**\n\n* Parametric: **`Non-stationary GEV model`** with **time-varying location parameter**\n* Non-parametric: **`Mann-Kendall test`**\n\nüö´ **No Significant Trend Detected**\n\n* Both methods consistently indicate **no statistically significant trend** in the annual maximum flow series.\n* The **Likelihood Ratio Test** showed a high p-value (0.65), and the **Mann-Kendall test** similarly returned a non-significant result (p = 0.22).\n\n‚úÖ **Modeling Implication**\n\n* The results support the use of a stationary GEV model.\n* There is **no strong evidence** of temporal change in flood extremes over the study period.\n\n\n# Flood quantiles estimation\n\n## Stationary context\n\nTo estimate **return levels** for given **return periods**, we can use the **`return.level(...)`** from the **`{extRemes}`** package: \n\n::: {.column-margin}\n::: {.callout-note}\nThe **`return level`** is the value expected to be exceeded on average **once every T years**.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(gevfit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n```\n\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`gevfit object`** is the **`gevfit`** object returned by the **`fevd(...)`** function when we have fitted a the **stationary GEV model**.\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-warning icon=\"false\"}\n## ‚ö†Ô∏è Warning\nFor simplicity and illsutration, we use the **normal approximation method** to compute confidence intervals for the estimated return levels. This method assumes the **sampling distribution** of the return level estimates is **approximately normal** (which becomes more reasonable with large sample sizes).\n\n**For more robust intervals, consider using `bootstrap` methods.**\n:::\n:::\n\n## Non-Stationary context\n\nIn non-stationary context, since the **location parameter is time-dependent**, **return levels** have to be calculated for each year over time period. We refer to this as **`‚Äúeffective‚Äù return levels`** The same **`return.level(...)`** from the **`{extRemes}`** package is used to compute **`\"effective\" return levels`**.\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`nsgevfit object`** is the object returned by the **`fevd(...)`** function when we have fitted a the **non-stationary GEV model**.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgevfit, return.period=rperiods)\n```\n\nFor easy visualization, we will plot the **`effective return levels`** for each year over time period. Lets convert the output of **`return.level(...)`** into a clean data.frame :\n\n```{r}\n#| echo: true\n\n# Convert to data.frame\ndf_rlevels <- data.frame(year=extractedAMF$Year,\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n```\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with **`ggplot`**:\n\n```{r}\n#| echo: true\n\ndataplot = df_rlevels |> pivot_longer(-1, names_to=\"RL\", values_to=\"Q\")\n```\n\nFinally, we plot the **`effective return levels`**:\n\n```{r}\n#| echo: true\n#| output: false\n\nrlplot = dataplot %>%\n         ggplot(aes(x=year, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01), limits = c(20, 150), breaks=seq(25, 200, by=25)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time (Year)\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n```\n\n@fig-effective-return-levels helps in understanding the variations and trends in flood quantiles across different time periods.\"\n\n```{r}\n#| cache: false\n#| fig-height: 4\n#| label: fig-effective-return-levels\n#| fig-cap: \"Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles.\"\n\nprint(rlplot)\n```\n\n# Uncertainty estimation\n\nComputing uncertainties in FFA is essential as it helps quantify the **confidence** and **reliability** of estimated flood magnitudes and return levels.\n\nüîç **Acknowledge Data Limitations**\n\nHydrological records are often short or incomplete. Uncertainty analysis helps reflect the limited information available and avoids overconfidence in estimates.\n\nüìä **Support Risk-Based Decision Making**\n\nInfrastructure design, floodplain mapping, and insurance policies rely on return levels. Knowing the uncertainty helps stakeholders assess risk margins and make safer choices.\n\nüìà **Evaluate Model Robustness**\n\nComparing uncertainty across models (e.g., GEV vs Gumbel) can guide model selection and reveal sensitivity to assumptions or input data.\n\n::: {.callout-note icon=\"false\"}\n## üåä Want to Go Further? Quantify Your Confidence!\nThe simpler way to estimate the uncertainty is to compute a **normalized uncertainty range**, defined as the ratio of the difference between the upper (97.5%) and lower (2.5%) bounds of the confidence interval and the estimated quantile was computed.\n:::\n\n\n# References\n\n\n::: {#refs}\n:::","srcMarkdownNoYaml":"\n\n# Libraries\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(trend)\nlibrary(extRemes)\n```\n\n# Data import {#sec-bmdata}\n\nWill work with real-world data, from the **`La Dr√¥me √† Luc-en-Diois`** station in France, to analyze flood events using daily streamflow data covering the period from August 1st, 1958, to December 31st, 2020. The metadata of the station are available on the [HydroPortail website (here)](https://www.hydro.eaufrance.fr/sitehydro/V4214010/fiche).\n\nLet‚Äôs consider the formatted daily streamflow data file for the target catchment. We can read the data into a data frame using the `read.csv()` function. We will also set the column names using the `col.names` argument, and use the `lubridate` library to prepare a time serie.\n\n```{r}\n#| echo: true\n#| eval: true\n\nlibrary(lubridate)\n\nfilename = './data/discharge.csv'\nQdata = read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))  \n\n# construct time serie\nQdata$Date = paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep=\"-\")\nQdata$Date  = ymd(Qdata$Date)\n\n# final time serie\nQdata = Qdata[, c(\"Date\", \"Q\")]\n```\n\nThe output is a data frame (or a matrix) with 22799 rows and 2 columns.\n\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_loaddata\n#| tbl-cap: \"First (6) lines of the daily streamflow data for Block-Maxima approach\"\n\ntail(Qdata, 6) %>% knitr::kable(align=rep(\"c\", 2))\n```\n\n::: {.column-margin}\n\n::: {.callout-note}\nWe can display the structure of the data frame using the `str()` function.\n\n```{r}\n#| echo: true\n#| eval: true\n\nstr(Qdata)  \n```\n\n:::\n\n:::\n\n# Extreme events sampling\n\n\n## The block maxima approach\n\nThe **Block-Maxima Approach [@gumbel1958statistics]**, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and reduce the possiblity of serial dependence.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n#| fig-height: 2\n#| label: fig-bm_illust\n#| fig-cap: \"Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima\"\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(scales)  # for better date formatting\n\n# Simulate daily streamflow data\nset.seed(42)\ndates = seq.Date(from = as.Date(\"1958-08-01\"), to = as.Date(\"1962-07-31\"), by = \"day\")\nQ = rgamma(length(dates), shape = 3, scale = 10)\nstreamflow = tibble(date = dates, Q = Q)\n\n# Define hydrological year starting August 1\nstreamflow = streamflow %>%\n  mutate(hydro_year = if_else(month(date) >= 8, year(date) + 1, year(date)),\n         hydro_year_start = as.Date(paste0(hydro_year - 1, \"-08-01\")))\n\n# Extract block maxima per hydrological year\nmaxima = streamflow %>%\n  group_by(hydro_year) %>%\n  filter(Q == max(Q)) %>%\n  ungroup()\n\n# Get unique hydrological year start dates for vertical lines\nhydro_year_starts = unique(streamflow$hydro_year_start)\n\n# Plot with enhancements\nggplot(streamflow, aes(x = date, y = Q)) +\n  geom_line(color = \"steelblue\", size = 0.3, alpha = 0.8) +\n  geom_point(data = maxima, aes(x = date, y = Q), color = \"red\", size = .7, shape = 8) +\n  geom_vline(xintercept = hydro_year_starts, linetype = \"dashed\", color = \"gray40\", alpha = 0.6) +\n  scale_x_date(date_breaks = \"1 years\", date_labels = \"%Y\") +\n  labs(\n    x = \"Date\",\n    y = \"Streamflow (Q)\"\n  ) +\n  theme_minimal(base_size = 5) +\n  theme(\n    axis.title = element_text(size=5, face=\"bold\"),\n    axis.text = element_text(size=5, color=\"black\"),\n  )\n\n\n```\n\n## Exract annual maximum floods (AMF) from the daily streamflow serie\n\n**1.** To extract annual maximum floods (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\nextract_annual_max = function(df) \n{\n    df %>%\n    mutate(Year = year(Date)) %>%    # extract year from Date\n    group_by(Year) %>%\n    filter(Q == max(Q)) %>%          # keep only max flow per year\n    slice(1) %>%                     # in case of ties, keep first max\n    ungroup() %>%\n    select(Year, Date, AMAX=Q)\n}\n```\n\n**2.** Now, we apply the function to the data to extract AMF for each year:\n\n```{r}\n#| echo: true\n#| eval: true\n#| message: false\n#| warning: false\n\nextractedAMF = extract_annual_max(Qdata)\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_extract\n#| tbl-cap: \"First (6) rows of the AMF extracted using the Block-Maxima approach\"\n\nhead(extractedAMF, 6) %>% knitr::kable(align=rep(\"c\", 2))\n```\n\n## Display sampled vvents\n\nNow, we overlap the extracted **Annual Maximum Flow** with the original daily discharge time serie to visualize the sampling process. We use the functions from the `dplyr` and `ggplot2` library to create the plot.\n\n```{r}\n#| echo: true\n#| message: false\n#| warning: false\n#| output: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n```\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n#| fig-height: 3\n#| label: fig-amf_with_dailystreamflow\n#| fig-cap: \"Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach\"\n\nprint(amf.plot)\n```\n\n\n\n# Autocorrelation testing\n\n::: {.column-margin}\n::: {.callout-tip}\n## Recall\nThe `extractedAMF` variable used in the following code blocs is created previously in the **Sampling Events** section\nThe `acf` function is a R-base function, no installation needed\n:::\n:::\n\n\n## What is autocorrelation? {#sec-autocorrelation}\n\n**`Autocorrelation (or autocovariance)`** of a series refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ...). An autocorrelated series is thus correlated with itself, with a given lag (please refer [here](https://perso.ens-lyon.fr/lise.vaudor/autocorrelation-de-series-temporelles-ou-spatiales/) for mode details). \n\n::: {.callout-note}\n\n## To assess autocorrelation, two methods will be used in this lab\n\nüîç **Correlogram:**\n\n* Visualization of correlation coefficients between the serie and its lags\n* Based on the [autocorrelation function (ACF)](https://medium.com/@kis.andras.nandor/understanding-autocorrelation-and-partial-autocorrelation-functions-acf-and-pacf-2998e7e1bcb5)\n* Helps identify the presence and extent of dependencies over time\n\nüìä **Wald-Wolfowitz Test [@wald_test_1940]:**\n\n* Non-parametric to assess the randomness of a time series\n* Used to detect the absence of randomness in the series\n* **Null hypothesis: The sequence of observations is random**\n\n:::\n\n### Correlogram\n\nIf you execute the above given chunk, it generates the @fig-acfplot as output:\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| message: true\n#| warning: false\n#| fig-height: 2.5\n#| label: fig-acfplot\n#| fig-cap: \"Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.\"\n\n# compute autocorrelation lag\nbacf = acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf = with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn = length(extractedAMF$AMAX)\nconf_limit = 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n:::\n:::\n\n\n### Wald-Wolfowitz (WW) test {#sec-wwtest}\n\nIn the chunk below, we use the `ww.test(...)` function from `{trend}` package to perform the `WW test` at the **0.05 significance level** to test the hypothesis that the maximum annual flood is a random variable.\n\n```{r}\n#| echo: true\n\nwwtest = trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.4609:`** indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness).\n:::\n:::\n\n# Trend detection {#sec-trend}\n\n## Non-parametric methods\n\n### The Mann-Kendall (MK) test {#sec-mk}\n\nThe MK test's null hypothesis (H0) assumes no trend in the data [@mann1945nonparametric; @kendall_rank_1975]. Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results. The Mann-Kendall test statistic $S$ is defined as:\n\n$$\nS = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\text{sgn}(x_j - x_i) \\tag{1}\n$$\n\nThe sign function is given by:\n\n$$\n\\text{sgn}(x_j - x_i) =\n\\begin{cases}\n+1 & \\text{if } (x_j - x_i) > 0 \\\\\n0 & \\text{if } (x_j - x_i) = 0 \\\\\n-1 & \\text{if } (x_j - x_i) < 0\n\\end{cases} \\tag{2}\n$$\n\nwhere $x_i$ and $x_j$ are values of the variable at time $i$ and $j$, respectively, and $n$ is the total number of observations.\n\nA significance level (commonly $\\alpha = 0.05$) and the trend slope indicating the direction and magnitude of the trend are key characteristics of the Mann-Kendall test. If $n > 8$, the test statistic $S$ approximates a normal distribution. In this case, the mean of $S$ is zero and its variance is:\n\n$$\n\\text{var}(S) = \\frac{n(n - 1)(2n + 5)}{18} \\tag{3}\n$$\n\nThe standardized test statistic $Z$, which indicates the direction of the trend, is calculated as:\n\n$$\nZ =\n\\begin{cases}\n\\frac{S - 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S > 0 \\\\\n0 & \\text{if } S = 0 \\\\\n\\frac{S + 1}{\\sqrt{\\text{var}(S)}} & \\text{if } S < 0\n\\end{cases} \\tag{4}\n$$\n\nThe null hypothesis $H_0$ (no trend) is rejected if $|Z| > 1.96$ at the 95% confidence level.\n\n- A **positive trend** is significant if $Z > 1.96$\n- A **negative trend** is significant if $Z < -1.96$\n\n\nWe use the `mk.test(...)` function from `{trend}` package to perform the `MK test` at the **0.05 significance level** to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n::: {.column-margin}\n::: {.callout-note}\n**`Non-Stationarity (NS)`** refers to changes in statistical properties over time.\n:::\n:::\n\n```{r}\n#| echo: true\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Interpretation\n**`p.value = 0.2183:`** there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\n\n**`tau = -0.1096665:`** Kendall's Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\n\n**`z = -1.231:`** the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n:::\n:::\n\n@fig-amfevolution shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n```{r}\n#| echo: true\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n#| fig-height: 3\n#| label: fig-amfevolution\n#| fig-cap: \"Evolution of the annual peak flood over time.\"\n\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n```\n\n### Sen's slope estimator {#sec-sens}\n\nThe **non-parametric method of Sen [-@Sen1968]** is commonly combined with the Mann-Kendall test to estimate the slope of the $(\\beta)$ of a trend. Sen‚Äôs method looks at every pair of data points, calculates the slope between them, then uses the the median of all those slopes as the estimated trend.\n\n$$\n\\beta = \\text{median} \\left( \\frac{x_j - x_i}{j - i} \\right), \\quad , ‚àÄ\\quad j > i \\tag{5}\n$$\n\nTo express the Sen's slope as a **percentage change per time step (i.e., relative trend rate per year)**, it can be normalized by the mean:\n\n$$\n\\beta (\\%) = \\left( \\frac{\\beta}{\\bar{x}} \\right) \\times 100 \\tag{6}\n$$\nwhere $\\beta (\\%)$ is the relative trend rate per year, $x_i$ and $x_j$ are respectively the observed values at time points $i$ and $j$, and $\\bar{x}$ represents the mean value of the variable across the period.\n\nThe Sen's slope can be easily computed using the `sens.slope(...)` function from the `{trend}` package, as in the chunk below:\n\n::: {.column-margin}\n::: {.callout-tip}\n## Sen slope\n**`Slope = -0.1187375 :`** the median trend is slightly negative, suggesting a weak decline.\n\n**`p-value = 0.2183:`** this negative trend is not statistically significant.\n:::\n:::\n\n```{r}\n#| echo: true\n\n#compute Sen's slope and intercept\nsenslope = trend::sens.slope(x = extractedAMF$AMAX)\nprint(senslope)\n```\n\n\nYet, the `sens.slope(...)` function returns the **absolute** slope (i.e., the median change per time unit). It is more meaningful to express the trend as a **relative percentage** change over the time period. \n\nTo do this, we define a **wrapper function** that calculates the Sen's slope and then converts it into a relative trend rate per time step.\n\n```{r}\n#| echo: true\n#| cache: false\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nsens_slope_percent <- function(x, time_step=NULL) \n{\n  #' Compute Sen's slope as a percentage change over the full time period\n  #' @param x A numeric vector of observations\n  #' @param time_step Optional (e.g., year) used for display purpose only\n  #' @return Relative percent change over the period\n\n  require(trend)\n  \n  result <- sens.slope(x)\n  slope <- result$estimates              # Absolute slope (per time unit)\n  p_val <- result$p.value                # p-value for trend test\n  mean_val <- mean(x, na.rm = TRUE)\n  \n  percent_change <- (slope / mean_val) * 100\n  \n  cat(\"Absolute Sen's slope: \", round(slope, 4), \"\\n\", sep=\"\")\n  cat(\"P-value: \", format.pval(p_val, digits = 4), \"\\n\", sep=\"\")\n  if(!is.null(time_step)){\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%/\", time_step, \"\\n\", sep=\"\")\n  }else{\n    cat(\"Relative trend rate: \", round(percent_change, 2), \"%\\n\", sep=\"\")\n  }\n  \n}\n```\n\nNow, we can compute the **relative Sen's slope** for the annual maximum flow data:\n\n```{r}\n#| echo: true\n\n# compute Sen's slope as a percentage change\nsens_slope_percent(extractedAMF$AMAX, time_step = \"year\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## Relative Sen's slope\nThe annual maximum flow shows a slight decreasing trend of ‚àí0.4% per year relative to its long-term average.\n:::\n:::\n\n\n## Parametric methods\n\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a `non-stationary (NS)` context. \n\n::: {.callout-note}\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter **`(Œº)`** will be expressed as a linear function of time, denoted as **`Œº(t)`**, leaving the others parameters constant.\n:::\n\n# Flood frequency analysis (FFA)\n\n## Probability distributions\n\nWe will compare two probability distributions adapted to the `Block-Maxima Framework`: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n::: {.callout-note}\n## GEV distribution\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n\n$$ \nF(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} \n\\tag{7}\n$$\n\nfor $1 + \\xi (x - \\mu)/\\sigma > 0$, where, $\\mu$ is the location parameter, $\\sigma > 0$ is the scale parameter, and $\\xi$ is the shape parameter.\n\nThe shape parameter $\\xi$ determines the type of extreme value distribution:\n\n* If $\\xi > 0$, the distribution is a Fr√©chet type.\n* If $\\xi < 0$, the distribution is a Weibull type.\n* If $\\xi = 0$, the distribution is a Gumbel type.\n\n:::\n\n::: {.callout-note}\n\n## Gumbel distribution\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter $\\xi = 0$. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:\n\n:::\n\n## Models fitting\n\nWe will employ the **`Generalized Maximum Likelihood Estimation (GMLE)`** method to fit probability distributions to our data. The **`{extRemes}`** package offers the function **`fevd(..)`**, which enables fitting a range of probability distributions ‚Äî such as the **GEV**, **Gumbe**l distributions ‚Äî to a given dataset using three distinct fitting methods: **`L-moments`**, **`Maximum Likelihood Estimation (MLE)`**, and **`GMLE`**.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Why the **`GML`** method?\n\nThe **`GMLE`** method use a prior distribution to constrain the **`GEVC shape parameter`** within a reasonnable interval . This prior helps avoid unrealistic or extreme negative values of the shape parameter, leading to more reliable and stable parameter estimates when data are limited (See [**`Martins & Stedinger (2000)`**](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1999WR900330) for more details).\n:::\n:::\n\nThe advantage of using **`GMLE`** and **`MLE`** over the **`L-moments`** method is that **`L-moments`** are limited in their ability to accommodate non-stationary processes. In contrast, **`GMLE`** and **`MLE`** can directly incorporate covariates or time trends, making them more flexible and suitable for analyzing data where the underlying distribution may change over time or as a function of external variables. This flexibility is especially important for trend detection.\n\n## Model selection\n\nTo select the best probability distribution, we will use the **`AIC`** and **`AIC`** criteria. The model with the lowest **`AIC`** or **`BIC`** value will be selected. The model with the **lowest** **`AIC`** and **`BIC`** values is the best-suited model. **`AIC`** tends to favor models with better fit, even if more complex, while **`BIC`** favors simpler models, especially as sample size grows. Combining these these performance metrics enhances the efficiency of model selection.\n\n::: {.column-margin}\n::: {.callout-tip}\n## Joint use of AIC and BIC\n* if both criteria select the same model, it is strongly preferred\n* if not, the choice depends on whether your goal is **predictive accuracy (AIC)** or **model simplicity (BIC)**\n:::\n:::\n\n::: {.callout-note}\n## AIC (Akaike Information Criterion)\n\nThe **`AIC`** balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n\n$$ \n\\text{AIC} = 2k - 2\\ln(\\hat{L}) \\tag{8}\n$$\nwhere $k$ is the number of parameters in the model, $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n::: {.callout-note}\n## BIC (Bayesian Information Criterion)\n\n**`BIC`** penalizes model complexity **more heavily than `AIC`**, especially for larger sample sizes. This means **`BIC`** tends to favor simpler models compared to AIC. The formula for BIC is:\n$$ \n\\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) \\tag{9}\n$$\nwhere: $k$ is the number of parameters in the model, $n$ is the number of data points, and $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n:::\n\n\n## Perform FFA\n\n### Setup utils {#sec-utils}\n\nTo pretty-print the summary of our models' fitting, we will create a custom function that will take the fitted model as an argument, extract and format key components of the fiited object, and print the output: \n\n```{r}\n#| echo: true\n\npretty_fit_summary = function(fitobj, model) {\n  #' Pretty print summary of a  extreme value model fit\n  #'\n  #' @param fitobj Fitted model object from extRemes::fevd()\n  #' @param model Character string describing the model (e.g., \"GEV\")\n  \n  cat(\"===\", model, \" Fit Summary ===\\n\\n\")\n  \n  # Estimated Parameters\n  cat(\"Estimated Parameters:\\n\")\n  params = fitobj$results$par\n  print(round(params, 4))\n  \n  # Log-likelihood, AIC, and BIC\n  look = summary(fitobj, silent=TRUE)\n  cat(\"\\nModel Fit Criteria:\\n\")\n  cat(sprintf(\"LogLik: %.2f\\n\", look$nllh))\n  cat(sprintf(\"AIC: %.2f\\n\", look$AIC))\n  cat(sprintf(\"BIC: %.2f\\n\", look$BIC))\n\n  # return the AIC and BIC values for further comparison\n  return(data.frame(Model=model, Crit=c(\"AIC\", \"BIC\"), Value=c(look$AIC, look$BIC)))\n}\n\n```\n\n### Distributions fitting\n\n#### Stationary context\n\n::: {.panel-tabset}\n\n## [GEV fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the GEV distribution to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| echo: true\n#| cache: false\n\n# fit the GEV distribution to the AMF\nlibrary(extRemes)\ngevfit = fevd(x=extractedAMF$AMAX, type=\"GEV\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted GEV distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\ngevfit_res = pretty_fit_summary(gevfit, \"GEV\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n**`location = 19.4868:`** is the **`central tendency (Œº)`** of the GEV distribution. It defines the central position of the distribution and serves as a baseline around which extreme values are distributed.\n\n**`scale = 9.932:`** reflects the spread or variability of the extreme values. In our case, the AMF distribution has moderate spread around the shifting central value **`Œº(t)`**.\n\n**`shape = 0.3576:`** this positive shape parameter implies a Fr√©chet-type (heavy-tailed) distribution. This means there is greater probability of very large flood events compared to a Gumbel (shape = 0) or Weibull (shape < 0) distribution.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-stationary, the diagnostics from the GEV distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), and  quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gevfitdiag-stationary\n#| fig-cap: \"Diagnostic plots for the Generalized Extreme Value (GEV) distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .043))\nplot(gevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n## [Gumbel fitting]{.custom-tabset-title2}\n\nThe chunk below use the **`{extRemes}`** library to fit the Gumbel distribution to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| warning: false\n#| echo: true\n#| cache: false\n\n# fit the Gumbel distribution to the AMF\ngumfit = fevd(x=extractedAMF$AMAX, type=\"Gumbel\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined previously. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted Gumbel distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\ngumfit_res = pretty_fit_summary(gumfit, \"Gumbel\")\n```\n\nFinally, we show in @fig-gumfitdiag-stationary, the diagnostics from the Gumbel distribution fitted to the Annual Maximum Floods:\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gumfitdiag-stationary\n#| fig-cap: \"Diagnostic plots for the Gumbel distribution fitted to the Annual Maximum Flood (AMF) series in a Stationary Context\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(gumfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .035))\nplot(gumfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n## [Best-suited model]{.custom-tabset-title2}\n\nAs mentionned previously, the model with the lowest **`AIC`** and **`BIC`** values is the best-suited model. The **`pretty_fit_summary()`** function we have defined previously return a data.frame of **AIC** and **BIC** values for each distribution. Lets compare these values to select the best-suited distribution for the next steps.\n\n```{r}\n#| echo: true\n#| eval: false\n\nprint(gevfit_res)\nprint(gumfit_res)\n```\n```{r}\n#| echo: false\n#| eval: true\n#| label: tbl-bm_aicbictable\n#| tbl-cap: \"Table of AIC and BIC values for each distribution\"\n#| tbl-subcap: \n#|   - GEV\n#|   - Gumbel\n#| layout-ncol: 2\n\nknitr::kable(slice(gevfit_res, -3), align=rep(\"c\", 2))\nknitr::kable(slice(gumfit_res, -3), align=rep(\"c\", 2))\n```\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## ‚úÖ Best-suited: GEV\nBoth **AIC and BIC are lower for the GEV model**, which suggests that it provides a better balance of fit and complexity than the Gumbel model.\n:::\n:::\n\n:::\n\n\n#### Parametric trend-detection\n\nWhen covariates are to be incorporated into EVA, a common approach to account for non-stationary is to incorporate covariates within the parameters of the distributions in a **`regression-like manner`**. To perform **parametric trend detection**, we model the **`location parameter (Œº)`** as a function of time, transforming the models into **`non-stationary distributions`**. The location parameter of the non-stationnary GEV and Gumbel is computed as follow:\n$$\n\\mu (t) = \\mu 0 + \\mu 1¬∑t \\tag{10}\n$$\nwhere $\\mu$ is the time-dependant location parameter, $t$ is the time index, $\\mu 0$ is the intercept, and $\\mu 1$ is the slope.\n\n::: {.callout-tip}\n# Trend?\nThe decision to adopt a non-stationary (or more complex) model in FFA depends on whether the temporal trend is **`statistically significant`**.\n\nThe **`deviance test`**, based on **`likelihood ratio statistic`** [(Coles, 2001)](https://link.springer.com/book/10.1007/978-1-4471-3675-0), offers a formal approach to test this hypothesis by comparing the stationary and non-stationary models.\n:::\n\n##### Creating covariate\n\n::: {.column-margin}\n::: {.callout-note}\nAs mentionned above, we use **time index** to account for trend in the AMF serie. We create in this chunk above a new column, `TimeIndex`, to be used as covariate.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# add time index to use for non-stationary context fitting\nextractedAMF[[\"TimeIndex\"]] = 1:nrow(extractedAMF)\nhead(tibble(extractedAMF), 5)\n```\n\n\n##### Non-Stationary GEV fitting\n\nThe chunk below use the **` {extRemes} `** library to fit the **`Non-Stationary GEV (NS-GEV) distribution`**  to the **Annual Maximum Floods** extracted earlier. \n\n```{r}\n#| message: false\n#| echo: true\n#| cache: false\n\n# fit the GEV distribution to the AMF\nextractedAMF = data.frame(extractedAMF)\nnsgevfit = fevd(x=AMAX, location.fun=~TimeIndex, data=extractedAMF, type=\"GEV\", method=\"GMLE\")\n```\n\nNow, we will print the results of the fit, using the **`pretty_fit_summary()`** function we have defined above. This will show the **AIC** and **BIC** values, as well as the estimated parameters of the fitted **`NS-GEV`** distribution.\n\n```{r}\n#| echo: true\n#| cache: false\n\n# show fitting sumary\nnsgevfit_res = pretty_fit_summary(nsgevfit, \"GEV\")\n```\n\n::: {.column-margin}\n::: {.callout-tip}\n## What about the parameters?\n\n* **`mu0 = Œº0 = 20.5045:`** is the **`intercept`** of the time-varying location parameter. It represents the baseline magnitude of the annual maximum flood (AMF) at the starting point of the time serie.\n\n* **`mu1 = Œº1 = ‚Äì0.0276:`** is the **`slope`** of the location parameter with respect to time. It indicates a decreasing trend in the location (i.e., central tendency) of the AMF over time. Specifically, for each unit increase in time (i.e., each year), the location parameter decreases by $0.0276m^3.s^{-1}$. This suggests that the typical magnitude of extreme floods has been gradually declining over the observation period.\n:::\n:::\n\nFinally, we show in @fig-gevfitdiag-ns, the diagnostics from the **`NS-GEV`** distribution fitted to the Annual Maximum Floods: density plot of empirical data and fitted GEV df (top-left), quantiles from a sample drawn from the fitted GEV against the empirical data quantiles with 95% confidence bands (top-right).\n\n```{r}\n#| echo: true\n#| cache: false\n#| fig-height: 4\n#| label: fig-gevfitdiag-ns\n#| fig-cap: \"Diagnostic plots for the Non-Stationary Generalized Extreme Value (NS-GEV) distribution fitted to the Annual Maximum Flood (AMF) series\"\n\n# show diagnostic plots of the GEV distribution fitting\npar(mfrow=c(1,2))\nplot(nsgevfit, type=\"density\", main=\"(a) Density plot\", ylim=c(0, .4))\nplot(nsgevfit, type=\"qq2\", main=NULL)\ntitle(main=\"(b) Q-Q plot\")\n```\n\n##### Deviance test {#sec-deviancetest}\n\n::: {.callout-note}\n## Mathematical of the deviance test\nAs mentionned above, The **Deviance test**, or **Likelihood Ratio (LR)** test, compares two nested models ‚Äî typically, a simpler (stationary) model and a more complex (non-stationary) model. It helps determine if introducing additional parameters (such as a time-varying location parameter in a GEV model) leads to a statistically significant improvement in model fit. \n\n$$ \nD = 2{log(ML_{NSGEV}) - log(ML_{SGEV})} \n\\tag{11}\n$$\n\n$D$ represents the deviance test statistic value $(D-statistic)$, $log(ML_{NSGEV})$ and $log(ML_{SGEV})$ are the maximised log-likelihood functions of the $NSGEV$ and the $SGEV$, respectively.\n:::\n\n\n::: {.column-margin}\n::: {.callout-tip}\n## log-likelihood function\nThe **`likelihood function`** itself measures how well a statistical model explains observed data by giving, for a set of parameters, the probability (or probability density) of observing the data under the model.\n:::\n:::\n\nTo perform the **`Deviance test`**, we can use the **`lr.test(...)`** of the **`{extRemes}`**. This function just requires the previous fitted objects of class **`‚Äúfevd‚Äù`** from the **`fevd(...)`** function: \n\n* **`gevfit`**: model with fewer parameters (stationary GEV model)\n* **`nsgevfit`**model with more parameters (non-stationary GEV model)\n\nThe null hypothesis of the **Deviance test** is that **simpler model is sufficient** ‚Äî i.e., no trend (e.g., the location parameter Œº is constant over time). Letting **`c.alpha`** be the **`(1 - alpha)`** quantile of the **`chi-square distribution`** with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that $D = 0$ is rejected if $D > c.alpha$ (i.e., in favor of model Non-Stationary GEV model).\n\n```{r}\n#| echo: true\n#| cache: false\n\n# applying the Deviance test\nlr.test(x=gevfit, y=nsgevfit, alpha=0.05, df=1)\n```\n\n::: {.column-margin}\n::: {.callout-tip icon=\"false\"}\n## Interpretation\nüö´ We fail to reject the null hypothesis.\n\n* **`D-statistic (0.20)`** much smaller than the **`critical value (3.84)`**.\n* **`p-value (0.6547)`** much larger than the **`significance level (0.05)`**.\n:::\n:::\n\n\n::: {.callout-note}\n## Trend significance\nThere is **no significant evidence** that the **Non-Stationary GEV model (with a time-varying location parameter)** provides a better fit than the **stationary model**. The trend is not statistically significant (p = 0.6547).\n:::\n\n\n# Trend analysis conclusions\n\nüîé **Trend Detection Methods Used**\n\n* Parametric: **`Non-stationary GEV model`** with **time-varying location parameter**\n* Non-parametric: **`Mann-Kendall test`**\n\nüö´ **No Significant Trend Detected**\n\n* Both methods consistently indicate **no statistically significant trend** in the annual maximum flow series.\n* The **Likelihood Ratio Test** showed a high p-value (0.65), and the **Mann-Kendall test** similarly returned a non-significant result (p = 0.22).\n\n‚úÖ **Modeling Implication**\n\n* The results support the use of a stationary GEV model.\n* There is **no strong evidence** of temporal change in flood extremes over the study period.\n\n\n# Flood quantiles estimation\n\n## Stationary context\n\nTo estimate **return levels** for given **return periods**, we can use the **`return.level(...)`** from the **`{extRemes}`** package: \n\n::: {.column-margin}\n::: {.callout-note}\nThe **`return level`** is the value expected to be exceeded on average **once every T years**.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(gevfit, return.period=rperiods, do.ci=TRUE)\n# print results\nprint(rlevels)\n```\n\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`gevfit object`** is the **`gevfit`** object returned by the **`fevd(...)`** function when we have fitted a the **stationary GEV model**.\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-warning icon=\"false\"}\n## ‚ö†Ô∏è Warning\nFor simplicity and illsutration, we use the **normal approximation method** to compute confidence intervals for the estimated return levels. This method assumes the **sampling distribution** of the return level estimates is **approximately normal** (which becomes more reasonable with large sample sizes).\n\n**For more robust intervals, consider using `bootstrap` methods.**\n:::\n:::\n\n## Non-Stationary context\n\nIn non-stationary context, since the **location parameter is time-dependent**, **return levels** have to be calculated for each year over time period. We refer to this as **`‚Äúeffective‚Äù return levels`** The same **`return.level(...)`** from the **`{extRemes}`** package is used to compute **`\"effective\" return levels`**.\n\n::: {.column-margin}\n::: {.callout-note}\nThe **`nsgevfit object`** is the object returned by the **`fevd(...)`** function when we have fitted a the **non-stationary GEV model**.\n:::\n:::\n\n```{r}\n#| echo: true\n\n# specific return periods (e.g., 10, 20 years)\nrperiods = c(2, 5, 10, 20, 50, 100)\n# compute return levels (with 95% confidence intervals)\nrlevels = return.level(nsgevfit, return.period=rperiods)\n```\n\nFor easy visualization, we will plot the **`effective return levels`** for each year over time period. Lets convert the output of **`return.level(...)`** into a clean data.frame :\n\n```{r}\n#| echo: true\n\n# Convert to data.frame\ndf_rlevels <- data.frame(year=extractedAMF$Year,\n                         Q2=rlevels[, 1],\n                         Q5=rlevels[, 2],\n                         Q10=rlevels[, 3],\n                         Q20=rlevels[, 4],\n                         Q50=rlevels[, 5],\n                         Q100=rlevels[, 6])\nhead(df_rlevels)\n```\n\nNow, we tranform the data into a long (tidy) format suitable for plotting multiple lines with **`ggplot`**:\n\n```{r}\n#| echo: true\n\ndataplot = df_rlevels |> pivot_longer(-1, names_to=\"RL\", values_to=\"Q\")\n```\n\nFinally, we plot the **`effective return levels`**:\n\n```{r}\n#| echo: true\n#| output: false\n\nrlplot = dataplot %>%\n         ggplot(aes(x=year, y=Q, color=RL, group=RL)) +\n         geom_line(linewidth=1) +\n         scale_x_continuous(expand=c(0.01, 0.01)) +\n         scale_y_continuous(expand=c(0.01, 0.01), limits = c(20, 150), breaks=seq(25, 200, by=25)) +\n         guides(color=guide_legend(keywidth=3)) +\n         theme_bw() +\n         theme(legend.text=element_text(size=12),\n               axis.title=element_text(size=12),\n               axis.text=element_text(size=12, color=\"black\")) +\n         labs(x=\"Time (Year)\", y=\"Return level (Q)\", color=NULL)\n           \nprint(rlplot)\n```\n\n@fig-effective-return-levels helps in understanding the variations and trends in flood quantiles across different time periods.\"\n\n```{r}\n#| cache: false\n#| fig-height: 4\n#| label: fig-effective-return-levels\n#| fig-cap: \"Effective return levels from the NS-GEV model. Each line represents a different return period (e.i., 2-, 5-, 10-, 20 50, and 100-year floods), and the colour coding indicates these return periods. The x-axis denotes the time index (year), and the y-axis indicates the estimated flood quantiles.\"\n\nprint(rlplot)\n```\n\n# Uncertainty estimation\n\nComputing uncertainties in FFA is essential as it helps quantify the **confidence** and **reliability** of estimated flood magnitudes and return levels.\n\nüîç **Acknowledge Data Limitations**\n\nHydrological records are often short or incomplete. Uncertainty analysis helps reflect the limited information available and avoids overconfidence in estimates.\n\nüìä **Support Risk-Based Decision Making**\n\nInfrastructure design, floodplain mapping, and insurance policies rely on return levels. Knowing the uncertainty helps stakeholders assess risk margins and make safer choices.\n\nüìà **Evaluate Model Robustness**\n\nComparing uncertainty across models (e.g., GEV vs Gumbel) can guide model selection and reveal sensitivity to assumptions or input data.\n\n::: {.callout-note icon=\"false\"}\n## üåä Want to Go Further? Quantify Your Confidence!\nThe simpler way to estimate the uncertainty is to compute a **normalized uncertainty range**, defined as the ratio of the difference between the upper (97.5%) and lower (2.5%) bounds of the confidence interval and the estimated quantile was computed.\n:::\n\n\n# References\n\n\n::: {#refs}\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":false,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr","message":false},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":6,"number-sections":true,"html-math-method":"katex","include-in-header":{"file":"../header.html"},"highlight-style":"github","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.552","bibliography":["../references.bib"],"csl":"../apa.csl","image":"https://github.com/diopBachir/tp_eva/blob/master/docs/evaillustr.png","date-format":"D MMMM YYYY","fig-cap-location":"bottom","jupyter":"python3","toc-expand":4,"toc-title":"On this page","toc-location":"left","toc-collapsible":true,"theme":["../styles.scss"],"anchor-sections":true,"smooth-scroll":true,"code-block-border-left":"#31BAE9","title":"Flood Frequency Analysis (FFA) with Block-Maxima"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}