{
  "hash": "ea961ce725724f0f2b1f32290702ced9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Flood Frequency Analysis (FFA) with Block-Maxima\"\n---\n\n\n\n# Data Import\n\nWill work with real-world data to analyze flood events using daily streamflow data covering the period from August 1st, 1958, to December 31st, 2020. \n\nLet‚Äôs consider the formatted daily streamflow data file for the target catchment. We can read the data into a data frame using the `read.csv()` function. We will also set the column names using the `col.names` argument, and use the `lubridate` library to prepare a time serie.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilename = './data/discharge.csv'\nQdata = read.csv(filename, col.names = c(\"Year\", \"Month\", \"Day\", \"Q\"))  \n\n# construct time serie\nQdata$Date = paste(Qdata[[\"Year\"]], Qdata[[\"Month\"]], Qdata[[\"Day\"]], sep=\"-\")\nQdata$Date  = ymd(Qdata$Date)\n\n# final time serie\nQdata = Qdata[, c(\"Date\", \"Q\")]\n```\n:::\n\n\nThe output is a data frame (or a matrix) with 22799 rows and 2 columns.\n\n\n::: {#tbl-bm_loaddata .cell tbl-cap='First (6) lines of the daily streamflow data for Block-Maxima approach'}\n::: {.cell-output-display}\n\n\n|    Date    |     Q     |\n|:----------:|:---------:|\n| 1958-08-01 | 0.2357639 |\n| 1958-08-02 | 0.1841204 |\n| 1958-08-03 | 0.1437037 |\n| 1958-08-04 | 0.1437037 |\n| 1958-08-05 | 0.1437037 |\n| 1958-08-06 | 0.1437037 |\n\n\n:::\n:::\n\n\n::: {.column-margin}\n\n::: {.callout-note}\nWe can display the structure of the data frame using the `str()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Qdata)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t22799 obs. of  2 variables:\n $ Date: Date, format: \"1958-08-01\" \"1958-08-02\" ...\n $ Q   : num  0.236 0.184 0.144 0.144 0.144 ...\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# Sampling Extreme Events\n\n\n\n## The Block-Maxima Approach\n\nThe **Block-Maxima Approach**, consists of sampling the maximum streamflow value for each year (or block). This method is straightforward to implement and ensures, by construction, that the sampled values are statistically independent, a key requirement in extreme value analysis (EVA).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of the Block-Maxima Approach: Daily Streamflow with Yearly Maxima](block_maxima_files/figure-html/fig-bm_illust-1.png){#fig-bm_illust width=672}\n:::\n:::\n\n\n## Exract Annual Maximum Flows (AMF) from the daily streamflow serie\n\n**1.** To extract annual maximum flows (AMF) from the streamflow series, we need to define a function that returns the maximum value of a block of data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(lubridate)\nextract_annual_max = function(df) \n{\n    df %>%\n    mutate(Year = year(Date)) %>%    # extract year from Date\n    group_by(Year) %>%\n    filter(Q == max(Q)) %>%          # keep only max flow per year\n    slice(1) %>%                     # in case of ties, keep first max\n    ungroup() %>%\n    select(Year, Date, AMAX=Q)\n}\n```\n:::\n\n\n**2.** Now, we apply the function to the data to extract AMF for each year:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextractedAMF = extract_annual_max(Qdata)\n```\n:::\n\n::: {#tbl-bm_extract .cell tbl-cap='First (6) rows of the AMF extracted using the Block-Maxima approach'}\n::: {.cell-output-display}\n\n\n| Year |    Date    |   AMAX   |\n|:----:|:----------:|:--------:|\n| 1958 | 1958-12-20 | 39.96984 |\n| 1959 | 1959-03-07 | 36.89593 |\n| 1960 | 1960-10-06 | 58.41780 |\n| 1961 | 1961-12-11 | 18.44796 |\n| 1962 | 1962-03-05 | 19.47185 |\n| 1963 | 1963-04-11 | 29.21002 |\n\n\n:::\n:::\n\n\n## Display Sampled Events\n\nNow, we overlap the extracted **Annual Maximum Flow** with the original daily discharge time serie to visualize the sampling process. We use the functions from the `dplyr` and `ggplot2` library to create the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlibrary(ggplot2)\namf.plot = ggplot() +\n           geom_line(data=Qdata, aes(x=Date, y=Q), color=\"steelblue\", size=0.7, alpha=0.8) +\n           geom_point(data=extractedAMF, aes(x=Date, y=AMAX), color =\"red\", size=1.2) +\n           labs(x=\"Date\", y=\"Discharge (Q)\") +\n           theme_minimal(base_size = 13) +\n           theme(axis.title=element_text(size=12, face=\"bold\"),\n                 axis.text=element_text(size=12, color=\"black\"))\nprint(amf.plot)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily streamflow time serie (blue) and with Extracted Annual Peak Floods (red) extracted using the Block-Maxima Approach](block_maxima_files/figure-html/fig-amf_with_dailystreamflow-1.png){#fig-amf_with_dailystreamflow width=672}\n:::\n:::\n\n\n\n\n# Autocorrelation Testing\n\n::: {.column-margin}\n::: {.callout-note}\n## Recall\nThe `extractedAMF` variable used in the following code blocs is created previously in the **Sampling Events** section\nThe `acf` function is a R-base function, no installation needed\n:::\n:::\n\n\n## What is Autocorrelation?\n\n**`Autocorrelation (or autocovariance)`** of a series refers to the fact that in a time or spatial serie, the measurement of a phenomenon at time t can be correlated with previous measurements (at time t-1, t-2, t-3, etc.) or with subsequent measurements (at t+1, t+2, t+3, ...). An autocorrelated series is thus correlated with itself, with a given lag (please refer [here](https://perso.ens-lyon.fr/lise.vaudor/autocorrelation-de-series-temporelles-ou-spatiales/) for mode details). \n\n::: {.callout-note}\n\n## To assess autocorrelation, two methods will be used in this lab\n\nüîç **Correlogram:**\n\n* Visualization of correlation coefficients between the serie and its lags\n* Based on the [autocorrelation function (ACF)](https://medium.com/@kis.andras.nandor/understanding-autocorrelation-and-partial-autocorrelation-functions-acf-and-pacf-2998e7e1bcb5)\n* Helps identify the presence and extent of dependencies over time\n\nüìä **Wald-Wolfowitz Test  [(1940)](https://doi.org/10.1214/aoms/1177731909):**\n\n* Non-parametric to assess the randomness of a time series\n* Used to detect the absence of randomness in the series\n* **Null hypothesis: The sequence of observations is random**\n\n:::\n\n### Correlogram\n\nIf you execute the above given chunk, it generates the @fig-acfplot as output:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# compute autocorrelation lag\nbacf <- acf(extractedAMF$AMAX, plot = FALSE)\nbacfdf <- with(bacf, data.frame(lag, acf))\n\n# confidence interval\nn <- length(extractedAMF$AMAX)\nconf_limit <- 1.96 / sqrt(n)\n\n# correlogram\nlibrary(ggplot2)\nggplot(data=bacfdf, mapping=aes(x=lag, y=acf)) +\n  geom_hline(aes(yintercept=0)) +\n  geom_bar(stat=\"identity\", position=\"identity\", width=.2) +\n  geom_hline(yintercept=conf_limit, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=-conf_limit, linetype=\"dashed\", color=\"red\") +\n  labs(y=\"ACF\", x=\"Lag\") +\n  theme_minimal() +\n  theme(axis.title=element_text(size=10, face=\"bold\", family=\"Times\"),\n        axis.text=element_text(size=10, color=\"black\", family=\"Times\"))\n```\n\n::: {.cell-output-display}\n![Autocorrelation (at different lags) of Annual Maximum Flood (AMAX). The red line represents the 95% confidence interval.](block_maxima_files/figure-html/fig-acfplot-1.png){#fig-acfplot width=672}\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\n## Interpretation\nLooking at the 95% confidence interval bounds in the correlogram, we can see that from lag -1 to lag-16, the autocorrelation is only significant at lag-8, so we can conclude that the maximum annual flood is a random variable.\n:::\n:::\n\n\n### Wald-Wolfowitz (WW) Test\n\nIn the chunk below, we use the `ww.test(...)` function from `{trend}` package to perform the `WW test` at the **0.05 significance level** to test the hypothesis that the maximum annual flood is a random variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwwtest = trend::ww.test(extractedAMF$AMAX)\nprint(wwtest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWald-Wolfowitz test for independence and stationarity\n\ndata:  extractedAMF$AMAX\nz = 0.73738, n = 60, p-value = 0.4609\nalternative hypothesis: The series is significantly different from \n independence and stationarity\n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\n## Interpretation\n**`p.value = 0.4609:`** indicates that the data series appears to be consistent with the assumption of independence and stationarity (i.e., there is not enough evidence to reject the null hypothesis of randomness).\n:::\n:::\n\n## Trend Detection\n\n### Non-Parametric methods: The Mann-Kendall (MK) test\n\nThe MK test's null hypothesis (H0) assumes no trend in the data (please see [here](https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm) for the detailed algorithm). Recommended by the World Meteorological Organization (WMO), it is simple and requires no assumptions about data distribution, which is particularly advantageous for hydroclimatic series that are rarely normally distributed. Additionally, outlier values have very little influence on the results.\n\nWe use the `mk.test(...)` function from `{trend}` package to perform the `MK test` at the **0.05 significance level** to test the hypothesis that there is no stationarity in the maximum annual floods serie.\n\n::: {.column-margin}\n::: {.callout-note}\n**`Non-Stationarity (NS)`** refers to changes in statistical properties over time.\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmktest = trend::mk.test(extractedAMF$AMAX)\nprint(mktest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMann-Kendall trend test\n\ndata:  extractedAMF$AMAX\nz = -1.231, n = 60, p-value = 0.2183\nalternative hypothesis: true S is not equal to 0\nsample estimates:\n            S          varS           tau \n -194.0000000 24581.3333333    -0.1096665 \n```\n\n\n:::\n:::\n\n\n::: {.column-margin}\n::: {.callout-note}\n## Interpretation\n**`p.value = 0.2183:`** there is not enough statistical evidence to conclude that a significant trend exists in the AMF serie.\n\n**`tau = -0.1096665:`** Kendall's Tau is a non-parametric measure of trend strength, ranging from -1 (perfect decreasing trend) to +1 (perfect increasing trend). A value near 0 indicates a weak or no trend.\n\n**`z = -1.231:`** the calculated test statistic (a z-score). A negative value suggests a potential decreasing trend, while a positive value suggests a potential increasing trend.\n:::\n:::\n\n@fig-amfevolution shows the data points and a red line indicating the overall linear trend. The Mann-Kendall test result (p-value = 0.2183) indicates that this visually apparent (decreasing) linear trend is not statistically significant at the 0.05 significance level.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlineplot = ggplot(extractedAMF, aes(Year, AMAX)) +\n           geom_line(color=\"blue\", linewidth=1) +\n           geom_smooth(color=\"red\", linewidth=1, method=\"lm\", se=F)\nprint(lineplot)\n```\n\n::: {.cell-output-display}\n![Evolution of the annual peak flood over time.](block_maxima_files/figure-html/fig-amfevolution-1.png){#fig-amfevolution width=672}\n:::\n:::\n\n\n### Parametric methods\n\nParametric methods rely on the adjustment of distributions adapted to extremes event (e.g., Generalized Extreme Value, Generalized Pareto Distribution, etc.) in a `non-stationary (NS)` context. \n\n::: {.callout-note}\nNS is taken into account by allowing the models parameters to vary as a linear function of time or other covariates. Here, the location parameter **`(Œº)`** will be expressed as a linear function of time, denoted as **`Œº(t)`**, leaving the others parameters constant.\n:::\n\n## Flood Frequency Analysis (FFA)\n\n### Probability distributions\n\nWe will compare two probability distributions adapted to the `Block-Maxima Framework`: the Generalized Extreme Value (GEV) and the Gumbel probability distributions.\n\n::: {.callout-note}\n## Generalized Extreme Value (GEV) Distribution\n\nThe Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions developed to combine the Gumbel, Fr√©chet, and Weibull distributions. According to the Extreme Value Theorem, the GEV distribution is the only possible limit distribution for properly normalized sequences of maxima of independent and identically distributed random variables. The cumulative distribution function (CDF) of the GEV distribution is given by:\n\n$$ F(x; \\mu, \\sigma, \\xi) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\} $$\n\nfor $1 + \\xi (x - \\mu)/\\sigma > 0$, where:\n\n* $\\mu$ is the location parameter.\n* $\\sigma > 0$ is the scale parameter.\n* $\\xi$ is the shape parameter.\n\nThe shape parameter $\\xi$ determines the type of extreme value distribution:\n\n* If $\\xi > 0$, the distribution is a Fr√©chet type.\n* If $\\xi < 0$, the distribution is a Weibull type.\n* If $\\xi = 0$, the distribution is a Gumbel type.\n\n:::\n\n::: {.callout-note}\n\n## Gumbel Distribution\n\nThe Gumbel distribution is a special case of the GEV distribution when the shape parameter $\\xi = 0$. It is often used to model the distribution of the maximum (or minimum) of a number of samples of various distributions. It is one of the three types of extreme value distributions. The cumulative distribution function (CDF) of the Gumbel distribution (Type I Extreme Value distribution) is given by:\n\n:::\n\n### Comparison of the probability distributions \n\nTo select the best probability distribution, we will use the **`AIC`** and **`AIC`** criteria. The model with the lowest AIC or BIC value will be selected.\n\n::: {.callout-note}\n## AIC (Akaike Information Criterion)\n\nThe **`AIC`** balances the goodness of fit of the model with the number of parameters used by the model. This mean that it rewards models that fit the data well but also penalizes models that use too many parameters. The formula for AIC is:\n\n$$ \\text{AIC} = 2k - 2\\ln(\\hat{L}) $$\n\nwhere:\n\n* $k$ is the number of parameters in the model.\n* $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n\n:::\n\n::: {.callout-note}\n## BIC (Bayesian Information Criterion)\n\n**`BIC`** penalizes model complexity **more heavily than `AIC`**, especially for larger sample sizes. This means **`BIC`** tends to favor simpler models compared to AIC. The formula for BIC is:\n\n$$ \\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L}) $$\n\nwhere:\n\n* $k$ is the number of parameters in the model.\n* $n$ is the number of data points.\n* $\\hat{L}$ is the maximum value of the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the model.\n\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}